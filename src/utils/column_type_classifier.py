#!/usr/bin/env python
"""
M√≥dulo de classifica√ß√£o autom√°tica de tipos de colunas com suporte a LLM.
Vers√£o 3.0 - Com corre√ß√µes de curto e m√©dio prazo

Este m√≥dulo identifica automaticamente os tipos de colunas em um DataFrame
usando regras heur√≠sticas e, opcionalmente, LLM local para casos amb√≠guos.

Tipos suportados:
- Num√©rica (inteiro, float)
- Categ√≥rica (baixa cardinalidade)
- Texto (alta cardinalidade, texto livre)
- Data/Tempo
- Email
- Telefone
- URL
- ID/C√≥digo
- Booleana
- Mista

Autor: Smart Ads Team
Vers√£o: 3.0 (com melhorias de detec√ß√£o)
"""

import re
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from datetime import datetime
import warnings
from collections import Counter
import json
import requests
import time
import os
import hashlib

warnings.filterwarnings('ignore')


class ColumnTypeClassifier:
    """
    Classificador robusto de tipos de colunas com suporte opcional a LLM e cache.
    """
    
    # Tipos de colunas suportados
    NUMERIC = 'numeric'
    CATEGORICAL = 'categorical'
    TEXT = 'text'
    DATETIME = 'datetime'
    EMAIL = 'email'
    PHONE = 'phone'
    URL = 'url'
    ID = 'id'
    BOOLEAN = 'boolean'
    MIXED = 'mixed'
    UNKNOWN = 'unknown'
    
    # Vers√£o do classificador (atualizada para v3)
    VERSION = '3.0'
    
    def __init__(self, 
                 categorical_threshold: int = 100,
                 text_min_avg_length: int = 20,  # REDUZIDO de 40
                 text_min_unique_ratio: float = 0.5,  # REDUZIDO de 0.7
                 text_min_words: int = 3,  # REDUZIDO de 5
                 numeric_string_threshold: float = 0.9,
                 date_detection_threshold: float = 0.7,  # REDUZIDO de 0.8
                 sample_size: int = 1000,
                 confidence_threshold: float = 0.7,
                 # Par√¢metros LLM
                 use_llm: bool = True,
                 llm_model: str = "llama3.2:3b",  # MUDADO para modelo melhor
                 ollama_host: str = "http://localhost:11434",
                 llm_confidence_threshold: float = 0.6,  # REDUZIDO de 0.75
                 llm_sample_size: int = 100,
                 cache_dir: str = "/Users/ramonmoreira/desktop/smart_ads/cache",
                 # Par√¢metros de cache do classificador
                 use_classification_cache: bool = True,
                 classification_cache_path: Optional[str] = None,
                 fail_on_llm_error: bool = False,
                 # NOVO: Conhecimento de dom√≠nio
                 use_domain_knowledge: bool = True):
        """
        Inicializa o classificador com par√¢metros otimizados.
        
        Args:
            categorical_threshold: N√∫mero m√°ximo de valores √∫nicos para considerar categ√≥rica
            text_min_avg_length: Comprimento m√©dio m√≠nimo para considerar texto
            text_min_unique_ratio: Raz√£o m√≠nima de valores √∫nicos para considerar texto
            text_min_words: N√∫mero m√≠nimo de palavras m√©dias para considerar texto
            numeric_string_threshold: Propor√ß√£o m√≠nima de strings num√©ricas
            date_detection_threshold: Propor√ß√£o m√≠nima de datas v√°lidas
            sample_size: Tamanho da amostra para an√°lise
            confidence_threshold: Confian√ßa m√≠nima para classifica√ß√£o
            use_llm: Se True, usa LLM para casos amb√≠guos
            llm_model: Modelo Ollama a usar
            ollama_host: URL do servidor Ollama
            llm_confidence_threshold: Threshold para usar LLM
            llm_sample_size: Amostra para enviar √† LLM
            cache_dir: Diret√≥rio para cache de classifica√ß√µes
            use_classification_cache: Se True, usa cache de classifica√ß√µes do dataset
            classification_cache_path: Caminho espec√≠fico para o cache de classifica√ß√µes
            fail_on_llm_error: Se True, falha se LLM n√£o estiver dispon√≠vel
            use_domain_knowledge: Se True, usa conhecimento de dom√≠nio para classifica√ß√£o
        """
        self.categorical_threshold = categorical_threshold
        self.text_min_avg_length = text_min_avg_length
        self.text_min_unique_ratio = text_min_unique_ratio
        self.text_min_words = text_min_words
        self.numeric_string_threshold = numeric_string_threshold
        self.date_detection_threshold = date_detection_threshold
        self.sample_size = sample_size
        self.confidence_threshold = confidence_threshold
        
        # Configura√ß√µes LLM
        self.use_llm = use_llm
        self.llm_model = llm_model
        self.ollama_host = ollama_host
        self.llm_confidence_threshold = llm_confidence_threshold
        self.llm_sample_size = llm_sample_size
        self.cache_dir = cache_dir
        self.fail_on_llm_error = fail_on_llm_error
        
        # Configura√ß√µes de cache de classifica√ß√£o
        self.use_classification_cache = use_classification_cache
        self.classification_cache_path = classification_cache_path or os.path.join(cache_dir, "column_classifications.json")
        self.classification_cache = None
        self.cache_hits = 0
        self.cache_misses = 0
        
        # NOVO: Flag para conhecimento de dom√≠nio
        self.use_domain_knowledge = use_domain_knowledge
        
        # Criar diret√≥rio de cache
        os.makedirs(cache_dir, exist_ok=True)
        
        # Verificar LLM se habilitado
        if self.use_llm:
            self._check_ollama_connection()
        
        # Padr√µes regex compilados
        self._compile_patterns()
        
        # Conhecimento de dom√≠nio para Smart Ads
        self._setup_domain_knowledge()
        
        # Par√¢metros do classificador para hash
        self._classifier_params = {
            'version': self.VERSION,
            'categorical_threshold': categorical_threshold,
            'text_min_avg_length': text_min_avg_length,
            'text_min_unique_ratio': text_min_unique_ratio,
            'text_min_words': text_min_words,
            'numeric_string_threshold': numeric_string_threshold,
            'date_detection_threshold': date_detection_threshold
        }
        
    def _compile_patterns(self):
        """Compila padr√µes regex para reutiliza√ß√£o."""
        # Email pattern
        self.email_pattern = re.compile(
            r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$',
            re.IGNORECASE
        )
        
        # Phone patterns
        self.phone_patterns = [
            re.compile(r'^\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}$'),
            re.compile(r'^\(\d{2,3}\)\s?\d{4,5}-?\d{4}$'),
            re.compile(r'^\d{10,15}$'),
        ]
        
        # URL pattern
        self.url_pattern = re.compile(
            r'^https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&/=]*)$',
            re.IGNORECASE
        )
        
        # ID patterns
        self.id_patterns = [
            re.compile(r'^[A-Z0-9]{8,}$'),
            re.compile(r'^\d{6,}$'),
            re.compile(r'^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$'),
        ]
        
        # Numeric string pattern
        self.numeric_string_pattern = re.compile(r'^-?\d+\.?\d*$')
        
        # Date patterns MAIS FLEX√çVEIS
        self.strict_date_patterns = [
            (re.compile(r'^\d{4}-\d{2}-\d{2}$'), 'YYYY-MM-DD'),
            (re.compile(r'^\d{2}/\d{2}/\d{4}$'), 'DD/MM/YYYY'),
            (re.compile(r'^\d{2}-\d{2}-\d{4}$'), 'DD-MM-YYYY'),
            (re.compile(r'^\d{1,2}/\d{1,2}/\d{4}$'), 'D/M/YYYY'),  # NOVO: datas com 1 ou 2 d√≠gitos
            (re.compile(r'^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}$'), 'YYYY-MM-DD HH:MM:SS'),
            (re.compile(r'^\d{2}-\d{2}-\d{4}\s\d{2}:\d{2}$'), 'DD-MM-YYYY HH:MM'),
            (re.compile(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}'), 'ISO8601'),
        ]
        
        # NOVO: Padr√µes de perguntas que indicam texto
        self.text_question_patterns = [
            r'¬øc√≥mo\s+te\s+llamas',
            r'¬øcu√°l\s+es\s+tu\s+(nombre|instagram|profesi√≥n)',
            r'd√©jame',
            r'mensaje',
            r'¬øqu√©\s+(esperas|cambiar√°)',
            r'describe',
            r'explica',
            r'¬øpor\s+qu√©',
            r'cuenta',
            r'comenta'
        ]
    
    def _setup_domain_knowledge(self):
        """Configura conhecimento espec√≠fico do dom√≠nio Smart Ads."""
        
        # NOVO: Mapeamento direto baseado em conhecimento do dom√≠nio
        self.domain_type_mapping = {
            'data': self.DATETIME,
            'marca_temporal': self.DATETIME,
            
            # Campos de texto conhecidos
            'como_te_llamas': self.TEXT,
            'cual_es_tu_instagram': self.TEXT,
            'cual_es_tu_profesion': self.TEXT,
            'cuando_hables_ingles_con_fluidez_que_cambiara_en_tu_vida_que_oportunidades_se_abriran_para_ti': self.TEXT,
            'que_esperas_aprender_en_el_evento_cero_a_ingles_fluido': self.TEXT,
            'dejame_un_mensaje': self.TEXT,
            
            # Campos categ√≥ricos conhecidos
            'cuales_son_tus_principales_razones_para_aprender_ingles': self.CATEGORICAL,
            'has_comprado_algun_curso_para_aprender_ingles_antes': self.BOOLEAN,
            }
        
        # ATUALIZADO: Listas brancas e negras mais precisas
        self.datetime_blacklist = [
            'qualidade', 'quality', 'g√©nero', 'gender', 
            'profesi√≥n', 'profession', 'razones', 'reasons'
        ]
        
        self.datetime_whitelist = [
            'data', 'date', 'fecha', 'timestamp', 'temporal',
            'created', 'updated', 'modified', 'cadastro',
            'marca temporal'  # ADICIONADO
        ]
        
        # Mapeamentos espec√≠ficos
        self.utm_keywords = [
            'utm_source', 'utm_medium', 'utm_campaign', 'utm_campaing',  # typo comum
            'utm_content', 'utm_term', 'gclid', 'fbclid', 'msclkid'
        ]
        
        self.text_keywords = [
            'mensaje', 'message', 'descripci√≥n', 'description',
            'observaci√≥n', 'observation', 'comentario', 'comment',
            'oportunidades', 'opportunities', 'esperas', 'expect',
            'cambiar√°', 'change', 'profesi√≥n', 'profession',
            'instagram', 'nombre', 'name'  # ADICIONADOS
        ]
    
    def _check_ollama_connection(self):
        """Verifica se o Ollama est√° acess√≠vel (apenas se use_llm=True)."""
        if not self.use_llm:
            return
            
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.llm_model not in model_names:
                    m1_suggestions = f"""
‚ùå ERRO: Modelo '{self.llm_model}' n√£o encontrado no Ollama.

Modelos recomendados para MacBook M1:
  1. ollama pull llama3.2:3b     # Melhor qualidade/performance
  2. ollama pull gemma:2b        # Mais leve e r√°pido
  3. ollama pull mistral:7b-instruct-q4_0  # Melhor qualidade (mais RAM)

Para instalar:
  ollama pull {self.llm_model}
"""
                    if self.fail_on_llm_error:
                        raise RuntimeError(m1_suggestions)
                    else:
                        print(m1_suggestions)
                        self.use_llm = False
                else:
                    print(f"‚úì Ollama conectado. Modelo '{self.llm_model}' dispon√≠vel.")
            else:
                raise ConnectionError(f"Ollama retornou status {response.status_code}")
                
        except requests.exceptions.ConnectionError as e:
            error_msg = f"""
‚ùå ERRO: N√£o foi poss√≠vel conectar ao Ollama em {self.ollama_host}

Por favor, certifique-se de que o Ollama est√° rodando:
  1. Abra um novo terminal
  2. Execute: ollama serve
  3. Tente novamente

Erro t√©cnico: {str(e)}
"""
            if self.fail_on_llm_error:
                raise RuntimeError(error_msg)
            else:
                print(error_msg)
                print("‚ö†Ô∏è Continuando sem LLM...")
                self.use_llm = False
        except Exception as e:
            error_msg = f"‚ùå Erro inesperado ao verificar Ollama: {e}"
            if self.fail_on_llm_error:
                raise RuntimeError(error_msg)
            else:
                print(error_msg)
                self.use_llm = False
    
    def classify_column(self, series: pd.Series, column_name: str = None) -> Dict[str, Any]:
        """
        Classifica o tipo de uma coluna com conhecimento de dom√≠nio.
        """
        # NOVO: Verificar conhecimento de dom√≠nio primeiro
        if self.use_domain_knowledge and column_name:
            if column_name in self.domain_type_mapping:
                return {
                    'type': self.domain_type_mapping[column_name],
                    'confidence': 0.99,
                    'metadata': {
                        'source': 'domain_knowledge',
                        'reason': 'exact_match'
                    }
                }
        
        # Usar classifica√ß√£o baseada em regras
        base_result = self._classify_with_rules(series, column_name)
        
        # Se LLM habilitado e confian√ßa baixa, usar LLM
        if self.use_llm and base_result['confidence'] < self.llm_confidence_threshold:
            llm_result = self._classify_with_llm(series, column_name, base_result)
            if llm_result:
                return llm_result
        
        return base_result
    
    def _classify_with_rules(self, series: pd.Series, column_name: str = None) -> Dict[str, Any]:
        """
        Classifica usando regras heur√≠sticas (m√©todo original).
        """
        # Amostragem
        if len(series) > self.sample_size:
            sample = series.dropna().sample(min(self.sample_size, len(series.dropna())), random_state=42)
        else:
            sample = series.dropna()
        
        if len(sample) == 0:
            return {
                'type': self.UNKNOWN,
                'confidence': 0.0,
                'metadata': {'reason': 'all_null'}
            }
        
        # An√°lise inicial
        dtype = str(series.dtype)
        unique_count = series.nunique()
        total_count = len(series)
        null_ratio = series.isna().sum() / total_count
        
        # 1. Verificar tipos num√©ricos nativos
        if pd.api.types.is_numeric_dtype(series):
            if unique_count <= 2 and set(series.dropna().unique()).issubset({0, 1, 0.0, 1.0}):
                return {
                    'type': self.BOOLEAN,
                    'confidence': 0.95,
                    'metadata': {
                        'subtype': 'numeric_boolean',
                        'unique_values': list(series.dropna().unique())
                    }
                }
            return {
                'type': self.NUMERIC,
                'confidence': 0.99,
                'metadata': {
                    'subtype': 'float' if series.dtype == float else 'integer',
                    'range': [series.min(), series.max()],
                    'mean': series.mean() if not series.empty else None
                }
            }
        
        # 2. Para tipos object, fazer an√°lise detalhada
        if dtype == 'object':
            str_sample = sample.astype(str)
            
            # ORDEM CR√çTICA DE VERIFICA√á√ïES:
            
            # 2.1 Verificar datetime PRIMEIRO (antes de UTM)
            date_result = self._check_datetime_strict(str_sample, column_name)
            if date_result['is_datetime']:
                return {
                    'type': self.DATETIME,
                    'confidence': date_result['confidence'],
                    'metadata': date_result['metadata']
                }
            
            # 2.2 Verificar UTM/Tracking
            utm_result = self._check_utm_or_tracking(str_sample, column_name, unique_count, total_count)
            if utm_result.get('is_utm'):
                return {
                    'type': self.CATEGORICAL,
                    'confidence': utm_result['confidence'],
                    'metadata': utm_result['metadata']
                }
            
            # 2.3 Verificar booleano
            bool_result = self._check_boolean(str_sample, series)
            if bool_result['is_boolean']:
                return {
                    'type': self.BOOLEAN,
                    'confidence': bool_result['confidence'],
                    'metadata': bool_result['metadata']
                }
            
            # 2.4 Verificar email
            email_result = self._check_email(str_sample, column_name)
            if email_result['is_email']:
                return {
                    'type': self.EMAIL,
                    'confidence': email_result['confidence'],
                    'metadata': email_result['metadata']
                }
            
            # 2.5 Verificar telefone
            phone_result = self._check_phone(str_sample, column_name)
            if phone_result['is_phone']:
                return {
                    'type': self.PHONE,
                    'confidence': phone_result['confidence'],
                    'metadata': phone_result['metadata']
                }
            
            # 2.6 Verificar URL
            url_result = self._check_url(str_sample)
            if url_result['is_url']:
                return {
                    'type': self.URL,
                    'confidence': url_result['confidence'],
                    'metadata': url_result['metadata']
                }
            
            # 2.7 Verificar num√©rico disfar√ßado
            numeric_result = self._check_numeric_string(str_sample)
            if numeric_result['is_numeric']:
                return {
                    'type': self.NUMERIC,
                    'confidence': numeric_result['confidence'],
                    'metadata': numeric_result['metadata']
                }
            
            # 2.8 Verificar ID
            id_result = self._check_id(str_sample, column_name, unique_count, total_count)
            if id_result['is_id']:
                return {
                    'type': self.ID,
                    'confidence': id_result['confidence'],
                    'metadata': id_result['metadata']
                }
            
            # 2.9 Texto vs Categ√≥rica
            text_result = self._check_text_vs_categorical(
                series, str_sample, unique_count, total_count, column_name
            )
            return text_result
        
        # 3. Tipo datetime nativo
        if pd.api.types.is_datetime64_any_dtype(series):
            return {
                'type': self.DATETIME,
                'confidence': 0.99,
                'metadata': {
                    'subtype': 'pandas_datetime',
                    'format': 'native'
                }
            }
        
        return {
            'type': self.UNKNOWN,
            'confidence': 0.0,
            'metadata': {'dtype': dtype}
        }
    
    # ===== M√âTODOS DE VERIFICA√á√ÉO =====
    
    def _check_utm_or_tracking(self, sample: pd.Series, column_name: Optional[str],
                              unique_count: int, total_count: int) -> Dict[str, Any]:
        """Verifica se √© uma coluna de tracking/UTM."""
        if not column_name:
            return {'is_utm': False}
        
        col_lower = column_name.lower()
        
        for utm_keyword in self.utm_keywords:
            if utm_keyword in col_lower:
                return {
                    'is_utm': True,
                    'confidence': 0.95,
                    'metadata': {
                        'type': 'tracking_parameter',
                        'matched_keyword': utm_keyword,
                        'unique_values': unique_count,
                        'unique_ratio': unique_count / total_count
                    }
                }
        
        return {'is_utm': False}
    
    def _check_datetime_strict(self, sample: pd.Series, column_name: Optional[str]) -> Dict[str, Any]:
        """Verifica datetime com mais flexibilidade."""
        
        # NOVO: Verificar whitelist primeiro
        if column_name:
            col_lower = column_name.lower()
            
            # Whitelist tem prioridade
            for white in self.datetime_whitelist:
                if white in col_lower:
                    # Verificar se realmente tem datas
                    for pattern, format_name in self.strict_date_patterns:
                        matches = sample.astype(str).str.match(pattern)
                        match_ratio = matches.sum() / len(sample)
                        
                        if match_ratio > 0.5:  # Threshold mais baixo para whitelist
                            return {
                                'is_datetime': True,
                                'confidence': min(1.0, match_ratio + 0.1),
                                'metadata': {
                                    'format': format_name,
                                    'match_ratio': match_ratio,
                                    'detected_by': 'whitelist_and_pattern'
                                }
                            }
            
            # Verificar blacklist (mas menos agressiva)
            for black in self.datetime_blacklist:
                if black in col_lower and not any(white in col_lower for white in self.datetime_whitelist):
                    return {'is_datetime': False}
        
        # Verificar se os dados parecem datas
        date_like_count = 0
        sample_str = sample.astype(str).head(20)
        
        for val in sample_str:
            # Verifica√ß√£o mais flex√≠vel
            has_numbers = any(c.isdigit() for c in val)
            has_date_separators = any(sep in val for sep in ['-', '/', ':'])
            reasonable_length = 6 <= len(val) <= 30  # Mais flex√≠vel
            
            if has_numbers and has_date_separators and reasonable_length:
                date_like_count += 1
        
        if date_like_count < len(sample_str) * 0.4:  # Threshold mais baixo
            return {'is_datetime': False}
        
        # Verificar padr√µes com threshold mais baixo
        for pattern, format_name in self.strict_date_patterns:
            matches = sample.astype(str).str.match(pattern)
            match_ratio = matches.sum() / len(sample)
            
            if match_ratio > self.date_detection_threshold:  # Usa o threshold configur√°vel
                return {
                    'is_datetime': True,
                    'confidence': match_ratio,
                    'metadata': {
                        'format': format_name,
                        'match_ratio': match_ratio,
                        'detected_by': 'pattern_matching'
                    }
                }
        
        return {'is_datetime': False}
    
    def _detect_multiple_choice_patterns(self, series: pd.Series, sample: pd.Series) -> Dict[str, Any]:
        """Detecta padr√µes de m√∫ltipla escolha."""
        value_counts = series.value_counts()
        top_values = value_counts.head(20)
        
        yes_no_patterns = {
            's√≠', 'si', 'no', 'tal vez', 'talvez', 'quiz√°s', 'quizas',
            'definitivamente', 'probablemente', 'nunca', 'siempre',
            'yes', 'no', 'maybe', 'definitely', 'probably'
        }
        
        time_patterns = {
            'a√±o', 'a√±os', 'mes', 'meses', 'semana', 'semanas',
            'd√≠a', 'd√≠as', 'hora', 'horas', 'minuto', 'minutos',
            'siempre', 'nunca', 'a veces', 'frecuentemente'
        }
        
        range_patterns = {
            'm√°s de', 'menos de', 'entre', 'hasta', 'desde',
            '-', 'a', 'o m√°s', 'o menos'
        }
        
        unique_values_lower = [str(v).lower().strip() for v in series.dropna().unique()]
        
        # Verificar respostas sim/n√£o
        yes_no_matches = sum(1 for val in unique_values_lower if val in yes_no_patterns)
        if yes_no_matches >= len(unique_values_lower) * 0.5:
            return {
                'is_multiple_choice': True,
                'pattern': 'yes_no',
                'confidence': 0.95
            }
        
        # Verificar ranges
        range_count = sum(1 for val in unique_values_lower 
                         if any(pattern in val for pattern in range_patterns))
        if range_count >= len(unique_values_lower) * 0.5:
            return {
                'is_multiple_choice': True,
                'pattern': 'numeric_ranges',
                'confidence': 0.9
            }
        
        # Verificar tempo
        time_count = sum(1 for val in unique_values_lower 
                        if any(pattern in val for pattern in time_patterns))
        if time_count >= len(unique_values_lower) * 0.5:
            return {
                'is_multiple_choice': True,
                'pattern': 'time_periods',
                'confidence': 0.9
            }
        
        # Op√ß√µes limitadas
        avg_length = sample.str.len().mean()
        if len(unique_values_lower) <= 20 and avg_length < 30:
            return {
                'is_multiple_choice': True,
                'pattern': 'limited_options',
                'confidence': 0.8
            }
        
        return {'is_multiple_choice': False}
    
    def _check_text_vs_categorical(self, series: pd.Series, sample: pd.Series,
                                  unique_count: int, total_count: int,
                                  column_name: Optional[str]) -> Dict[str, Any]:
        """Decide entre texto e categ√≥rica com crit√©rios mais flex√≠veis."""
        
        # NOVO: Verificar padr√µes de perguntas abertas
        if column_name:
            col_lower = column_name.lower()
            
            # Verificar se √© uma pergunta que tipicamente espera texto
            for pattern in self.text_question_patterns:
                if re.search(pattern, col_lower, re.IGNORECASE):
                    return {
                        'type': self.TEXT,
                        'confidence': 0.95,
                        'metadata': {
                            'reason': 'open_question_pattern',
                            'pattern_matched': pattern,
                            'unique_count': unique_count,
                            'unique_ratio': unique_count / total_count
                        }
                    }
            
            # Verificar keywords de texto
            for keyword in self.text_keywords:
                if keyword in col_lower:
                    return {
                        'type': self.TEXT,
                        'confidence': 0.9,
                        'metadata': {
                            'reason': 'text_keyword_match',
                            'keyword': keyword,
                            'unique_count': unique_count
                        }
                    }
        
        # Verificar m√∫ltipla escolha
        mc_result = self._detect_multiple_choice_patterns(series, sample)
        if mc_result.get('is_multiple_choice'):
            return {
                'type': self.CATEGORICAL,
                'confidence': mc_result['confidence'],
                'metadata': {
                    'reason': 'multiple_choice_pattern',
                    'pattern': mc_result['pattern'],
                    'unique_count': unique_count,
                    'categories': list(series.value_counts().head(10).index),
                    'distribution': series.value_counts(normalize=True).head(10).to_dict()
                }
            }
        
        # M√©tricas
        unique_ratio = unique_count / total_count
        avg_length = sample.str.len().mean()
        max_length = sample.str.len().max()
        min_length = sample.str.len().min()
        length_std = sample.str.len().std()
        
        word_counts = sample.str.split().str.len()
        avg_words = word_counts.mean() if not word_counts.empty else 0
        max_words = word_counts.max() if not word_counts.empty else 0
        
        has_punctuation = sample.str.contains(r'[.!?;,:]', regex=True).sum() / len(sample)
        has_multiple_sentences = sample.str.contains(r'[.!?]\s+[A-Z]', regex=True).sum() / len(sample)
        
        # NOVO: Verificar se tem alta variabilidade de respostas (indica texto)
        if unique_ratio > 0.8 and avg_length > 10:
            return {
                'type': self.TEXT,
                'confidence': 0.85,
                'metadata': {
                    'reason': 'high_unique_ratio_with_length',
                    'unique_ratio': unique_ratio,
                    'avg_length': avg_length
                }
            }
        
        # Crit√©rios MAIS FLEX√çVEIS para texto
        if avg_words >= self.text_min_words and (has_punctuation > 0.2 or avg_length > 15):
            return {
                'type': self.TEXT,
                'confidence': 0.85,
                'metadata': {
                    'reason': 'sufficient_words_with_structure',
                    'avg_words': avg_words,
                    'has_punctuation_ratio': has_punctuation,
                    'avg_length': avg_length
                }
            }
        
        if avg_length >= self.text_min_avg_length and (length_std > 10 or unique_ratio > 0.5):
            return {
                'type': self.TEXT,
                'confidence': 0.8,
                'metadata': {
                    'reason': 'length_based_classification',
                    'avg_length': avg_length,
                    'length_std': length_std,
                    'unique_ratio': unique_ratio
                }
            }
        
        if has_multiple_sentences > 0.1:  # Threshold mais baixo
            return {
                'type': self.TEXT,
                'confidence': 0.85,
                'metadata': {
                    'reason': 'multiple_sentences',
                    'multiple_sentence_ratio': has_multiple_sentences,
                    'avg_words': avg_words
                }
            }
        
        # Verifica√ß√µes para categ√≥rica
        if unique_count < 10 and avg_words <= 2:
            return {
                'type': self.CATEGORICAL,
                'confidence': 0.95,
                'metadata': {
                    'reason': 'low_unique_short_answers',
                    'unique_count': unique_count,
                    'avg_words': avg_words,
                    'categories': list(series.value_counts().head(10).index)
                }
            }
        
        top_values = series.value_counts().head(10)
        top_values_ratio = top_values.sum() / total_count
        
        if top_values_ratio > 0.7 and avg_words < self.text_min_words:
            return {
                'type': self.CATEGORICAL,
                'confidence': 0.85,
                'metadata': {
                    'reason': 'concentrated_distribution',
                    'top_10_coverage': top_values_ratio,
                    'unique_count': unique_count,
                    'distribution': series.value_counts(normalize=True).head(10).to_dict()
                }
            }
        
        # Default mais inteligente baseado em caracter√≠sticas
        if unique_ratio >= self.text_min_unique_ratio or avg_length >= self.text_min_avg_length:
            return {
                'type': self.TEXT,
                'confidence': 0.7,
                'metadata': {
                    'reason': 'default_text_characteristics',
                    'unique_ratio': unique_ratio,
                    'avg_length': avg_length,
                    'avg_words': avg_words
                }
            }
        
        return {
            'type': self.CATEGORICAL,
            'confidence': 0.7,
            'metadata': {
                'reason': 'default_categorical',
                'unique_count': unique_count,
                'avg_length': avg_length,
                'avg_words': avg_words
            }
        }
    
    def _check_boolean(self, sample: pd.Series, original_series: pd.Series) -> Dict[str, Any]:
        """Verifica se √© booleano."""
        bool_values = {
            'true', 'false', 't', 'f', 'yes', 'no', 'y', 'n',
            'sim', 'n√£o', 'nao', 's', 'n', '1', '0',
            'verdadero', 'falso', 'v', 'f', 'si', 's√≠'
        }
        
        lower_sample = sample.str.lower().str.strip()
        unique_lower = set(lower_sample.unique())
        
        if len(unique_lower) <= 2 and unique_lower.issubset(bool_values):
            return {
                'is_boolean': True,
                'confidence': 0.95,
                'metadata': {
                    'unique_values': list(original_series.dropna().unique()),
                    'value_counts': original_series.value_counts().to_dict()
                }
            }
        
        return {'is_boolean': False}
    
    def _check_email(self, sample: pd.Series, column_name: Optional[str]) -> Dict[str, Any]:
        """Verifica emails."""
        if column_name:
            email_keywords = ['email', 'e-mail', 'mail', 'correo', 'correio']
            if any(keyword in column_name.lower() for keyword in email_keywords):
                email_match_ratio = sample.str.match(self.email_pattern).sum() / len(sample)
                if email_match_ratio > 0.5:
                    return {
                        'is_email': True,
                        'confidence': 0.9,
                        'metadata': {
                            'match_ratio': email_match_ratio,
                            'detected_by': 'name_and_pattern'
                        }
                    }
        
        email_match_ratio = sample.str.match(self.email_pattern).sum() / len(sample)
        if email_match_ratio > 0.8:
            return {
                'is_email': True,
                'confidence': email_match_ratio,
                'metadata': {
                    'match_ratio': email_match_ratio,
                    'detected_by': 'pattern'
                }
            }
        
        return {'is_email': False}
    
    def _check_phone(self, sample: pd.Series, column_name: Optional[str]) -> Dict[str, Any]:
        """Verifica telefones."""
        if column_name:
            phone_keywords = ['phone', 'telefone', 'tel', 'celular', 'mobile', 'whatsapp', 'fone']
            if any(keyword in column_name.lower() for keyword in phone_keywords):
                confidence_boost = 0.2
            else:
                confidence_boost = 0.0
        else:
            confidence_boost = 0.0
        
        cleaned_sample = sample.str.replace(r'[\s\-\(\)\.]+', '', regex=True)
        
        max_match_ratio = 0
        for pattern in self.phone_patterns:
            match_ratio = cleaned_sample.str.match(pattern).sum() / len(cleaned_sample)
            max_match_ratio = max(max_match_ratio, match_ratio)
        
        adjusted_confidence = min(1.0, max_match_ratio + confidence_boost)
        
        if adjusted_confidence > 0.6:
            return {
                'is_phone': True,
                'confidence': adjusted_confidence,
                'metadata': {
                    'match_ratio': max_match_ratio,
                    'name_boost': confidence_boost > 0
                }
            }
        
        return {'is_phone': False}
    
    def _check_url(self, sample: pd.Series) -> Dict[str, Any]:
        """Verifica URLs."""
        url_match_ratio = sample.str.match(self.url_pattern).sum() / len(sample)
        
        if url_match_ratio < 0.7:
            domain_pattern = re.compile(r'^[\w\-]+(\.[\w\-]+)+(/.*)?$')
            domain_match_ratio = sample.str.match(domain_pattern).sum() / len(sample)
            
            if domain_match_ratio > 0.8:
                return {
                    'is_url': True,
                    'confidence': domain_match_ratio * 0.8,
                    'metadata': {
                        'match_ratio': domain_match_ratio,
                        'type': 'domain_only'
                    }
                }
        
        if url_match_ratio > 0.7:
            return {
                'is_url': True,
                'confidence': url_match_ratio,
                'metadata': {
                    'match_ratio': url_match_ratio,
                    'type': 'full_url'
                }
            }
        
        return {'is_url': False}
    
    def _check_numeric_string(self, sample: pd.Series) -> Dict[str, Any]:
        """Verifica strings num√©ricas."""
        cleaned_sample = sample.str.strip()
        numeric_match_ratio = cleaned_sample.str.match(self.numeric_string_pattern).sum() / len(cleaned_sample)
        
        if numeric_match_ratio > self.numeric_string_threshold:
            try:
                numeric_values = pd.to_numeric(cleaned_sample, errors='coerce')
                conversion_success_ratio = numeric_values.notna().sum() / len(cleaned_sample)
                
                if conversion_success_ratio > 0.9:
                    return {
                        'is_numeric': True,
                        'confidence': conversion_success_ratio,
                        'metadata': {
                            'match_ratio': numeric_match_ratio,
                            'conversion_ratio': conversion_success_ratio,
                            'subtype': 'numeric_string'
                        }
                    }
            except:
                pass
        
        return {'is_numeric': False}
    
    def _check_id(self, sample: pd.Series, column_name: Optional[str], 
                  unique_count: int, total_count: int) -> Dict[str, Any]:
        """Verifica IDs."""
        if column_name:
            id_keywords = ['id', 'code', 'codigo', 'key', 'identifier', 'uuid', 'guid', 'sku']
            if any(keyword in column_name.lower() for keyword in id_keywords):
                confidence_boost = 0.3
            else:
                confidence_boost = 0.0
        else:
            confidence_boost = 0.0
        
        uniqueness_ratio = unique_count / total_count
        
        if uniqueness_ratio < 0.8:
            return {'is_id': False}
        
        max_match_ratio = 0
        for pattern in self.id_patterns:
            match_ratio = sample.str.match(pattern).sum() / len(sample)
            max_match_ratio = max(max_match_ratio, match_ratio)
        
        lengths = sample.str.len()
        same_length_ratio = (lengths == lengths.mode()[0]).sum() / len(lengths)
        
        confidence = (uniqueness_ratio * 0.4 + 
                     max_match_ratio * 0.3 + 
                     same_length_ratio * 0.3 + 
                     confidence_boost)
        
        if confidence > 0.7:
            return {
                'is_id': True,
                'confidence': min(1.0, confidence),
                'metadata': {
                    'uniqueness_ratio': uniqueness_ratio,
                    'pattern_match_ratio': max_match_ratio,
                    'same_length_ratio': same_length_ratio
                }
            }
        
        return {'is_id': False}
    
    # ===== M√âTODOS LLM =====
    
    def _get_cache_key(self, column_name: str, sample_data: List[str]) -> str:
        """Gera chave para cache."""
        data_str = f"{column_name}:{':'.join(sorted(set(str(s) for s in sample_data[:20])))}"
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Carrega do cache."""
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.json")
        if os.path.exists(cache_path):
            with open(cache_path, 'r') as f:
                return json.load(f)
        return None
    
    def _save_to_cache(self, cache_key: str, classification: Dict[str, Any]):
        """Salva no cache."""
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.json")
        with open(cache_path, 'w') as f:
            json.dump(classification, f)
    
    def _prepare_llm_sample(self, series: pd.Series) -> Tuple[List[str], Dict[str, Any]]:
        """Prepara amostra para LLM."""
        clean_series = series.dropna()
        
        if len(clean_series) == 0:
            return [], {'total_values': 0, 'null_count': len(series)}
        
        stats = {
            'total_values': len(series),
            'null_count': series.isna().sum(),
            'unique_count': series.nunique(),
            'unique_ratio': series.nunique() / len(series)
        }
        
        if len(clean_series) <= self.llm_sample_size:
            sample = clean_series.astype(str).tolist()
        else:
            value_counts = clean_series.value_counts()
            n_frequent = min(len(value_counts), self.llm_sample_size // 2)
            frequent_values = value_counts.head(n_frequent).index.tolist()
            
            remaining = self.llm_sample_size - n_frequent
            random_values = clean_series[~clean_series.isin(frequent_values)].sample(
                min(remaining, len(clean_series) - n_frequent),
                random_state=42
            ).tolist()
            
            sample = [str(v) for v in frequent_values + random_values]
        
        if series.dtype == 'object':
            str_series = clean_series.astype(str)
            stats['avg_length'] = str_series.str.len().mean()
            stats['avg_words'] = str_series.str.split().str.len().mean()
            stats['has_punctuation_ratio'] = str_series.str.contains(r'[.!?;,:]', regex=True).sum() / len(str_series)
        
        return sample, stats
    
    def _create_llm_prompt(self, column_name: str, sample_data: List[str], 
                          stats: Dict[str, Any]) -> str:
        """Cria prompt melhorado para LLM."""
        prompt = f"""You are a data type classifier for a machine learning pipeline analyzing survey data about English learning courses in Spanish.

Column Name: {column_name}

Statistics:
- Total values: {stats['total_values']}
- Null values: {stats['null_count']}
- Unique values: {stats['unique_count']}
- Unique ratio: {stats.get('unique_ratio', 0):.2f}
- Average length: {stats.get('avg_length', 'N/A')}
- Average words: {stats.get('avg_words', 'N/A')}

Sample Values (showing {len(sample_data)} examples):
{json.dumps(sample_data[:50], indent=2, ensure_ascii=False)}

CONTEXT: This is survey data where people answer questions about learning English. Common patterns:
- Questions starting with "¬øC√≥mo te llamas?" ask for names ‚Üí TEXT
- Questions about "instagram", "profesi√≥n" ask for free text ‚Üí TEXT  
- "DATA" or "Marca temporal" are timestamps ‚Üí DATETIME
- Questions with limited options (age ranges, yes/no) ‚Üí CATEGORICAL

CRITICAL CLASSIFICATION RULES:

1. **datetime** - MUST classify as datetime if:
   - Column named "DATA", "Marca temporal", "timestamp", "fecha"
   - Values match date patterns: dd/mm/yyyy, yyyy-mm-dd, timestamps
   - Even if only 70% of values match date patterns

2. **text** - MUST classify as text if:
   - Asking for names: "¬øC√≥mo te llamas?"
   - Asking for social media: "instagram", "twitter"
   - Asking for professions: "profesi√≥n", "occupation"
   - Open-ended questions: "D√©jame un mensaje", "¬øQu√© esperas?"
   - High unique ratio (>0.5) with average length >10 characters

3. **categorical** - Only if:
   - Clear multiple choice pattern
   - Very limited set of repeated values (<20 unique)
   - Yes/No questions
   - Age/salary ranges

4. **email** - Email addresses
5. **phone** - Phone numbers  
6. **boolean** - Only true/false or yes/no with 2 values
7. **numeric** - Pure numbers
8. **url** - Web addresses
9. **id** - Unique identifiers

Based on the column name and values, classify this column.

Respond with JSON:
{{
  "type": "the_column_type",
  "confidence": 0.0-1.0,
  "reasoning": "explanation in English"
}}"""
        
        return prompt
    
    def _query_llm(self, prompt: str) -> Dict[str, Any]:
        """Consulta LLM."""
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.llm_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9,
                        "seed": 42
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '').strip()
                
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    classification = json.loads(json_match.group())
                    return classification
                else:
                    print(f"‚ö†Ô∏è Resposta sem JSON v√°lido")
                    return None
            else:
                print(f"‚ùå Erro na API: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"‚ùå Erro LLM: {e}")
            return None
    
    def _classify_with_llm(self, series: pd.Series, column_name: str,
                          base_classification: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Classifica usando LLM."""
        if not self.use_llm:
            return None
            
        print(f"\nü§ñ Usando LLM para '{column_name}' (confian√ßa: {base_classification['confidence']:.2f})")
        
        sample_data, stats = self._prepare_llm_sample(series)
        if not sample_data:
            return None
        
        # Cache
        cache_key = self._get_cache_key(column_name, sample_data)
        cached = self._load_from_cache(cache_key)
        if cached:
            print("  ‚úì Usando cache")
            return cached
        
        # Prompt
        prompt = self._create_llm_prompt(column_name, sample_data, stats)
        
        # Query
        print(f"  üîÑ Consultando {self.llm_model}...")
        start_time = time.time()
        llm_result = self._query_llm(prompt)
        elapsed = time.time() - start_time
        print(f"  ‚è±Ô∏è {elapsed:.1f}s")
        
        if llm_result and all(k in llm_result for k in ['type', 'confidence']):
            valid_types = [
                'categorical', 'text', 'datetime', 'numeric', 
                'email', 'phone', 'boolean', 'url', 'id'
            ]
            
            if llm_result['type'] in valid_types:
                final_result = {
                    'type': llm_result['type'],
                    'confidence': float(llm_result['confidence']),
                    'metadata': {
                        'llm_model': self.llm_model,
                        'llm_reasoning': llm_result.get('reasoning', ''),
                        'base_type': base_classification['type'],
                        'base_confidence': base_classification['confidence'],
                        'response_time': elapsed
                    }
                }
                
                print(f"  ‚úì LLM: {llm_result['type']} ({llm_result['confidence']:.2f})")
                self._save_to_cache(cache_key, final_result)
                return final_result
        
        return None
    
    def _compute_dataset_hash(self, df: pd.DataFrame) -> str:
        """Computa hash do dataset para detectar mudan√ßas."""
        # Incluir: shape, nomes das colunas, tipos
        dataset_info = {
            'shape': df.shape,
            'columns': sorted(df.columns.tolist()),
            'dtypes': {col: str(df[col].dtype) for col in df.columns}
        }
        
        # Adicionar amostra de dados de cada coluna
        data_samples = {}
        for col in df.columns:
            # Pegar primeiros e √∫ltimos valores √∫nicos
            unique_vals = df[col].dropna().unique()
            if len(unique_vals) > 0:
                sample_vals = sorted([str(v) for v in unique_vals[:5]])
                data_samples[col] = sample_vals
        
        dataset_info['data_samples'] = data_samples
        
        # Criar hash
        info_str = json.dumps(dataset_info, sort_keys=True)
        return hashlib.md5(info_str.encode()).hexdigest()
    
    def _compute_column_hash(self, series: pd.Series) -> str:
        """Computa hash de uma coluna espec√≠fica."""
        col_info = {
            'dtype': str(series.dtype),
            'shape': series.shape,
            'nunique': int(series.nunique()),  # Converter para int
            'null_count': int(series.isna().sum()),  # Converter para int
            'value_counts': series.value_counts().head(20).to_dict() if series.nunique() <= 100 else None,
            'stats': {
                'mean': float(series.mean()) if pd.api.types.is_numeric_dtype(series) else None,
                'std': float(series.std()) if pd.api.types.is_numeric_dtype(series) else None
            }
        }
        
        # Converter valores do value_counts para tipos serializ√°veis
        if col_info['value_counts']:
            col_info['value_counts'] = {
                str(k): int(v) if isinstance(v, (np.integer, np.int64)) else float(v)
                for k, v in col_info['value_counts'].items()
            }
        
        info_str = json.dumps(col_info, sort_keys=True, default=str)
        return hashlib.md5(info_str.encode()).hexdigest()
    
    def _compute_params_hash(self) -> str:
        """Computa hash dos par√¢metros do classificador."""
        params_str = json.dumps(self._classifier_params, sort_keys=True)
        return hashlib.md5(params_str.encode()).hexdigest()
    
    def _load_classification_cache(self) -> bool:
        """Carrega cache de classifica√ß√µes se existir e for v√°lido."""
        if not self.use_classification_cache:
            return False
            
        try:
            if os.path.exists(self.classification_cache_path):
                with open(self.classification_cache_path, 'r') as f:
                    self.classification_cache = json.load(f)
                print(f"‚úì Cache de classifica√ß√µes carregado de: {self.classification_cache_path}")
                return True
            else:
                print(f"‚ÑπÔ∏è Cache n√£o encontrado em: {self.classification_cache_path}")
                return False
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao carregar cache: {e}. Continuando sem cache...")
            self.classification_cache = None
            return False
    
    def _save_classification_cache(self, df: pd.DataFrame, classifications: Dict[str, Dict[str, Any]]):
        """Salva cache de classifica√ß√µes."""
        if not self.use_classification_cache:
            return
            
        try:
            # Preparar cache
            cache_data = {
                'version': self.VERSION,
                'classifier_params': self._classifier_params,
                'params_hash': self._compute_params_hash(),
                'dataset_hash': self._compute_dataset_hash(df),
                'timestamp': datetime.now().isoformat(),
                'classifications': {}
            }
            
            # Fun√ß√£o para converter tipos numpy para Python nativos
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                return obj
            
            # Adicionar classifica√ß√µes com hash de cada coluna
            for col_name, classification in classifications.items():
                # Converter tipos numpy
                clean_classification = convert_numpy_types(classification)
                cache_data['classifications'][col_name] = {
                    **clean_classification,
                    'column_hash': self._compute_column_hash(df[col_name])
                }
            
            # Salvar
            with open(self.classification_cache_path, 'w') as f:
                json.dump(cache_data, f, indent=2, default=str)
            
            print(f"‚úì Cache de classifica√ß√µes salvo em: {self.classification_cache_path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao salvar cache: {e}")
    
    def _validate_cache_entry(self, cache_data: Dict, df: pd.DataFrame, column_name: str) -> bool:
        """Valida se uma entrada do cache ainda √© v√°lida."""
        try:
            # Verificar vers√£o
            if cache_data.get('version') != self.VERSION:
                return False
            
            # Verificar par√¢metros
            if cache_data.get('params_hash') != self._compute_params_hash():
                return False
            
            # Verificar se a coluna existe no cache
            if column_name not in cache_data.get('classifications', {}):
                return False
            
            # Verificar hash da coluna
            cached_column = cache_data['classifications'][column_name]
            current_hash = self._compute_column_hash(df[column_name])
            
            if cached_column.get('column_hash') != current_hash:
                return False
            
            return True
            
        except Exception:
            return False
    
    def classify_dataframe(self, df: pd.DataFrame, 
                          parallel: bool = False,
                          force_reclassify: bool = False) -> Dict[str, Dict[str, Any]]:
        """
        Classifica todas as colunas de um DataFrame.
        
        Args:
            df: DataFrame a classificar
            parallel: Se True, usa processamento paralelo
            force_reclassify: Se True, ignora cache e reclassifica tudo
        """
        results = {}
        llm_count = 0
        cache_used = False
        
        print("\n" + "="*60)
        print("CLASSIFICA√á√ÉO DE TIPOS DE COLUNAS")
        if self.use_llm:
            print(f"Modo: H√≠brido (Regras + LLM)")
            print(f"LLM: {self.llm_model}")
        else:
            print("Modo: Apenas Regras")
        
        # Tentar carregar cache
        if not force_reclassify and self._load_classification_cache():
            if self.classification_cache:
                # Verificar se o cache √© v√°lido para este dataset
                dataset_hash = self._compute_dataset_hash(df)
                cached_dataset_hash = self.classification_cache.get('dataset_hash')
                
                if dataset_hash == cached_dataset_hash:
                    print("‚úì Cache v√°lido para o dataset completo")
                    cache_used = True
                else:
                    print("‚ö†Ô∏è Dataset mudou, cache ser√° atualizado")
        
        print("="*60)
        
        # Processar cada coluna
        for col in df.columns:
            # Verificar cache primeiro
            if cache_used and self._validate_cache_entry(self.classification_cache, df, col):
                # Usar classifica√ß√£o do cache
                cached_result = self.classification_cache['classifications'][col].copy()
                # Remover column_hash do resultado
                cached_result.pop('column_hash', None)
                cached_result['metadata'] = cached_result.get('metadata', {})
                cached_result['metadata']['source'] = 'cache'
                results[col] = cached_result
                self.cache_hits += 1
                print(f"‚úì {col}: {cached_result['type']} (cache)")
            else:
                # Classificar normalmente
                self.cache_misses += 1
                results[col] = self.classify_column(df[col], col)
            if 'llm_model' in results[col].get('metadata', {}):
                llm_count += 1
            print(f"‚Üí {col}: {results[col]['type']} (classificado)")
       
       # Salvar novo cache se houve classifica√ß√µes
        if self.cache_misses > 0:
            self._save_classification_cache(df, results)
        
        # Resumo
        print(f"\nüìä Resumo:")
        print(f"   Total: {len(df.columns)} colunas")
        if self.use_classification_cache:
            print(f"   Do cache: {self.cache_hits}")
            print(f"   Classificadas: {self.cache_misses}")
        if self.use_llm:
            print(f"   Com LLM: {llm_count}")
        
        return results
   
def get_summary(self, classifications: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
       """Cria resumo das classifica√ß√µes."""
       summary_data = []
       
       for col_name, info in classifications.items():
           summary_data.append({
               'column': col_name,
               'type': info['type'],
               'confidence': info['confidence'],
               'used_llm': 'llm_model' in info.get('metadata', {}),
               'metadata': str(info.get('metadata', {}))[:100] + '...' 
                         if len(str(info.get('metadata', {}))) > 100 else str(info.get('metadata', {}))
           })
       
       return pd.DataFrame(summary_data).sort_values('column')
   
def save_to_disk(self, filepath: str):
       """
       Salva o classificador e seu cache em disco.
       √ötil para salvar junto com o modelo.
       """
       save_data = {
           'classifier_params': self._classifier_params,
           'classification_cache_path': self.classification_cache_path,
           'cache_stats': {
               'hits': self.cache_hits,
               'misses': self.cache_misses
           }
       }
       
       # Se tiver cache carregado, incluir
       if self.classification_cache:
           save_data['classification_cache'] = self.classification_cache
       
       with open(filepath, 'w') as f:
           json.dump(save_data, f, indent=2)
       
       print(f"‚úì Classificador salvo em: {filepath}")
   
@classmethod
def load_from_disk(cls, filepath: str, **override_params):
    """
    Carrega classificador do disco.
    
    Args:
        filepath: Caminho do arquivo
        **override_params: Par√¢metros para sobrescrever
    """
    with open(filepath, 'r') as f:
        save_data = json.load(f)
    
    # Extrair par√¢metros
    params = save_data['classifier_params'].copy()
    params.pop('version', None)  # Remover vers√£o dos params
    
    # Aplicar overrides
    params.update(override_params)
    
    # Criar inst√¢ncia
    classifier = cls(**params)
    
    # Restaurar cache se dispon√≠vel
    if 'classification_cache' in save_data:
        classifier.classification_cache = save_data['classification_cache']
        print(f"‚úì Cache de classifica√ß√µes restaurado")
    
    return classifier
   
def get_cache_statistics(self) -> Dict[str, Any]:
    """Retorna estat√≠sticas do cache."""
    total = self.cache_hits + self.cache_misses
    hit_rate = (self.cache_hits / total * 100) if total > 0 else 0
    
    return {
        'cache_hits': self.cache_hits,
        'cache_misses': self.cache_misses,
        'total_classifications': total,
        'hit_rate': hit_rate
    }


# Fun√ß√£o de conveni√™ncia
def auto_detect_column_types(df: pd.DataFrame, 
                          classifier: Optional[ColumnTypeClassifier] = None,
                          use_llm: bool = True,
                          use_cache: bool = True,
                          cache_path: Optional[str] = None,
                          verbose: bool = True) -> Dict[str, Dict[str, Any]]:
   """
   Detecta tipos de colunas automaticamente.
   
   Args:
       df: DataFrame a analisar
       classifier: Inst√¢ncia do classificador (cria nova se None)
       use_llm: Se True, usa LLM para casos amb√≠guos
       use_cache: Se True, usa cache de classifica√ß√µes
       cache_path: Caminho espec√≠fico para o cache
       verbose: Se True, imprime resumo
   """
   if classifier is None:
       classifier = ColumnTypeClassifier(
           use_llm=use_llm,
           use_classification_cache=use_cache,
           classification_cache_path=cache_path,
           llm_model="llama3.2:3b"  # Modelo recomendado para M1
       )
   
   classifications = classifier.classify_dataframe(df)
   
   if verbose:
       type_counts = Counter(info['type'] for info in classifications.values())
       
       print("\n" + "="*60)
       print("RESUMO DA CLASSIFICA√á√ÉO")
       print("="*60)
       print(f"Total de colunas: {len(classifications)}")
       print("\nDistribui√ß√£o de tipos:")
       for col_type, count in type_counts.most_common():
           print(f"  {col_type:15} : {count:3} colunas")
       
       # Estat√≠sticas de cache
       if classifier.use_classification_cache:
           cache_stats = classifier.get_cache_statistics()
           print(f"\nEstat√≠sticas de Cache:")
           print(f"  Taxa de acerto: {cache_stats['hit_rate']:.1f}%")
           print(f"  Hits: {cache_stats['cache_hits']}")
           print(f"  Misses: {cache_stats['cache_misses']}")
       
       low_confidence = [
           (col, info['confidence']) 
           for col, info in classifications.items() 
           if info['confidence'] < 0.7
       ]
       
       if low_confidence:
           print(f"\nBaixa confian√ßa (<0.7): {len(low_confidence)} colunas")
           for col, conf in sorted(low_confidence, key=lambda x: x[1]):
               print(f"  {col}: {conf:.2f}")
       
       print("="*60)
   
   return classifications


# Teste
if __name__ == "__main__":
   print("Testando classificador com melhorias...")
   
   try:
       # Tentar dados reais
       df = pd.read_csv("/Users/ramonmoreira/desktop/smart_ads/data/new/01_split/train.csv")
       print(f"‚úì Dados carregados: {df.shape}")
       
       # Limpar cache antigo
       cache_path = "/Users/ramonmoreira/desktop/smart_ads/cache/column_classifications.json"
       if os.path.exists(cache_path):
           os.remove(cache_path)
           print("‚úì Cache antigo removido")
       
       # Teste 1: Primeira execu√ß√£o com melhorias
       print("\n1. PRIMEIRA EXECU√á√ÉO (com melhorias):")
       classifier = ColumnTypeClassifier(
           use_llm=True,
           use_classification_cache=True,
           classification_cache_path=cache_path,
           fail_on_llm_error=False,
           llm_model="llama3.2:3b",  # Ou "gemma:2b" se preferir
           use_domain_knowledge=True,  # IMPORTANTE: Usar conhecimento de dom√≠nio
           # Thresholds otimizados
           text_min_avg_length=20,
           text_min_unique_ratio=0.5,
           text_min_words=3,
           date_detection_threshold=0.7,
           llm_confidence_threshold=0.6
       )
       
       import time
       start_time = time.time()
       results1 = classifier.classify_dataframe(df)
       time1 = time.time() - start_time
       print(f"‚è±Ô∏è Tempo: {time1:.2f}s")
       
       # Verifica√ß√£o das classifica√ß√µes corretas
       print("\n2. VERIFICA√á√ÉO DAS CLASSIFICA√á√ïES:")
       expected = {
           'DATA': 'datetime',
           'Marca temporal': 'datetime',
           '¬øC√≥mo te llamas?': 'text',
           '¬øCu√°l es tu instagram?': 'text',
           '¬øCu√°l es tu profesi√≥n?': 'text',
           'Cuando hables ingl√©s con fluidez, ¬øqu√© cambiar√° en tu vida? ¬øQu√© oportunidades se abrir√°n para ti?': 'text',
           '¬øQu√© esperas aprender en el evento Cero a Ingl√©s Fluido?': 'text',
           'D√©jame un mensaje': 'text'
       }
       
       correct_count = 0
       for col, expected_type in expected.items():
           if col in results1:
               actual_type = results1[col]['type']
               status = "‚úì" if actual_type == expected_type else "‚ùå"
               print(f"{status} {col}: {actual_type} (esperado: {expected_type})")
               if actual_type == expected_type:
                   correct_count += 1
       
       accuracy = (correct_count / len(expected)) * 100
       print(f"\nPrecis√£o: {accuracy:.1f}% ({correct_count}/{len(expected)} corretos)")
       
       # Mostrar distribui√ß√£o de tipos
       print("\n3. DISTRIBUI√á√ÉO DE TIPOS:")
       type_counts = Counter(info['type'] for info in results1.values())
       for type_name, count in type_counts.most_common():
           print(f"  {type_name}: {count} colunas")
       
       # Salvar classificador
       print("\n4. SALVANDO CLASSIFICADOR:")
       classifier.save_to_disk("/Users/ramonmoreira/desktop/smart_ads/cache/classifier_config_v3.json")
       
   except FileNotFoundError:
       print("Arquivo n√£o encontrado. Usando dados de exemplo...")
       
       # Dados de exemplo
       test_data = {
           'DATA': ['15/01/2024', '16/01/2024'] * 50,
           'Marca temporal': ['2024-01-15 10:30:00', '2024-01-16 14:45:00'] * 50,
           'UTM_SOURCE': ['google', 'facebook'] * 50,
           '¬øC√≥mo te llamas?': ['Juan P√©rez Garc√≠a', 'Mar√≠a L√≥pez Fern√°ndez'] * 50,
           '¬øCu√°l es tu edad?': ['25-34', '35-44'] * 50,
           '¬øCu√°l es tu instagram?': ['@juanperez', '@marialopez'] * 50,
           '¬øCu√°l es tu profesi√≥n?': ['Ingeniero de software', 'Doctora'] * 50,
           'Cuando hables ingl√©s con fluidez, ¬øqu√© cambiar√° en tu vida? ¬øQu√© oportunidades se abrir√°n para ti?': [
               'Podr√© conseguir un mejor trabajo y viajar m√°s. Tambi√©n podr√© comunicarme mejor.',
               'Tendr√© m√°s oportunidades laborales y podr√© estudiar en el extranjero.'
           ] * 50,
           'target': [0, 1] * 50
       }
       
       df = pd.DataFrame(test_data)
       
       # Testar com dados de exemplo
       print("\nTestando com dados de exemplo...")
       classifier = ColumnTypeClassifier(
           use_llm=False,  # Sem LLM para teste r√°pido
           use_classification_cache=True,
           use_domain_knowledge=True
       )
       
       results = auto_detect_column_types(df, classifier=classifier)