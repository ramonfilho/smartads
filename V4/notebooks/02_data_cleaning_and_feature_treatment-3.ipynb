{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8038efd-b3f3-40e5-bcef-e58a0c3b9b27",
   "metadata": {},
   "source": [
    "# Quality features treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33e8d9f-5d62-4b89-8c6b-845560193f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando dataset 01_data_collection_and_integration.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_8434/925404849.py:7: DtypeWarning: Columns (18,21,23,24,26,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"datasets/01_data_collection_and_integration.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado: 138820 linhas, 48 colunas\n",
      "\n",
      "Verificando status das colunas de qualidade...\n",
      "Variantes de 'Qualidade' encontradas: 8\n",
      "  - Qualidade (Nome): 49143 valores não-nulos\n",
      "  - Qualidade (Número): 17846 valores não-nulos\n",
      "  - Qualidade (Numero): 31297 valores não-nulos\n",
      "  - Qualidade (nome): 19796 valores não-nulos\n",
      "  - Qualidade (número): 19796 valores não-nulos\n",
      "  - Qualidade: 43392 valores não-nulos\n",
      "  - Qualidade (Número) : 17597 valores não-nulos\n",
      "  - Qualidade (Nome) : 17597 valores não-nulos\n",
      "\n",
      "Consolidando colunas de qualidade...\n",
      "\n",
      "Consolidando 4 colunas numéricas:\n",
      "  - Qualidade (Número)\n",
      "  - Qualidade (Numero)\n",
      "  - Qualidade (número)\n",
      "  - Qualidade (Número) \n",
      "    Preenchendo 31297 valores de 'Qualidade (Numero)'\n",
      "    Preenchendo 19796 valores de 'Qualidade (número)'\n",
      "    Preenchendo 17846 valores de 'Qualidade (Número)'\n",
      "    Preenchendo 17597 valores de 'Qualidade (Número) '\n",
      "  - qualidade_numerica criada com 86536 valores não-nulos\n",
      "\n",
      "Consolidando 4 colunas textuais:\n",
      "  - Qualidade (Nome)\n",
      "  - Qualidade (nome)\n",
      "  - Qualidade\n",
      "  - Qualidade (Nome) \n",
      "    Preenchendo 49143 valores de 'Qualidade (Nome)'\n",
      "    Preenchendo 43392 valores de 'Qualidade'\n",
      "    Preenchendo 19796 valores de 'Qualidade (nome)'\n",
      "    Preenchendo 17597 valores de 'Qualidade (Nome) '\n",
      "  - qualidade_textual criada com 129928 valores não-nulos\n",
      "\n",
      "Removendo colunas originais...\n",
      "  - 8 colunas originais removidas\n",
      "\n",
      "Salvando dataset atualizado...\n",
      "Dataset salvo como '01_data_collection_and_integration.csv' com 42 colunas\n",
      "  - Inclui apenas 'qualidade_numerica' e 'qualidade_textual' como colunas de qualidade\n",
      "\n",
      "Primeiras linhas do dataset atualizado:\n",
      "            Marca temporal  ¿Cómo te llamas? ¿Cuál es tu género?  \\\n",
      "0  2024-07-01 09:31:54.872         Maricela                Mujer   \n",
      "1  2024-07-01 09:32:40.902             Diana               Mujer   \n",
      "2  2024-07-01 09:33:03.452  Macarena Larrea                Mujer   \n",
      "3  2024-07-01 09:33:03.452  Macarena Larrea                Mujer   \n",
      "4  2024-07-01 09:33:56.284              Luis              Hombre   \n",
      "\n",
      "   ¿Cuál es tu edad? ¿Cual es tu país?                     email  \\\n",
      "0          Mas de 54            México        marirero@gmail.com   \n",
      "1          Mas de 54          Colombia    ramirezdiana5@yahoo.es   \n",
      "2          Mas de 54           Ecuador  macarenalarrea@yahoo.com   \n",
      "3          Mas de 54           Ecuador  macarenalarrea@yahoo.com   \n",
      "4  45 años a 54 años    Estados Unidos  luissantelly@hotmail.com   \n",
      "\n",
      "  ¿Cual es tu telefono? ¿Cuál es tu instagram?  \\\n",
      "0            5523815792  Maricela Reyes Rosas    \n",
      "1            2104098048          Ramirezdiana5   \n",
      "2            0984486631        @macarenalarrea   \n",
      "3            0984486631        @macarenalarrea   \n",
      "4          +17542079009          Luissantelly    \n",
      "\n",
      "             ¿Hace quánto tiempo me conoces?  \\\n",
      "0  Te acabo de conocer a través del anuncio.   \n",
      "1  Te acabo de conocer a través del anuncio.   \n",
      "2  Te acabo de conocer a través del anuncio.   \n",
      "3  Te acabo de conocer a través del anuncio.   \n",
      "4  Te acabo de conocer a través del anuncio.   \n",
      "\n",
      "  ¿Cuál es tu disponibilidad de tiempo para estudiar inglés?  ... Unnamed: 8  \\\n",
      "0                                      1 hora al día          ...        NaN   \n",
      "1                             Menos de 1 hora al día          ...        NaN   \n",
      "2                                      1 hora al día          ...        NaN   \n",
      "3                                      1 hora al día          ...        NaN   \n",
      "4                                      1 hora al día          ...        NaN   \n",
      "\n",
      "  UTM_CAMPAIGN2 Unnamed: 10 UTM_CAMPAIGN (VINI) Unnamed: 9 Unnamed: 11  \\\n",
      "0           NaN         NaN                 NaN        NaN         NaN   \n",
      "1           NaN         NaN                 NaN        NaN         NaN   \n",
      "2           NaN         NaN                 NaN        NaN         NaN   \n",
      "3           NaN         NaN                 NaN        NaN         NaN   \n",
      "4           NaN         NaN                 NaN        NaN         NaN   \n",
      "\n",
      "  Unnamed: 12 UTM_CAMPAIGN(CORRECTA) qualidade_numerica  qualidade_textual  \n",
      "0         NaN                    NaN                NaN              média  \n",
      "1         NaN                    NaN                NaN               alta  \n",
      "2         NaN                    NaN                NaN              média  \n",
      "3         NaN                    NaN                NaN              média  \n",
      "4         NaN                    NaN                NaN               alta  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Carregar o dataset\n",
    "print(\"\\nCarregando dataset 01_data_collection_and_integration.csv...\")\n",
    "try:\n",
    "    df = pd.read_csv(\"datasets/01_data_collection_and_integration.csv\")\n",
    "    print(f\"Dataset carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo data_collection_and_integration.csv não encontrado.\")\n",
    "    raise Exception(\"Arquivo de dados não encontrado\")\n",
    "\n",
    "# 2. Verificar se as colunas de qualidade já foram processadas\n",
    "print(\"\\nVerificando status das colunas de qualidade...\")\n",
    "\n",
    "# Verificar se as colunas consolidadas já existem\n",
    "has_numeric = 'qualidade_numerica' in df.columns\n",
    "has_textual = 'qualidade_textual' in df.columns\n",
    "\n",
    "# Listar todas as variantes das colunas de qualidade\n",
    "quality_variants = [\n",
    "    'Qualidade (Nome)', 'Qualidade (Número)', 'Qualidade (Numero)', \n",
    "    'Qualidade (nome)', 'Qualidade (número)', 'Qualidade (Nombre)', \n",
    "    'Qualidade', 'Qualidade (Número) ', 'Qualidade (Nome) '\n",
    "]\n",
    "\n",
    "# Checar quais variantes existem no dataset\n",
    "existing_variants = [col for col in quality_variants if col in df.columns]\n",
    "print(f\"Variantes de 'Qualidade' encontradas: {len(existing_variants)}\")\n",
    "for col in existing_variants:\n",
    "    print(f\"  - {col}: {df[col].notna().sum()} valores não-nulos\")\n",
    "\n",
    "# 3. Determinar próximos passos com base na análise\n",
    "if len(existing_variants) == 0 and not (has_numeric or has_textual):\n",
    "    print(\"\\nNenhuma coluna de qualidade encontrada no dataset.\")\n",
    "    \n",
    "    # Verificar se talvez as colunas foram renomeadas\n",
    "    possible_renamed = [col for col in df.columns if 'qual' in col.lower()]\n",
    "    if possible_renamed:\n",
    "        print(\"Possíveis colunas relacionadas encontradas:\")\n",
    "        for col in possible_renamed:\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\nVamos criar colunas vazias para qualidade_numerica e qualidade_textual.\")\n",
    "    # Criar colunas vazias\n",
    "    df['qualidade_numerica'] = np.nan\n",
    "    df['qualidade_textual'] = None\n",
    "    \n",
    "elif has_numeric and has_textual:\n",
    "    print(\"\\nAs colunas consolidadas já existem:\")\n",
    "    print(f\"  - qualidade_numerica: {df['qualidade_numerica'].notna().sum()} valores não-nulos\")\n",
    "    print(f\"  - qualidade_textual: {df['qualidade_textual'].notna().sum()} valores não-nulos\")\n",
    "    \n",
    "    # Remover as colunas originais se ainda existirem\n",
    "    if existing_variants:\n",
    "        print(\"\\nRemovendo colunas originais redundantes...\")\n",
    "        df = df.drop(columns=existing_variants)\n",
    "        print(f\"  - {len(existing_variants)} colunas originais removidas\")\n",
    "    \n",
    "else:\n",
    "    # Processar e consolidar as colunas de qualidade\n",
    "    print(\"\\nConsolidando colunas de qualidade...\")\n",
    "    \n",
    "    # Separar colunas numéricas e textuais\n",
    "    numeric_cols = []\n",
    "    text_cols = []\n",
    "    \n",
    "    for col in existing_variants:\n",
    "        if 'nome' in col.lower() or 'name' in col.lower() or 'nombre' in col.lower():\n",
    "            text_cols.append(col)\n",
    "        else:\n",
    "            # Tentar converter para confirmar se é numérica\n",
    "            try:\n",
    "                numeric_test = pd.to_numeric(df[col], errors='coerce')\n",
    "                # Se pelo menos 50% dos valores não-nulos converteram, considerar numérica\n",
    "                not_null = df[col].notna().sum()\n",
    "                if not_null > 0 and numeric_test.notna().sum() / not_null >= 0.5:\n",
    "                    numeric_cols.append(col)\n",
    "                else:\n",
    "                    text_cols.append(col)\n",
    "            except:\n",
    "                text_cols.append(col)\n",
    "    \n",
    "    # Consolidar colunas numéricas\n",
    "    if numeric_cols:\n",
    "        print(f\"\\nConsolidando {len(numeric_cols)} colunas numéricas:\")\n",
    "        for col in numeric_cols:\n",
    "            print(f\"  - {col}\")\n",
    "        \n",
    "        df['qualidade_numerica'] = np.nan\n",
    "        # Preencher em ordem de prioridade (mais valores não-nulos primeiro)\n",
    "        numeric_cols_sorted = sorted(numeric_cols, key=lambda x: df[x].notna().sum(), reverse=True)\n",
    "        \n",
    "        for col in numeric_cols_sorted:\n",
    "            mask = df['qualidade_numerica'].isna() & df[col].notna()\n",
    "            if mask.sum() > 0:\n",
    "                df.loc[mask, 'qualidade_numerica'] = pd.to_numeric(df.loc[mask, col], errors='coerce')\n",
    "                print(f\"    Preenchendo {mask.sum()} valores de '{col}'\")\n",
    "        \n",
    "        print(f\"  - qualidade_numerica criada com {df['qualidade_numerica'].notna().sum()} valores não-nulos\")\n",
    "    \n",
    "    # Consolidar colunas textuais\n",
    "    if text_cols:\n",
    "        print(f\"\\nConsolidando {len(text_cols)} colunas textuais:\")\n",
    "        for col in text_cols:\n",
    "            print(f\"  - {col}\")\n",
    "        \n",
    "        df['qualidade_textual'] = None\n",
    "        # Preencher em ordem de prioridade\n",
    "        text_cols_sorted = sorted(text_cols, key=lambda x: df[x].notna().sum(), reverse=True)\n",
    "        \n",
    "        for col in text_cols_sorted:\n",
    "            mask = df['qualidade_textual'].isna() & df[col].notna()\n",
    "            if mask.sum() > 0:\n",
    "                df.loc[mask, 'qualidade_textual'] = df.loc[mask, col]\n",
    "                print(f\"    Preenchendo {mask.sum()} valores de '{col}'\")\n",
    "        \n",
    "        print(f\"  - qualidade_textual criada com {df['qualidade_textual'].notna().sum()} valores não-nulos\")\n",
    "    \n",
    "    # Remover as colunas originais\n",
    "    print(\"\\nRemovendo colunas originais...\")\n",
    "    df = df.drop(columns=existing_variants)\n",
    "    print(f\"  - {len(existing_variants)} colunas originais removidas\")\n",
    "\n",
    "# 4. Salvar o dataset atualizado\n",
    "print(\"\\nSalvando dataset atualizado...\")\n",
    "df.to_csv(\"datasets/01_data_collection_and_integration.csv\", index=False)\n",
    "print(f\"Dataset salvo como '01_data_collection_and_integration.csv' com {df.shape[1]} colunas\")\n",
    "print(\"  - Inclui apenas 'qualidade_numerica' e 'qualidade_textual' como colunas de qualidade\")\n",
    "\n",
    "# 5. Mostrar as primeiras linhas para verificação\n",
    "print(\"\\nPrimeiras linhas do dataset atualizado:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506184d-583b-4284-a459-f09f90ff1ab6",
   "metadata": {},
   "source": [
    "# Data Cleaning, Preprocessing\n",
    "1. Handle missing values\n",
    "2. Remove or treat outliers\n",
    "3. Normalize/standardize values\n",
    "4. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d774c2-9b3c-40fe-8571-06d744b67b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset...\n",
      "Dataset carregado: 138820 linhas, 42 colunas\n",
      "Tratamento de duplicatas...\n",
      "Identificados 47832 registros com emails duplicados\n",
      "Separados 1880 compradores e 136940 não-compradores\n",
      "Removendo apenas duplicatas entre não-compradores...\n",
      "Removidas 32207 duplicatas de não-compradores\n",
      "Dataset após tratamento de duplicatas: 106613 linhas, 42 colunas\n",
      "\n",
      "Analisando valores ausentes...\n",
      "Top 10 colunas com mais valores ausentes:\n",
      "  UTM_CAMPAIGN(CORRECTA): 106613.0 ausentes (100.00%)\n",
      "  Unnamed: 12: 106613.0 ausentes (100.00%)\n",
      "  Unnamed: 11: 106613.0 ausentes (100.00%)\n",
      "  Unnamed: 9: 106613.0 ausentes (100.00%)\n",
      "  Unnamed: 8: 106613.0 ausentes (100.00%)\n",
      "  Unnamed: 10: 105644.0 ausentes (99.09%)\n",
      "  UTM_CAMPAIGN (VINI): 96190.0 ausentes (90.22%)\n",
      "  UTM_CAMPAIGN2: 94410.0 ausentes (88.55%)\n",
      "  ¿Qué esperas aprender en la Inmersión Desbloquea Tu Inglés En 72 horas?: 92615.0 ausentes (86.87%)\n",
      "  GCLID: 90053.0 ausentes (84.47%)\n",
      "\n",
      "Aplicando estratégias para valores ausentes...\n",
      "Removendo 6 colunas com >95% ausência:\n",
      "  UTM_CAMPAIGN(CORRECTA): 100.00% ausentes\n",
      "  Unnamed: 12: 100.00% ausentes\n",
      "  Unnamed: 11: 100.00% ausentes\n",
      "  Unnamed: 9: 100.00% ausentes\n",
      "  Unnamed: 8: 100.00% ausentes\n",
      "  ... e 1 outras colunas\n",
      "  Preenchidos valores ausentes em 'email_utm' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_CAMPAING' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_SOURCE' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_MEDIUM' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_CONTENT' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_TERM' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'lançamento_utm' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_CAMPAIGN2' com 'unknown'\n",
      "  Preenchidos valores ausentes em 'UTM_CAMPAIGN (VINI)' com 'unknown'\n",
      "  Preenchidos 5 valores ausentes em 'Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?' com string vazia\n",
      "  Preenchidos 14019 valores ausentes em '¿Qué esperas aprender en la Semana de Cero a Inglés Fluido?' com string vazia\n",
      "  Preenchidos 124 valores ausentes em 'Déjame un mensaje' com string vazia\n",
      "  Preenchidos 92615 valores ausentes em '¿Qué esperas aprender en la Inmersión Desbloquea Tu Inglés En 72 horas?' com string vazia\n",
      "  Coluna 'qualidade_textual' contém valores não numéricos, será tratada como categórica\n",
      "  Preenchidos 36851 valores ausentes em 'qualidade_numerica' com mediana global (74.00)\n",
      "  Preenchidos 87042 valores ausentes em 'teste' com mediana (65.0)\n",
      "\n",
      "Identificando e tratando outliers...\n",
      "  'teste': 17352 outliers identificados (16.28%)\n",
      "    Aplicado capping (limite inferior: 65.00, superior: 65.00)\n",
      "  'qualidade_numerica': 1550 outliers identificados (1.45%)\n",
      "    Aplicado capping conservador (limite inferior: 40.00, superior: 95.00)\n",
      "\n",
      "Normalizando valores numéricos...\n",
      "Normalizadas 1 colunas numéricas com variabilidade suficiente:\n",
      "  qualidade_numerica: média=72.80, std=10.94 → média=0.00, std=1.00\n",
      "\n",
      "Convertendo tipos de dados...\n",
      "  'Marca temporal' convertido para datetime\n",
      "  'DATA' convertido para datetime\n",
      "\n",
      "Resumo do dataset pós-processamento:\n",
      "Dimensões: 106613 linhas, 36 colunas\n",
      "Valores ausentes restantes: 152558\n",
      "Detalhes dos valores ausentes restantes:\n",
      "  ¿Cual es tu telefono?: 24 valores ausentes\n",
      "  ¿Cuál es tu instagram?: 591 valores ausentes\n",
      "  ¿Cuál es tu profesión?: 78 valores ausentes\n",
      "  email_norm: 1 valores ausentes\n",
      "  DATA: 61811 valores ausentes\n",
      "  GCLID: 90053 valores ausentes\n",
      "\n",
      "Distribuição da variável target:\n",
      "  0 (não converteu): 104733 (98.24%)\n",
      "  1 (converteu): 1880 (1.76%)\n",
      "\n",
      "Estatísticas por lançamento:\n",
      "  L16: 17142 registros, 294 conversões (1.72%)\n",
      "  L17: 19594 registros, 362 conversões (1.85%)\n",
      "  L18: 14719 registros, 210 conversões (1.43%)\n",
      "  L19: 13980 registros, 179 conversões (1.28%)\n",
      "  L20: 15423 registros, 290 conversões (1.88%)\n",
      "  L21: 25755 registros, 545 conversões (2.12%)\n",
      "\n",
      "Dataset limpo salvo em 'datasets/02_1_data_cleaned_and_preprocessed.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1 - Carregar dataset\n",
    "print(\"Carregando dataset...\")\n",
    "df = pd.read_csv(\"datasets/01_data_collection_and_integration.csv\")\n",
    "print(f\"Dataset carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "\n",
    "# 2 - Remoção seletiva de duplicatas (preservando compradores)\n",
    "print(\"Tratamento de duplicatas...\")\n",
    "duplicated_emails = df.duplicated('email_norm', keep=False)\n",
    "duplicate_count = duplicated_emails.sum()\n",
    "print(f\"Identificados {duplicate_count} registros com emails duplicados\")\n",
    "\n",
    "# Separar compradores antes do processamento\n",
    "buyers = df[df['target'] == 1].copy()\n",
    "non_buyers = df[df['target'] == 0].copy()\n",
    "print(f\"Separados {len(buyers)} compradores e {len(non_buyers)} não-compradores\")\n",
    "\n",
    "# Remover apenas duplicatas entre não-compradores\n",
    "print(\"Removendo apenas duplicatas entre não-compradores...\")\n",
    "non_buyers_dedup = non_buyers.sort_values('Marca temporal').drop_duplicates(subset=['email_norm'], keep='first')\n",
    "print(f\"Removidas {len(non_buyers) - len(non_buyers_dedup)} duplicatas de não-compradores\")\n",
    "\n",
    "# Recombinar dataset\n",
    "df = pd.concat([buyers, non_buyers_dedup], ignore_index=True)\n",
    "print(f\"Dataset após tratamento de duplicatas: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "\n",
    "# 3 - Verificar valores ausentes\n",
    "print(\"\\nAnalisando valores ausentes...\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "print(\"Top 10 colunas com mais valores ausentes:\")\n",
    "missing_df = pd.DataFrame({'Count': missing_counts, 'Percentage': missing_pct})\n",
    "missing_df = missing_df.sort_values('Percentage', ascending=False)\n",
    "for col, row in missing_df.head(10).iterrows():\n",
    "    print(f\"  {col}: {row['Count']} ausentes ({row['Percentage']:.2f}%)\")\n",
    "\n",
    "# 4 - Estratégias de tratamento para valores ausentes\n",
    "print(\"\\nAplicando estratégias para valores ausentes...\")\n",
    "\n",
    "# 4.1 - Colunas de alta ausência (>95%) - remover (elevamos o limite para 95%)\n",
    "high_missing_cols = missing_df[missing_df['Percentage'] > 95].index.tolist()\n",
    "if high_missing_cols:\n",
    "    print(f\"Removendo {len(high_missing_cols)} colunas com >95% ausência:\")\n",
    "    for col in high_missing_cols[:5]:  # Mostrar apenas as 5 primeiras para concisão\n",
    "        print(f\"  {col}: {missing_df.loc[col, 'Percentage']:.2f}% ausentes\")\n",
    "    if len(high_missing_cols) > 5:\n",
    "        print(f\"  ... e {len(high_missing_cols) - 5} outras colunas\")\n",
    "    df = df.drop(columns=high_missing_cols)\n",
    "\n",
    "# 4.2 - Colunas UTM (tratar para análise de marketing)\n",
    "utm_cols = [col for col in df.columns if col.startswith('UTM_') or 'utm' in col.lower()]\n",
    "for col in utm_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "        print(f\"  Preenchidos valores ausentes em '{col}' com 'unknown'\")\n",
    "\n",
    "# 4.3 - Dados categóricos (preencher com 'desconhecido')\n",
    "cat_cols = [\n",
    "    '¿Cuál es tu género?', '¿Cuál es tu edad?', '¿Cual es tu país?',\n",
    "    '¿Hace quánto tiempo me conoces?', '¿Cuál es tu disponibilidad de tiempo para estudiar inglés?',\n",
    "    '¿Cuál es tu sueldo anual? (en dólares)', '¿Cuánto te gustaría ganar al año?',\n",
    "    '¿Crees que aprender inglés te acercaría más al salario que mencionaste anteriormente?',\n",
    "    '¿Crees que aprender inglés puede ayudarte en el trabajo o en tu vida diaria?',\n",
    "    'Qualidade', 'lançamento'\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        na_count = df[col].isna().sum()\n",
    "        if na_count > 0:\n",
    "            df[col] = df[col].fillna('desconhecido')\n",
    "            print(f\"  Preenchidos {na_count} valores ausentes em '{col}' com 'desconhecido'\")\n",
    "\n",
    "# 4.4 - Colunas de texto livre (preencher com string vazia)\n",
    "text_cols = [\n",
    "    'Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?',\n",
    "    '¿Qué esperas aprender en la Semana de Cero a Inglés Fluido?',\n",
    "    'Déjame un mensaje',\n",
    "    '¿Qué esperas aprender en la Inmersión Desbloquea Tu Inglés En 72 horas?'\n",
    "]\n",
    "\n",
    "for col in text_cols:\n",
    "    if col in df.columns:\n",
    "        na_count = df[col].isna().sum()\n",
    "        if na_count > 0:\n",
    "            df[col] = df[col].fillna('')\n",
    "            print(f\"  Preenchidos {na_count} valores ausentes em '{col}' com string vazia\")\n",
    "\n",
    "# 4.5 - Colunas de qualidade (tratamento específico)\n",
    "# Primeiro identificar colunas de qualidade que são realmente numéricas\n",
    "quality_cols = [col for col in df.columns if 'qualidade' in col.lower() or 'qualidad' in col.lower()]\n",
    "quality_numeric_cols = []\n",
    "\n",
    "# Verificar quais colunas de qualidade são numéricas\n",
    "for col in quality_cols:\n",
    "    if col in df.columns:\n",
    "        # Tentar converter para numérico\n",
    "        try:\n",
    "            # Verificar se os valores não nulos podem ser convertidos para numérico\n",
    "            non_null_values = df[col].dropna()\n",
    "            pd.to_numeric(non_null_values, errors='raise')\n",
    "            quality_numeric_cols.append(col)\n",
    "        except:\n",
    "            print(f\"  Coluna '{col}' contém valores não numéricos, será tratada como categórica\")\n",
    "            # Se não for numérica, tratar como categórica\n",
    "            if df[col].isna().sum() > 0:\n",
    "                df[col] = df[col].fillna('desconhecido')\n",
    "\n",
    "# Processar apenas colunas de qualidade verdadeiramente numéricas\n",
    "for col in quality_numeric_cols:\n",
    "    if col in df.columns:\n",
    "        # Converter para numérico forçando valores não numéricos para NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        na_count = df[col].isna().sum()\n",
    "        if na_count > 0:\n",
    "            # Usar mediana global\n",
    "            median_value = df[col].median()\n",
    "            df[col] = df[col].fillna(median_value)\n",
    "            print(f\"  Preenchidos {na_count} valores ausentes em '{col}' com mediana global ({median_value:.2f})\")\n",
    "\n",
    "# 4.6 - Outras colunas numéricas (preencher com mediana)\n",
    "other_numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "other_numeric_cols = [col for col in other_numeric_cols \n",
    "                     if col not in ['target'] + quality_numeric_cols]\n",
    "\n",
    "for col in other_numeric_cols:\n",
    "    if col in df.columns:\n",
    "        na_count = df[col].isna().sum()\n",
    "        if na_count > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "            print(f\"  Preenchidos {na_count} valores ausentes em '{col}' com mediana ({df[col].median()})\")\n",
    "\n",
    "# 5 - Tratamento de Outliers (mais conservador para colunas de qualidade)\n",
    "print(\"\\nIdentificando e tratando outliers...\")\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['target']]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns and df[col].nunique() > 10:  # Apenas colunas com variabilidade suficiente\n",
    "        # Tratamento especial para colunas de qualidade (tratamento menos agressivo)\n",
    "        if col in quality_numeric_cols:\n",
    "            # Usar percentis 1 e 99 para preservar mais variabilidade\n",
    "            lower_bound = df[col].quantile(0.01)\n",
    "            upper_bound = df[col].quantile(0.99)\n",
    "            \n",
    "            # Contar outliers\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "            outlier_count = len(outliers)\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"  '{col}': {outlier_count} outliers identificados ({outlier_count/len(df):.2%})\")\n",
    "                \n",
    "                # Aplicar capping para valores extremos\n",
    "                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"    Aplicado capping conservador (limite inferior: {lower_bound:.2f}, superior: {upper_bound:.2f})\")\n",
    "        else:\n",
    "            # Tratamento padrão para outras colunas numéricas\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Contar outliers\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "            outlier_count = len(outliers)\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"  '{col}': {outlier_count} outliers identificados ({outlier_count/len(df):.2%})\")\n",
    "                \n",
    "                # Aplicar capping para valores extremos\n",
    "                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"    Aplicado capping (limite inferior: {lower_bound:.2f}, superior: {upper_bound:.2f})\")\n",
    "\n",
    "# 6 - Normalização de valores\n",
    "print(\"\\nNormalizando valores numéricos...\")\n",
    "# Selecionar colunas numéricas que não são target\n",
    "cols_to_normalize = [col for col in numeric_cols if col != 'target' and col in df.columns]\n",
    "\n",
    "if cols_to_normalize:\n",
    "    # Criar cópia para preservar os dados originais\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Normalizar usando StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Verificar se ainda tem variabilidade após o tratamento de outliers\n",
    "    cols_with_std = []\n",
    "    for col in cols_to_normalize:\n",
    "        if df[col].std() > 0:\n",
    "            cols_with_std.append(col)\n",
    "    \n",
    "    if cols_with_std:\n",
    "        df_norm[cols_with_std] = scaler.fit_transform(df[cols_with_std])\n",
    "    \n",
    "    print(f\"Normalizadas {len(cols_with_std)} colunas numéricas com variabilidade suficiente:\")\n",
    "    for col in cols_with_std[:5]:  # Mostrar apenas as 5 primeiras para concisão\n",
    "        print(f\"  {col}: média={df[col].mean():.2f}, std={df[col].std():.2f} → média={df_norm[col].mean():.2f}, std={df_norm[col].std():.2f}\")\n",
    "    if len(cols_with_std) > 5:\n",
    "        print(f\"  ... e {len(cols_with_std) - 5} outras colunas\")\n",
    "    \n",
    "    # Substituir o DataFrame original pelo normalizado\n",
    "    df = df_norm\n",
    "\n",
    "# 7 - Converter tipos de dados\n",
    "print(\"\\nConvertendo tipos de dados...\")\n",
    "# Data e hora\n",
    "if 'Marca temporal' in df.columns:\n",
    "    df['Marca temporal'] = pd.to_datetime(df['Marca temporal'], errors='coerce')\n",
    "    print(\"  'Marca temporal' convertido para datetime\")\n",
    "if 'DATA' in df.columns:\n",
    "    df['DATA'] = pd.to_datetime(df['DATA'], errors='coerce')\n",
    "    print(\"  'DATA' convertido para datetime\")\n",
    "\n",
    "# 8 - Verificar valores pós-processamento\n",
    "print(\"\\nResumo do dataset pós-processamento:\")\n",
    "print(f\"Dimensões: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "missing_counts = df.isnull().sum().sum()\n",
    "print(f\"Valores ausentes restantes: {missing_counts}\")\n",
    "if missing_counts > 0:\n",
    "    print(\"Detalhes dos valores ausentes restantes:\")\n",
    "    for col, count in df.isnull().sum().items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} valores ausentes\")\n",
    "\n",
    "# 9 - Verificar distribuição da variável target\n",
    "print(\"\\nDistribuição da variável target:\")\n",
    "if 'target' in df.columns:\n",
    "    value_counts = df['target'].value_counts()\n",
    "    print(f\"  0 (não converteu): {value_counts.get(0, 0)} ({value_counts.get(0, 0)/len(df):.2%})\")\n",
    "    print(f\"  1 (converteu): {value_counts.get(1, 0)} ({value_counts.get(1, 0)/len(df):.2%})\")\n",
    "\n",
    "# 10 - Estatísticas por lançamento\n",
    "if 'lançamento' in df.columns and 'target' in df.columns:\n",
    "    print(\"\\nEstatísticas por lançamento:\")\n",
    "    # Criar uma tabela pivô para exibir estatísticas sem depender de groupby.median()\n",
    "    launch_stats = pd.DataFrame({\n",
    "        'registros': df.groupby('lançamento')['target'].count(),\n",
    "        'conversoes': df.groupby('lançamento')['target'].sum()\n",
    "    })\n",
    "    launch_stats['taxa_conversao'] = (launch_stats['conversoes'] / launch_stats['registros']) * 100\n",
    "    \n",
    "    for launch, row in launch_stats.iterrows():\n",
    "        print(f\"  {launch}: {int(row['registros'])} registros, {int(row['conversoes'])} conversões ({row['taxa_conversao']:.2f}%)\")\n",
    "\n",
    "# 11 - Salvar dataset limpo\n",
    "df.to_csv(\"datasets/02_1_data_cleaned_and_preprocessed.csv\", index=False)\n",
    "print(\"\\nDataset limpo salvo em 'datasets/02_1_data_cleaned_and_preprocessed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f8c37-5f12-4a85-9b5a-2758cd7ea558",
   "metadata": {},
   "source": [
    "# Feature treatment (categorical and numerical)\n",
    "1. Do the treatment specified at EDA (column_treatment.txt) - Except the text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6aba94-bc78-4392-a3dc-96bea9b3a82a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset pré-processado...\n",
      "Dataset carregado: 106613 linhas, 34 colunas\n",
      "\n",
      "Removendo colunas irrelevantes (exceto texto)...\n",
      "Criando features derivadas (não textuais)...\n",
      "Removidas 3 colunas originais (mantendo colunas de texto)\n",
      "\n",
      "Tratando variáveis categóricas...\n",
      "mapping done\n",
      "mapping done\n",
      "mapping done\n",
      "mapping done\n",
      "mapping done\n",
      "mapping done\n",
      "mapping done\n",
      "grouping done\n",
      "Label encoding done\n",
      "grouping done\n",
      "Label encoding done\n",
      "binary done\n",
      "UTMs done\n",
      "\n",
      "Salvando dataset com features categóricas e numéricas tratadas...\n",
      "Dataset salvo com 80 colunas em '02_2_data_with_feature_treatment_num_and_cat.csv'\n",
      "\n",
      "Documentando conjunto final de features:\n",
      "\n",
      "Categóricas Codificadas (21):\n",
      "  - age_encoded\n",
      "  - time_known_encoded\n",
      "  - availability_encoded\n",
      "  - current_salary_encoded\n",
      "  - desired_salary_encoded\n",
      "  - belief_salary_encoded\n",
      "  - belief_work_encoded\n",
      "  - country_encoded\n",
      "  - profession_encoded\n",
      "  - gender_encoded\n",
      "  - ... e mais 11 features\n",
      "\n",
      "Agrupadas (10):\n",
      "  - ¿Cual es tu país?_grouped\n",
      "  - ¿Cuál es tu profesión?_grouped\n",
      "  - UTM_CAMPAING_grouped\n",
      "  - UTM_SOURCE_grouped\n",
      "  - UTM_MEDIUM_grouped\n",
      "  - UTM_CONTENT_grouped\n",
      "  - UTM_TERM_grouped\n",
      "  - UTM_CAMPAIGN2_grouped\n",
      "  - UTM_CAMPAIGN (VINI)_grouped\n",
      "  - utm_month_grouped\n",
      "\n",
      "Features Temporais (9):\n",
      "  - hour\n",
      "  - day_of_week\n",
      "  - month\n",
      "  - year\n",
      "  - hour_sin\n",
      "  - hour_cos\n",
      "  - day_sin\n",
      "  - day_cos\n",
      "  - period_of_day\n",
      "\n",
      "Features Binárias (3):\n",
      "  - has_instagram\n",
      "  - valid_phone\n",
      "  - has_gclid\n",
      "\n",
      "Outras (36):\n",
      "  - Marca temporal\n",
      "  - ¿Cuál es tu género?\n",
      "  - ¿Cuál es tu edad?\n",
      "  - ¿Cual es tu país?\n",
      "  - email\n",
      "  - ¿Hace quánto tiempo me conoces?\n",
      "  - ¿Cuál es tu disponibilidad de tiempo para estudiar inglés?\n",
      "  - Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?\n",
      "  - ¿Cuál es tu profesión?\n",
      "  - ¿Cuál es tu sueldo anual? (en dólares)\n",
      "  - ... e mais 26 features\n",
      "\n",
      "Feature engineering (sem NLP) concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1 - Carregar dados pré-processados\n",
    "print(\"Carregando dataset pré-processado...\")\n",
    "df = pd.read_csv(\"datasets/02_1_data_cleaned_and_preprocessed.csv\")\n",
    "print(f\"Dataset carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "\n",
    "# 2 - Verificar e remover colunas irrelevantes iniciais\n",
    "print(\"\\nRemovendo colunas irrelevantes (exceto texto)...\")\n",
    "# Colunas que devem ser removidas conforme especificação\n",
    "cols_to_remove = [\n",
    "    '¿Cómo te llamas?',  # Substituída por feature de comprimento\n",
    "    '¿Cual es tu telefono?',  # Substituída por indicador de validade\n",
    "    '¿Cuál es tu instagram?'  # Substituída por indicador de username válido\n",
    "]\n",
    "\n",
    "# Verificar quais colunas existem no dataframe\n",
    "cols_to_remove = [col for col in cols_to_remove if col in df.columns]\n",
    "\n",
    "# Criar as features derivadas antes de remover as originais\n",
    "print(\"Criando features derivadas (não textuais)...\")\n",
    "\n",
    "# 3 - Features de identidade\n",
    "if '¿Cómo te llamas?' in df.columns:\n",
    "    df['name_length'] = df['¿Cómo te llamas?'].str.len()\n",
    "    df['name_word_count'] = df['¿Cómo te llamas?'].str.split().str.len()\n",
    "\n",
    "if '¿Cual es tu telefono?' in df.columns:\n",
    "    # Verificar se o telefone tem pelo menos 8 dígitos\n",
    "    df['valid_phone'] = df['¿Cual es tu telefono?'].str.replace(r'\\D', '', regex=True).str.len() >= 8\n",
    "\n",
    "if '¿Cuál es tu instagram?' in df.columns:\n",
    "    df['has_instagram'] = df['¿Cuál es tu instagram?'].notna() & (df['¿Cuál es tu instagram?'] != '')\n",
    "\n",
    "# 4 - Features de tempo (Marca temporal)\n",
    "if 'Marca temporal' in df.columns:\n",
    "    # Converter para datetime se ainda não for\n",
    "    if not pd.api.types.is_datetime64_dtype(df['Marca temporal']):\n",
    "        df['Marca temporal'] = pd.to_datetime(df['Marca temporal'], errors='coerce')\n",
    "    \n",
    "    # Extrair componentes básicos\n",
    "    df['hour'] = df['Marca temporal'].dt.hour\n",
    "    df['day_of_week'] = df['Marca temporal'].dt.dayofweek\n",
    "    df['month'] = df['Marca temporal'].dt.month\n",
    "    df['year'] = df['Marca temporal'].dt.year\n",
    "    \n",
    "    # Features cíclicas para hora e dia da semana\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Período do dia\n",
    "    df['period_of_day'] = pd.cut(\n",
    "        df['hour'], \n",
    "        bins=[0, 6, 12, 18, 24], \n",
    "        labels=['madrugada', 'manha', 'tarde', 'noite']\n",
    "    )\n",
    "\n",
    "# Repetir processo para DATA se existir\n",
    "if 'DATA' in df.columns:\n",
    "    if not pd.api.types.is_datetime64_dtype(df['DATA']):\n",
    "        df['DATA'] = pd.to_datetime(df['DATA'], errors='coerce')\n",
    "    \n",
    "    # Extrair componentes apenas se conversão foi bem-sucedida\n",
    "    if not df['DATA'].isna().all():\n",
    "        df['utm_hour'] = df['DATA'].dt.hour\n",
    "        df['utm_day_of_week'] = df['DATA'].dt.dayofweek\n",
    "        df['utm_month'] = df['DATA'].dt.month\n",
    "        df['utm_year'] = df['DATA'].dt.year\n",
    "\n",
    "# 5 - Remover as colunas originais (exceto texto)\n",
    "df = df.drop(columns=cols_to_remove)\n",
    "print(f\"Removidas {len(cols_to_remove)} colunas originais (mantendo colunas de texto)\")\n",
    "\n",
    "# 6 - Tratamento de variáveis categóricas\n",
    "print(\"\\nTratando variáveis categóricas...\")\n",
    "\n",
    "# 6.1 - Label Encoding para variáveis ordinais\n",
    "ordinal_cols = [\n",
    "    '¿Cuál es tu edad?',  # Faixas etárias ordenadas\n",
    "    '¿Hace quánto tiempo me conoces?',  # Tempo ordenado\n",
    "    '¿Cuál es tu disponibilidad de tiempo para estudiar inglés?',  # Disponibilidade ordenada\n",
    "    '¿Cuál es tu sueldo anual? (en dólares)',  # Faixas de salário ordenadas\n",
    "    '¿Cuánto te gustaría ganar al año?',  # Faixas de salário desejado ordenadas\n",
    "    '¿Crees que aprender inglés te acercaría más al salario que mencionaste anteriormente?',  # Não/Talvez/Sim\n",
    "    '¿Crees que aprender inglés puede ayudarte en el trabajo o en tu vida diaria?',  # Não/Talvez/Sim\n",
    "]\n",
    "\n",
    "# 6.1.1 - Mapas específicos para garantir a ordenação correta\n",
    "age_map = {\n",
    "    '18 años a 24 años': 1,\n",
    "    '25 años a 34 años': 2,\n",
    "    '35 años a 44 años': 3,\n",
    "    '45 años a 54 años': 4,\n",
    "    'Mas de 54': 5,\n",
    "    'desconhecido': 0\n",
    "}\n",
    "\n",
    "time_map = {\n",
    "    'Te acabo de conocer a través d...': 0,\n",
    "    'Te sigo desde hace 1 mes': 1,\n",
    "    'Te sigo desde hace 3 meses': 2,\n",
    "    'Te sigo desde hace más de 5 me...': 3,\n",
    "    'Te sigo desde hace 1 año': 4,\n",
    "    'Te sigo hace más de 1 año': 5,\n",
    "    'desconhecido': -1\n",
    "}\n",
    "\n",
    "availability_map = {\n",
    "    'Menos de 1 hora al día': 0,\n",
    "    '1 hora al día': 1,\n",
    "    '2 horas al día': 2,\n",
    "    '3 horas al día': 3,\n",
    "    'Más de 3 horas al día': 4,\n",
    "    'desconhecido': -1\n",
    "}\n",
    "\n",
    "salary_map = {\n",
    "    'Menos de US$3000': 1,\n",
    "    'US$3000 a US$5000': 2,\n",
    "    'US$5000 o más': 3,\n",
    "    'US$10000 o más': 4,\n",
    "    'US$20000 o más': 5,\n",
    "    'desconhecido': 0\n",
    "}\n",
    "\n",
    "desired_salary_map = {\n",
    "    'Al menos US$ 3000 por año': 1,\n",
    "    'Más de US$5000 por año': 2,\n",
    "    'Más de US$10000 por año': 3,\n",
    "    'Más de US$20000 por año': 4,\n",
    "    'desconhecido': 0\n",
    "}\n",
    "\n",
    "belief_map = {\n",
    "    'Creo que no...': 0,\n",
    "    'Tal vez': 1,\n",
    "    '¡Sí, sin duda!': 2,\n",
    "    '¡Si por su puesto!': 2,  # Variação ortográfica do sim\n",
    "    'desconhecido': -1\n",
    "}\n",
    "\n",
    "# 6.1.2 - Aplicar mapas específicos para variáveis com ordem conhecida\n",
    "if '¿Cuál es tu edad?' in df.columns:\n",
    "    df['age_encoded'] = df['¿Cuál es tu edad?'].map(age_map)\n",
    "    print(\"mapping done\")\n",
    "\n",
    "if '¿Hace quánto tiempo me conoces?' in df.columns:\n",
    "    df['time_known_encoded'] = df['¿Hace quánto tiempo me conoces?'].map(time_map)\n",
    "    print(\"mapping done\")\n",
    "if '¿Cuál es tu disponibilidad de tiempo para estudiar inglés?' in df.columns:\n",
    "    df['availability_encoded'] = df['¿Cuál es tu disponibilidad de tiempo para estudiar inglés?'].map(availability_map)\n",
    "    print(\"mapping done\")\n",
    "if '¿Cuál es tu sueldo anual? (en dólares)' in df.columns:\n",
    "    df['current_salary_encoded'] = df['¿Cuál es tu sueldo anual? (en dólares)'].map(salary_map)\n",
    "    print(\"mapping done\")\n",
    "if '¿Cuánto te gustaría ganar al año?' in df.columns:\n",
    "    df['desired_salary_encoded'] = df['¿Cuánto te gustaría ganar al año?'].map(desired_salary_map)\n",
    "    print(\"mapping done\")\n",
    "# Ambas colunas de crença\n",
    "for col in ['¿Crees que aprender inglés te acercaría más al salario que mencionaste anteriormente?', \n",
    "            '¿Crees que aprender inglés puede ayudarte en el trabajo o en tu vida diaria?']:\n",
    "    if col in df.columns:\n",
    "        new_col = 'belief_salary_encoded' if 'salario' in col else 'belief_work_encoded'\n",
    "        df[new_col] = df[col].map(belief_map)\n",
    "        print(\"mapping done\")\n",
    "\n",
    "# 6.2 - Agrupar categorias raras em variáveis de alta cardinalidade\n",
    "nominal_high_cardinality = ['¿Cual es tu país?', '¿Cuál es tu profesión?']\n",
    "\n",
    "for col in nominal_high_cardinality:\n",
    "    if col in df.columns:\n",
    "        # Agrupar categorias raras com base em contagem absoluta\n",
    "        value_counts = df[col].value_counts()\n",
    "        threshold = 10  # categorias com menos de 10 ocorrências\n",
    "        rare_categories = [cat for cat, count in value_counts.items() if count < threshold]\n",
    "        \n",
    "        # Criar variável agrupada\n",
    "        grouped_col = col + '_grouped'\n",
    "        df[grouped_col] = df[col].apply(lambda x: 'Rare' if x in rare_categories else x)\n",
    "        print(\"grouping done\")\n",
    "        \n",
    "        # Aplicar Label Encoding na variável agrupada\n",
    "        le = LabelEncoder()\n",
    "        encoded_col = 'country_encoded' if 'país' in col else 'profession_encoded'\n",
    "        df[encoded_col] = le.fit_transform(df[grouped_col])\n",
    "        print(\"Label encoding done\")\n",
    "\n",
    "# 6.3 - Binary encoding para variáveis binárias\n",
    "if '¿Cuál es tu género?' in df.columns:\n",
    "    gender_map = {'Mujer': 1, 'Hombre': 0, 'desconhecido': -1}\n",
    "    df['gender_encoded'] = df['¿Cuál es tu género?'].map(gender_map)\n",
    "    print(\"binary done\")\n",
    "\n",
    "# 6.4 - Tratamento de UTMs\n",
    "# Identificar colunas UTM\n",
    "utm_cols = [col for col in df.columns if 'UTM_' in col or 'utm_' in col]\n",
    "\n",
    "for col in utm_cols:\n",
    "    # Verificar cardinalidade\n",
    "    cardinality = df[col].nunique()\n",
    "    \n",
    "    if cardinality <= 10:  # Baixa cardinalidade\n",
    "        # Label Encoding convertendo todos os valores para string\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col].fillna('unknown').astype(str))\n",
    "    else:  # Alta cardinalidade\n",
    "        # Vamos apenas agrupar valores raros\n",
    "        value_counts = df[col].value_counts()\n",
    "        threshold = 10\n",
    "        rare_values = [val for val, count in value_counts.items() if count < threshold]\n",
    "        df[f'{col}_grouped'] = df[col].apply(lambda x: 'Rare' if x in rare_values else x)\n",
    "        \n",
    "        # Aplicar label encoding no agrupamento\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[f'{col}_grouped'].fillna('unknown').astype(str))\n",
    "print(\"UTMs done\")\n",
    "\n",
    "# 6.5 - GCLID como indicador binário\n",
    "if 'GCLID' in df.columns:\n",
    "    df['has_gclid'] = df['GCLID'].notna().astype(int)\n",
    "\n",
    "# 8 - Salvar dataset com features engineered (não textuais)\n",
    "print(\"\\nSalvando dataset com features categóricas e numéricas tratadas...\")\n",
    "df.to_csv(\"datasets/02_2_data_with_feature_treatment_num_and_cat.csv\", index=False)\n",
    "print(f\"Dataset salvo com {df.shape[1]} colunas em 'datasets/02_2_data_with_feature_treatment_num_and_cat.csv'\")\n",
    "\n",
    "# 9 - Documentar conjunto final de features\n",
    "print(\"\\nDocumentando conjunto final de features:\")\n",
    "feature_types = {\n",
    "    'Categóricas Codificadas': [col for col in df.columns if '_encoded' in col],\n",
    "    'Agrupadas': [col for col in df.columns if '_grouped' in col],\n",
    "    'Features Temporais': ['hour', 'day_of_week', 'month', 'year', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'period_of_day'],\n",
    "    'Features Binárias': ['has_instagram', 'valid_phone', 'has_gclid'],\n",
    "    'Outras': [col for col in df.columns if not any(col in group for group in [\n",
    "        [c for c in df.columns if '_encoded' in c],\n",
    "        [c for c in df.columns if '_grouped' in c],\n",
    "        ['hour', 'day_of_week', 'month', 'year', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'period_of_day'],\n",
    "        ['has_instagram', 'valid_phone', 'has_gclid'],\n",
    "        ['target']\n",
    "    ])]\n",
    "}\n",
    "\n",
    "for feature_type, cols in feature_types.items():\n",
    "    # Filtrar apenas colunas que existem no dataframe\n",
    "    cols = [col for col in cols if col in df.columns]\n",
    "    if cols:\n",
    "        print(f\"\\n{feature_type} ({len(cols)}):\")\n",
    "        for col in cols[:10]:  # Limitar a 10 por categoria para concisão\n",
    "            print(f\"  - {col}\")\n",
    "        if len(cols) > 10:\n",
    "            print(f\"  - ... e mais {len(cols) - 10} features\")\n",
    "\n",
    "print(\"\\nFeature engineering (sem NLP) concluído com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327ad0b-d7bb-41a5-8709-7254f9fc6ef3",
   "metadata": {},
   "source": [
    "# Features treatment (text features)\n",
    "1. Using NLP and other techniques to treat text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a1d450-0089-4233-947d-da72d669bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset pré-processado...\n",
      "Dataset carregado: 106613 linhas, 80 colunas\n",
      "Colunas de texto identificadas: 4\n",
      "\n",
      "Realizando pré-processamento do texto...\n",
      "Extraindo features básicas de texto...\n",
      "Realizando análise de sentimento...\n",
      "\n",
      "Criando features baseadas em palavras-chave de motivação...\n",
      "\n",
      "Resultados da análise de texto:\n",
      "  Sentimento em 'Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?': média=0.0017, std=0.0333\n",
      "  Sentimento em '¿Qué esperas aprender en la Semana de Cero a Inglés Fluido?': média=0.0000, std=0.0437\n",
      "  Sentimento em 'Déjame un mensaje': média=0.0074, std=0.0734\n",
      "  Sentimento em '¿Qué esperas aprender en la Inmersión Desbloquea Tu Inglés En 72 horas?': média=0.0003, std=0.0173\n",
      "\n",
      "  Categorias de motivação em 'Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?':\n",
      "    - work: 65439 menções\n",
      "    - opportunity: 21541 menções\n",
      "    - improvement: 21497 menções\n",
      "    - travel: 18490 menções\n",
      "    - communication: 14480 menções\n",
      "    - education: 7197 menções\n",
      "\n",
      "  Categorias de motivação em '¿Qué esperas aprender en la Semana de Cero a Inglés Fluido?':\n",
      "    - communication: 36417 menções\n",
      "    - improvement: 17090 menções\n",
      "    - education: 16094 menções\n",
      "    - work: 987 menções\n",
      "    - opportunity: 576 menções\n",
      "    - travel: 364 menções\n",
      "\n",
      "  Categorias de motivação em 'Déjame un mensaje':\n",
      "    - education: 31923 menções\n",
      "    - opportunity: 11633 menções\n",
      "    - communication: 10472 menções\n",
      "    - improvement: 8326 menções\n",
      "    - work: 3677 menções\n",
      "    - travel: 869 menções\n",
      "\n",
      "  Categorias de motivação em '¿Qué esperas aprender en la Inmersión Desbloquea Tu Inglés En 72 horas?':\n",
      "    - communication: 6044 menções\n",
      "    - improvement: 2884 menções\n",
      "    - education: 2299 menções\n",
      "    - work: 179 menções\n",
      "    - opportunity: 97 menções\n",
      "    - travel: 75 menções\n",
      "\n",
      "Salvando dataset com features textuais e todas as demais features...\n",
      "Dataset completo salvo (152 colunas) em 'datasets/02_3_data_with_feature_treatment_nlp.csv'\n",
      "\n",
      "Processamento de features textuais concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1 - Carregar dataset pré-processado\n",
    "print(\"Carregando dataset pré-processado...\")\n",
    "df = pd.read_csv(\"datasets/02_2_data_with_feature_treatment_num_and_cat.csv\")\n",
    "print(f\"Dataset carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "\n",
    "# 2 - Identificar colunas de texto para processamento\n",
    "text_cols = [\n",
    "    'Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?',\n",
    "    '¿Qué esperas aprender en la Semana de Cero a Inglés Fluido?',\n",
    "    'Déjame un mensaje',\n",
    "    '¿Qué esperas aprender en la Inmersión Desbloquea Tu Inglés En 72 horas?'\n",
    "]\n",
    "\n",
    "# Filtrar apenas colunas existentes no dataframe\n",
    "text_cols = [col for col in text_cols if col in df.columns]\n",
    "print(f\"Colunas de texto identificadas: {len(text_cols)}\")\n",
    "\n",
    "if not text_cols:\n",
    "    print(\"Nenhuma coluna textual encontrada no dataset. Verifique os nomes das colunas.\")\n",
    "    exit()\n",
    "\n",
    "# 3 - Pré-processamento para análise de texto\n",
    "print(\"\\nRealizando pré-processamento do texto...\")\n",
    "\n",
    "# 3.1 - Função para limpar e normalizar texto\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    # Remover URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remover emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remover pontuação e caracteres especiais, mas manter espaços e letras com acentos\n",
    "    text = re.sub(r'[^\\w\\s\\á\\é\\í\\ó\\ú\\ñ\\ü]', ' ', text)\n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remover espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 3.2 - Aplicar limpeza de texto\n",
    "for col in text_cols:\n",
    "    df[f'{col}_clean'] = df[col].apply(clean_text)\n",
    "\n",
    "# 4 - Features básicas de texto\n",
    "print(\"Extraindo features básicas de texto...\")\n",
    "for col in text_cols:\n",
    "    # 4.1 - Comprimento do texto\n",
    "    df[f'{col}_length'] = df[col].str.len()\n",
    "    \n",
    "    # 4.2 - Contagem de palavras\n",
    "    df[f'{col}_word_count'] = df[col].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
    "    \n",
    "    # 4.3 - Presença de caracteres específicos\n",
    "    df[f'{col}_has_question'] = df[col].str.contains('\\?', regex=True, na=False).astype(int)\n",
    "    df[f'{col}_has_exclamation'] = df[col].str.contains('!', regex=True, na=False).astype(int)\n",
    "    \n",
    "    # 4.4 - Média do tamanho das palavras\n",
    "    df[f'{col}_avg_word_length'] = df[col].apply(\n",
    "        lambda x: np.mean([len(w) for w in str(x).split()]) if pd.notna(x) and len(str(x).split()) > 0 else 0\n",
    "    )\n",
    "\n",
    "# 5 - Análise de sentimento\n",
    "print(\"Realizando análise de sentimento...\")\n",
    "for col in text_cols:\n",
    "    # Função para análise de sentimento com TextBlob (lidar com exceções)\n",
    "    def get_sentiment(text):\n",
    "        try:\n",
    "            if not isinstance(text, str) or pd.isna(text) or text == '':\n",
    "                return 0\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    # Aplicar análise de sentimento\n",
    "    df[f'{col}_sentiment'] = df[col].apply(get_sentiment)\n",
    "\n",
    "# 6.4 - Criar features com base em palavras-chave de motivação\n",
    "print(\"\\nCriando features baseadas em palavras-chave de motivação...\")\n",
    "\n",
    "# 6.2 - Palavras-chave específicas relacionadas a motivação para aprender inglês\n",
    "motivation_keywords = {\n",
    "    'trabajo': 'work', 'empleo': 'work', 'carrera': 'work', 'profesional': 'work', \n",
    "    'puesto': 'work', 'laboral': 'work', 'sueldo': 'work', 'trabajar': 'work',\n",
    "    'viaje': 'travel', 'viajar': 'travel', 'turismo': 'travel', 'países': 'travel', \n",
    "    'extranjero': 'travel', 'mundo': 'travel', 'internacional': 'travel',\n",
    "    'comunicar': 'communication', 'comunicación': 'communication', 'hablar': 'communication', \n",
    "    'entender': 'communication', 'expresar': 'communication',\n",
    "    'estudio': 'education', 'estudiar': 'education', 'universidad': 'education', \n",
    "    'curso': 'education', 'aprender': 'education', 'educación': 'education',\n",
    "    'mejor': 'improvement', 'mejorar': 'improvement', 'crecer': 'improvement', \n",
    "    'avanzar': 'improvement', 'progresar': 'improvement', 'desarrollar': 'improvement',\n",
    "    'oportunidad': 'opportunity', 'futuro': 'opportunity', 'posibilidad': 'opportunity', \n",
    "    'chance': 'opportunity', 'opción': 'opportunity'\n",
    "}\n",
    "\n",
    "# Agrupar por categoria de motivação\n",
    "for col in text_cols:\n",
    "    # Inicializar colunas de categorias de motivação\n",
    "    for category in set(motivation_keywords.values()):\n",
    "        df[f'{col}_motiv_{category}'] = 0\n",
    "    \n",
    "    # Processar cada linha\n",
    "    for idx, text in enumerate(df[f'{col}_clean']):\n",
    "        if not isinstance(text, str) or text == '':\n",
    "            continue\n",
    "            \n",
    "        # Verificar presença de cada palavra-chave\n",
    "        for keyword, category in motivation_keywords.items():\n",
    "            if keyword in text:\n",
    "                df.loc[idx, f'{col}_motiv_{category}'] += 1\n",
    "    \n",
    "    # Normalizar por comprimento do texto (para textos não vazios)\n",
    "    mask = df[f'{col}_word_count'] > 0\n",
    "    for category in set(motivation_keywords.values()):\n",
    "        col_name = f'{col}_motiv_{category}'\n",
    "        df.loc[mask, f'{col_name}_norm'] = df.loc[mask, col_name] / df.loc[mask, f'{col}_word_count']\n",
    "\n",
    "# REMOVIDO: TF-IDF aplicado ao dataset completo (para evitar vazamento de dados)\n",
    "# REMOVIDA: Análise de poder discriminativo baseada no target\n",
    "\n",
    "# 8 - Analisar estatísticas básicas do texto (sem uso de target)\n",
    "print(\"\\nResultados da análise de texto:\")\n",
    "\n",
    "# 8.1 - Estatísticas de sentimento\n",
    "for col in text_cols:\n",
    "    sentiment_col = f'{col}_sentiment'\n",
    "    if sentiment_col in df.columns:\n",
    "        sentiment_mean = df[sentiment_col].mean()\n",
    "        sentiment_std = df[sentiment_col].std()\n",
    "        print(f\"  Sentimento em '{col}': média={sentiment_mean:.4f}, std={sentiment_std:.4f}\")\n",
    "\n",
    "# 8.2 - Categorias de motivação mais frequentes\n",
    "for col in text_cols:\n",
    "   motiv_cols = [c for c in df.columns if f'{col}_motiv_' in c and not c.endswith('_norm')]\n",
    "   if motiv_cols:\n",
    "       motiv_sums = {c.split('_')[-1]: df[c].sum() for c in motiv_cols}\n",
    "       sorted_motivs = sorted(motiv_sums.items(), key=lambda x: x[1], reverse=True)\n",
    "       print(f\"\\n  Categorias de motivação em '{col}':\")\n",
    "       for category, count in sorted_motivs:\n",
    "           if count > 0:\n",
    "               print(f\"    - {category}: {count} menções\")\n",
    "\n",
    "# 10 - Salvar dataset com features textuais\n",
    "print(\"\\nSalvando dataset com features textuais e todas as demais features...\")\n",
    "# Manter todas as colunas, exceto versões \"limpas\" do texto original\n",
    "columns_to_drop = [f'{col}_clean' for col in text_cols if f'{col}_clean' in df.columns]\n",
    "df_final = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Salvar dataset final com todas as features\n",
    "df_final.to_csv(\"datasets/02_3_data_with_feature_treatment_nlp.csv\", index=False)\n",
    "print(f\"Dataset completo salvo ({df_final.shape[1]} colunas) em 'datasets/02_3_data_with_feature_treatment_nlp.csv'\")\n",
    "\n",
    "print(\"\\nProcessamento de features textuais concluído com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbed0c-72fc-45e5-8141-dcda52e27459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
