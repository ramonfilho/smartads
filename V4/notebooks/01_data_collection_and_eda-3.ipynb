{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed7c31c-c127-499f-87b9-ca9aedccfa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Collection and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80df8879-d949-43f4-8434-cd779344b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up connection to Google Cloud Storage...\n",
      "Found 18 files\n",
      "\n",
      "Categorizing files by type and launch...\n",
      "Survey files: 6\n",
      "Buyer files: 6\n",
      "UTM files: 6\n",
      "L16 files: 3\n",
      "L17 files: 3\n",
      "L18 files: 3\n",
      "L19 files: 3\n",
      "L20 files: 3\n",
      "L21 files: 3\n",
      "\n",
      "Loading survey files...\n",
      "  - Loaded: raw_data/Pesquisa_L16.xlsx (L16), 26732 rows, 20 columns\n",
      "  - Loaded: raw_data/Pesquisa_L17.xlsx (L17), 21020 rows, 21 columns\n",
      "  - Loaded: raw_data/Pesquisa_L18.xlsx (L18), 15610 rows, 21 columns\n",
      "  - Loaded: raw_data/Pesquisa_L19.xlsx (L19), 15156 rows, 21 columns\n",
      "  - Loaded: raw_data/Pesquisa_L20.xlsx (L20), 17257 rows, 21 columns\n",
      "  - Loaded: raw_data/Pesquisa_L21.xlsx (L21), 28581 rows, 21 columns\n",
      "\n",
      "Loading buyer files...\n",
      "  - Loaded: raw_data/compradores_mario_L16.csv (L16), 756 rows, 61 columns\n",
      "  - Loaded: raw_data/compradores_mario_L17.csv (L17), 796 rows, 61 columns\n",
      "  - Loaded: raw_data/compradores_mario_L18.csv (L18), 463 rows, 61 columns\n",
      "  - Loaded: raw_data/compradores_mario_L19.csv (L19), 436 rows, 61 columns\n",
      "  - Loaded: raw_data/compradores_mario_L20.xls (L20), 614 rows, 60 columns\n",
      "  - Loaded: raw_data/compradores_mario_L21.xls (L21), 1242 rows, 60 columns\n",
      "\n",
      "Loading UTM files...\n",
      "  - Detected delimiter ',' for raw_data/L16 - UTMS.csv\n",
      "  - Loaded: raw_data/L16 - UTMS.csv (L16), 72684 rows, 9 columns\n",
      "  - Detected delimiter ',' for raw_data/L17 - UTMS.csv\n",
      "  - Loaded: raw_data/L17 - UTMS.csv (L17), 76401 rows, 9 columns\n",
      "  - Detected delimiter ',' for raw_data/L18 - UTMS.csv\n",
      "  - Loaded: raw_data/L18 - UTMS.csv (L18), 57985 rows, 9 columns\n",
      "  - Detected delimiter ',' for raw_data/L19 - UTMS.csv\n",
      "  - Loaded: raw_data/L19 - UTMS.csv (L19), 65419 rows, 12 columns\n",
      "  - Detected delimiter ',' for raw_data/L20 - UTMS.csv\n",
      "  - Loaded: raw_data/L20 - UTMS.csv (L20), 79622 rows, 14 columns\n",
      "  - Detected delimiter ';' for raw_data/L21 - UTMS.csv\n",
      "  - Loaded: raw_data/L21 - UTMS.csv (L21), 157382 rows, 10 columns\n",
      "\n",
      "Combining datasets...\n",
      "Survey data: 124356 rows, 29 columns\n",
      "Buyer data: 4307 rows, 61 columns\n",
      "UTM data: 509493 rows, 17 columns\n",
      "\n",
      "Documenting data structure...\n",
      "\n",
      "Surveys structure:\n",
      "  - Columns: 29\n",
      "  - Data types: {dtype('O'): 23, dtype('float64'): 5, dtype('<M8[ns]'): 1}\n",
      "  - Sample columns: Marca temporal, ¿Cómo te llamas?, ¿Cuál es tu género?, ¿Cuál es tu edad?, ¿Cual es tu país?\n",
      "\n",
      "Buyers structure:\n",
      "  - Columns: 61\n",
      "  - Data types: {dtype('O'): 38, dtype('float64'): 18, dtype('int64'): 5}\n",
      "  - Sample columns: Nome do Produto, Nome do Produtor, Documento do Produtor, Nome do Afiliado, Transação\n",
      "\n",
      "UTMs structure:\n",
      "  - Columns: 17\n",
      "  - Data types: {dtype('O'): 15, dtype('float64'): 2}\n",
      "  - Sample columns: DATA, email, UTM_CAMPAING, UTM_SOURCE, UTM_MEDIUM\n",
      "\n",
      "Normalizing email addresses...\n",
      "\n",
      "Matching surveys with buyers...\n",
      "Total matches found: 1646\n",
      "\n",
      "Creating target variable...\n",
      "Conversion rate: 1.27%\n",
      "Merged survey data with UTM data\n",
      "\n",
      "Final merged dataset: 138820 rows, 48 columns\n",
      "\n",
      "Registros por lançamento:\n",
      "  - L21: 31342 registros\n",
      "  - L16: 28638 registros\n",
      "  - L17: 23523 registros\n",
      "  - L20: 19887 registros\n",
      "  - L19: 17824 registros\n",
      "  - L18: 17606 registros\n",
      "\n",
      "Splitting data into train, validation, and test sets...\n",
      "Train set: 100297 rows, 48 columns\n",
      "Validation set: 17700 rows, 48 columns\n",
      "Test set: 20823 rows, 48 columns\n",
      "\n",
      "Target distribution:\n",
      "  - Train: 1.35% positive\n",
      "  - Validation: 1.36% positive\n",
      "  - Test: 1.35% positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1 - Conexão com Google Cloud Storage\n",
    "print(\"Setting up connection to Google Cloud Storage...\")\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(\"new_times\")\n",
    "\n",
    "# Listar e categorizar arquivos\n",
    "blobs = list(bucket.list_blobs(prefix=\"raw_data/\"))\n",
    "file_paths = [blob.name for blob in blobs if blob.name.endswith(('.xlsx', '.xls', '.csv'))]\n",
    "print(f\"Found {len(file_paths)} files\")\n",
    "\n",
    "# 2 - Categorizar arquivos por tipo e lançamento\n",
    "print(\"\\nCategorizing files by type and launch...\")\n",
    "survey_files = []\n",
    "buyer_files = []\n",
    "utm_files = []\n",
    "all_files_by_launch = {f\"L{i}\": [] for i in range(16, 22)}  # Para L16 até L21\n",
    "\n",
    "# Função para extrair ID de lançamento\n",
    "def extract_launch_id(filename):\n",
    "    patterns = [\n",
    "        r'L(\\d+)[_\\s\\-]',  # L16_, L16-, L16 \n",
    "        r'[_\\s\\-]L(\\d+)',  # _L16, -L16, L16\n",
    "        r'L(\\d+)\\.csv',    # L16.csv\n",
    "        r'L(\\d+)\\.xls',    # L16.xls\n",
    "        r'L(\\d+)$'         # termina com L16\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            launch_num = match.group(1)\n",
    "            return f\"L{launch_num}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Categorizar arquivos\n",
    "for file_path in file_paths:\n",
    "    # Determinar o tipo de arquivo\n",
    "    if any(keyword in file_path.lower() for keyword in ['pesquisa', 'survey', 'respuestas', 'ayudame']):\n",
    "        survey_files.append(file_path)\n",
    "    elif any(keyword in file_path.lower() for keyword in ['comprador', 'mario']):\n",
    "        buyer_files.append(file_path)\n",
    "    elif any(keyword in file_path.lower() for keyword in ['utm']):\n",
    "        utm_files.append(file_path)\n",
    "    \n",
    "    # Identificar o lançamento\n",
    "    launch_id = extract_launch_id(file_path)\n",
    "    if launch_id and launch_id in all_files_by_launch:\n",
    "        all_files_by_launch[launch_id].append(file_path)\n",
    "\n",
    "# Mostrar informações de categorização\n",
    "print(f\"Survey files: {len(survey_files)}\")\n",
    "print(f\"Buyer files: {len(buyer_files)}\")\n",
    "print(f\"UTM files: {len(utm_files)}\")\n",
    "\n",
    "for launch_id, files in all_files_by_launch.items():\n",
    "    if files:\n",
    "        print(f\"{launch_id} files: {len(files)}\")\n",
    "\n",
    "# 3 - Funções para normalizar emails\n",
    "def normalize_email(email):\n",
    "    if pd.isna(email) or email is None:\n",
    "        return None\n",
    "    \n",
    "    email_str = str(email).lower().strip().replace(\" \", \"\")\n",
    "    if '@' not in email_str:\n",
    "        return None\n",
    "    \n",
    "    username, domain = email_str.split('@', 1)\n",
    "    \n",
    "    # Gmail-specific normalization\n",
    "    if domain == 'gmail.com':\n",
    "        username = username.replace('.', '')\n",
    "    \n",
    "    # Common domain corrections\n",
    "    domain_corrections = {\n",
    "        'gmial.com': 'gmail.com', 'gmail.con': 'gmail.com', 'hotmial.com': 'hotmail.com',\n",
    "        'outlook.con': 'outlook.com', 'yahoo.con': 'yahoo.com'\n",
    "    }\n",
    "    \n",
    "    if domain in domain_corrections:\n",
    "        domain = domain_corrections[domain]\n",
    "    \n",
    "    return f\"{username}@{domain}\"\n",
    "\n",
    "# 4 - Ler e processar dados\n",
    "survey_dfs = []\n",
    "buyer_dfs = []\n",
    "utm_dfs = []\n",
    "launch_data = {launch_id: {'survey': None, 'buyer': None, 'utm': None} for launch_id in all_files_by_launch.keys()}\n",
    "\n",
    "# Processar arquivos de pesquisa\n",
    "print(\"\\nLoading survey files...\")\n",
    "for file_path in survey_files:\n",
    "    try:\n",
    "        blob = bucket.blob(file_path)\n",
    "        content = blob.download_as_bytes()\n",
    "        \n",
    "        # Identificar o lançamento deste arquivo\n",
    "        launch_id = extract_launch_id(file_path)\n",
    "        \n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(io.BytesIO(content))\n",
    "        else:  # Excel\n",
    "            df = pd.read_excel(io.BytesIO(content))\n",
    "            \n",
    "        # Encontrar coluna de email\n",
    "        email_cols = [col for col in df.columns if any(pattern in col.lower() for pattern in \n",
    "                     ['email', 'e-mail', 'correo', '@', 'mail'])]\n",
    "        \n",
    "        if email_cols:\n",
    "            df = df.rename(columns={email_cols[0]: 'email'})\n",
    "        \n",
    "        # Adicionar identificador de lançamento se pudermos determiná-lo\n",
    "        if launch_id:\n",
    "            df['lançamento'] = launch_id\n",
    "            # Armazenar DataFrame pelo lançamento\n",
    "            launch_data[launch_id]['survey'] = df\n",
    "        \n",
    "        survey_dfs.append(df)\n",
    "        launch_info = f\" ({launch_id})\" if launch_id else \"\"\n",
    "        print(f\"  - Loaded: {file_path}{launch_info}, {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error loading {file_path}: {str(e)}\")\n",
    "\n",
    "# Processar arquivos de compradores\n",
    "print(\"\\nLoading buyer files...\")\n",
    "for file_path in buyer_files:\n",
    "    try:\n",
    "        blob = bucket.blob(file_path)\n",
    "        content = blob.download_as_bytes()\n",
    "        \n",
    "        # Identificar o lançamento deste arquivo\n",
    "        launch_id = extract_launch_id(file_path)\n",
    "        \n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(io.BytesIO(content))\n",
    "        else:  # Excel\n",
    "            df = pd.read_excel(io.BytesIO(content), engine='openpyxl')\n",
    "            \n",
    "        # Encontrar coluna de email\n",
    "        email_cols = [col for col in df.columns if any(pattern in col.lower() for pattern in \n",
    "                     ['email', 'e-mail', 'correo', '@', 'mail'])]\n",
    "        \n",
    "        if email_cols:\n",
    "            df = df.rename(columns={email_cols[0]: 'email'})\n",
    "        else:\n",
    "            print(f\"  - Warning: No email column found in {file_path}. Available columns: {', '.join(df.columns[:5])}...\")\n",
    "        \n",
    "        # Adicionar identificador de lançamento se pudermos determiná-lo\n",
    "        if launch_id:\n",
    "            df['lançamento'] = launch_id\n",
    "            # Armazenar DataFrame pelo lançamento\n",
    "            launch_data[launch_id]['buyer'] = df\n",
    "        \n",
    "        buyer_dfs.append(df)\n",
    "        launch_info = f\" ({launch_id})\" if launch_id else \"\"\n",
    "        print(f\"  - Loaded: {file_path}{launch_info}, {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error loading {file_path}: {str(e)}\")\n",
    "\n",
    "# 5 - Processar arquivos UTM com detecção automática de delimitador\n",
    "print(\"\\nLoading UTM files...\")\n",
    "for file_path in utm_files:\n",
    "    try:\n",
    "        blob = bucket.blob(file_path)\n",
    "        content = blob.download_as_bytes()\n",
    "        \n",
    "        # Identificar o lançamento deste arquivo\n",
    "        launch_id = extract_launch_id(file_path)\n",
    "        \n",
    "        # 1. Detectar o delimitador do arquivo\n",
    "        try:\n",
    "            content_str = content.decode('utf-8')\n",
    "            test_lines = content_str.split('\\n')[:10]  # Primeiras 10 linhas\n",
    "            \n",
    "            # Contar ocorrências de delimitadores potenciais na primeira linha\n",
    "            comma_count = test_lines[0].count(',')\n",
    "            semicolon_count = test_lines[0].count(';')\n",
    "            \n",
    "            # Usar o delimitador que aparece mais vezes\n",
    "            delimiter = ';' if semicolon_count > comma_count else ','\n",
    "            print(f\"  - Detected delimiter '{delimiter}' for {file_path}\")\n",
    "                \n",
    "            # 2. Processar o arquivo com o delimitador correto\n",
    "            df = pd.read_csv(\n",
    "                io.BytesIO(content),\n",
    "                sep=delimiter,\n",
    "                encoding='utf-8',\n",
    "                on_bad_lines='skip',  # Pular linhas problemáticas\n",
    "                low_memory=False      # Evitar avisos de tipos mistos\n",
    "            )\n",
    "            \n",
    "            # 3. Verificação pós-carregamento - Se só temos 1-2 colunas, algo deu errado\n",
    "            if df.shape[1] <= 2:\n",
    "                print(f\"  - Warning: Only {df.shape[1]} columns detected. Trying alternative delimiter...\")\n",
    "                \n",
    "                # Tentar o delimitador oposto\n",
    "                alt_delimiter = ';' if delimiter == ',' else ','\n",
    "                df = pd.read_csv(\n",
    "                    io.BytesIO(content),\n",
    "                    sep=alt_delimiter,\n",
    "                    encoding='utf-8',\n",
    "                    on_bad_lines='skip',\n",
    "                    low_memory=False\n",
    "                )\n",
    "                \n",
    "                print(f\"  - After using delimiter '{alt_delimiter}': {df.shape[1]} columns detected\")\n",
    "            \n",
    "        except UnicodeDecodeError:\n",
    "            # Tentar outra codificação se utf-8 falhar\n",
    "            print(f\"  - Unicode error. Trying latin-1 encoding for {file_path}\")\n",
    "            content_str = content.decode('latin-1')\n",
    "            test_lines = content_str.split('\\n')[:10]\n",
    "            \n",
    "            # Detectar delimitador novamente\n",
    "            comma_count = test_lines[0].count(',')\n",
    "            semicolon_count = test_lines[0].count(';')\n",
    "            delimiter = ';' if semicolon_count > comma_count else ','\n",
    "            \n",
    "            df = pd.read_csv(\n",
    "                io.BytesIO(content),\n",
    "                sep=delimiter,\n",
    "                encoding='latin-1',\n",
    "                on_bad_lines='skip',\n",
    "                low_memory=False\n",
    "            )\n",
    "        \n",
    "        # Encontrar coluna de email\n",
    "        email_cols = [col for col in df.columns if any(pattern in col.lower() for pattern in \n",
    "                     ['email', 'e-mail', 'correo', '@', 'mail'])]\n",
    "        \n",
    "        if email_cols:\n",
    "            df = df.rename(columns={email_cols[0]: 'email'})\n",
    "        else:\n",
    "            print(f\"  - Warning: No email column found in {file_path}. Available columns: {', '.join(df.columns[:5])}...\")\n",
    "        \n",
    "        # Adicionar identificador de lançamento se pudermos determiná-lo\n",
    "        if launch_id:\n",
    "            df['lançamento'] = launch_id\n",
    "            # Armazenar DataFrame pelo lançamento\n",
    "            launch_data[launch_id]['utm'] = df\n",
    "        \n",
    "        utm_dfs.append(df)\n",
    "        launch_info = f\" ({launch_id})\" if launch_id else \"\"\n",
    "        print(f\"  - Loaded: {file_path}{launch_info}, {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error loading {file_path}: {str(e)}\")\n",
    "\n",
    "# 6 - Combinar conjuntos de dados\n",
    "print(\"\\nCombining datasets...\")\n",
    "surveys = pd.concat(survey_dfs, ignore_index=True) if survey_dfs else pd.DataFrame()\n",
    "buyers = pd.concat(buyer_dfs, ignore_index=True) if buyer_dfs else pd.DataFrame()\n",
    "utms = pd.concat(utm_dfs, ignore_index=True) if utm_dfs else pd.DataFrame()\n",
    "\n",
    "print(f\"Survey data: {surveys.shape[0]} rows, {surveys.shape[1]} columns\")\n",
    "print(f\"Buyer data: {buyers.shape[0]} rows, {buyers.shape[1]} columns\")\n",
    "print(f\"UTM data: {utms.shape[0]} rows, {utms.shape[1]} columns\")\n",
    "\n",
    "# 7 - Documentar estrutura de dados\n",
    "print(\"\\nDocumenting data structure...\")\n",
    "for name, df in {\"Surveys\": surveys, \"Buyers\": buyers, \"UTMs\": utms}.items():\n",
    "    if not df.empty:\n",
    "        print(f\"\\n{name} structure:\")\n",
    "        print(f\"  - Columns: {df.shape[1]}\")\n",
    "        print(f\"  - Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "        print(f\"  - Sample columns: {', '.join(df.columns[:5])}\")\n",
    "\n",
    "# 8 - Normalizar emails\n",
    "print(\"\\nNormalizing email addresses...\")\n",
    "surveys['email_norm'] = surveys['email'].apply(normalize_email)\n",
    "\n",
    "# Verificar compradores\n",
    "if not buyers.empty and 'email' in buyers.columns:\n",
    "    buyers['email_norm'] = buyers['email'].apply(normalize_email)\n",
    "else:\n",
    "    print(\"Warning: Cannot normalize emails in buyers dataframe - empty or missing email column\")\n",
    "    if buyers.empty:\n",
    "        buyers = pd.DataFrame(columns=['email', 'email_norm'])\n",
    "\n",
    "# Verificar UTMs\n",
    "if not utms.empty and 'email' in utms.columns:\n",
    "    utms['email_norm'] = utms['email'].apply(normalize_email)\n",
    "else:\n",
    "    print(\"Warning: Cannot normalize emails in UTM dataframe - empty or missing email column\")\n",
    "\n",
    "# 9 - Correspondência de pesquisas com dados de compradores\n",
    "print(\"\\nMatching surveys with buyers...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Verificar se podemos prosseguir com a correspondência\n",
    "if surveys.empty or buyers.empty or 'email_norm' not in buyers.columns:\n",
    "    print(\"Warning: Cannot perform matching. Either surveys or buyers data is empty or missing email_norm column.\")\n",
    "    matches_df = pd.DataFrame(columns=['buyer_id', 'survey_id', 'match_type', 'score'])\n",
    "else:\n",
    "    # Dicionários de consulta para correspondência mais rápida\n",
    "    survey_emails_dict = dict(zip(surveys['email_norm'], surveys.index))\n",
    "    survey_emails_set = set(surveys['email_norm'].dropna())\n",
    "\n",
    "    # Correspondência exata\n",
    "    matches = []\n",
    "    buyers_with_exact_match = buyers[buyers['email_norm'].isin(survey_emails_set)]\n",
    "    for idx, buyer in buyers_with_exact_match.iterrows():\n",
    "        if pd.isna(buyer['email_norm']):\n",
    "            continue\n",
    "            \n",
    "        survey_idx = survey_emails_dict.get(buyer['email_norm'])\n",
    "        if survey_idx is not None:\n",
    "            match_data = {\n",
    "                'buyer_id': idx,\n",
    "                'survey_id': survey_idx,\n",
    "                'match_type': 'exact',\n",
    "                'score': 1.0\n",
    "            }\n",
    "            \n",
    "            # Adicionar informação de lançamento se disponível\n",
    "            if 'lançamento' in buyer and not pd.isna(buyer['lançamento']):\n",
    "                match_data['lançamento'] = buyer['lançamento']\n",
    "                \n",
    "            matches.append(match_data)\n",
    "\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    print(f\"Total matches found: {len(matches_df)}\")\n",
    "\n",
    "# 10 - Criar variável alvo\n",
    "print(\"\\nCreating target variable...\")\n",
    "surveys['target'] = 0\n",
    "\n",
    "if not matches_df.empty and 'survey_id' in matches_df.columns:\n",
    "    match_survey_ids = set(matches_df['survey_id'])\n",
    "    surveys.loc[surveys.index.isin(match_survey_ids), 'target'] = 1\n",
    "    \n",
    "    # Adicionar informação de lançamento às surveys com base nas correspondências\n",
    "    if 'lançamento' in matches_df.columns:\n",
    "        if 'lançamento' not in surveys.columns:\n",
    "            surveys['lançamento'] = None\n",
    "        \n",
    "        # Iterar pelas correspondências para atribuir o lançamento correto\n",
    "        for _, match in matches_df.iterrows():\n",
    "            if 'lançamento' in match and not pd.isna(match['lançamento']):\n",
    "                surveys.loc[match['survey_id'], 'lançamento'] = match['lançamento']\n",
    "    \n",
    "    conversion_rate = surveys['target'].mean() * 100\n",
    "    print(f\"Conversion rate: {conversion_rate:.2f}%\")\n",
    "else:\n",
    "    print(\"No matches found - target variable will be all zeros\")\n",
    "\n",
    "# 11 - Mesclar conjuntos de dados\n",
    "merged_data = surveys\n",
    "if not utms.empty and 'email_norm' in utms.columns:\n",
    "    merged_data = pd.merge(\n",
    "        surveys,\n",
    "        utms,\n",
    "        on='email_norm',\n",
    "        how='left',\n",
    "        suffixes=('', '_utm')\n",
    "    )\n",
    "    print(f\"Merged survey data with UTM data\")\n",
    "\n",
    "# 12 - Adicionar informação de lançamento se ainda não foi adicionada\n",
    "if 'lançamento' not in merged_data.columns and 'lançamento' in buyers.columns:\n",
    "    # Criar um dicionário mapeando email_norm para lançamento\n",
    "    email_to_launch = dict(zip(buyers['email_norm'], buyers['lançamento']))\n",
    "    \n",
    "    # Adicionar coluna de lançamento ao DataFrame mesclado\n",
    "    merged_data['lançamento'] = merged_data['email_norm'].map(email_to_launch)\n",
    "    print(\"Added launch information from buyer data\")\n",
    "\n",
    "print(f\"\\nFinal merged dataset: {merged_data.shape[0]} rows, {merged_data.shape[1]} columns\")\n",
    "\n",
    "# 13 - Estatísticas de lançamento\n",
    "if 'lançamento' in merged_data.columns:\n",
    "    launch_counts = merged_data['lançamento'].value_counts(dropna=False)\n",
    "    print(\"\\nRegistros por lançamento:\")\n",
    "    for launch, count in launch_counts.items():\n",
    "        launch_str = \"Sem lançamento identificado\" if pd.isna(launch) else launch\n",
    "        print(f\"  - {launch_str}: {count} registros\")\n",
    "\n",
    "# 14 - Criar diretório para salvar os dados divididos\n",
    "data_dir = \"../data/split\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# 15 - Dividir os dados em treino, validação e teste\n",
    "print(\"\\nSplitting data into train, validation, and test sets...\")\n",
    "\n",
    "# Verificar se temos dados suficientes para divisão\n",
    "if merged_data.shape[0] < 100:\n",
    "    print(\"Warning: Dataset too small for splitting. Saving all data as train set.\")\n",
    "    merged_data.to_csv(f\"{data_dir}/train.csv\", index=False)\n",
    "else:\n",
    "    # Primeiro separar o conjunto de teste (20% dos dados)\n",
    "    train_val, test = train_test_split(\n",
    "        merged_data, \n",
    "        test_size=0.15, \n",
    "        random_state=42,\n",
    "        stratify=merged_data['target'] if 'target' in merged_data.columns else None\n",
    "    )\n",
    "    \n",
    "    # Depois separar entre treino (64% do total) e validação (16% do total)\n",
    "    train, val = train_test_split(\n",
    "        train_val, \n",
    "        test_size=0.15, \n",
    "        random_state=42,\n",
    "        stratify=train_val['target'] if 'target' in train_val.columns else None\n",
    "    )\n",
    "    \n",
    "    # Salvar os conjuntos de dados\n",
    "    train.to_csv(f\"{data_dir}/train.csv\", index=False)\n",
    "    val.to_csv(f\"{data_dir}/validation.csv\", index=False)\n",
    "    test.to_csv(f\"{data_dir}/test.csv\", index=False)\n",
    "    \n",
    "    print(f\"Train set: {train.shape[0]} rows, {train.shape[1]} columns\")\n",
    "    print(f\"Validation set: {val.shape[0]} rows, {val.shape[1]} columns\")\n",
    "    print(f\"Test set: {test.shape[0]} rows, {test.shape[1]} columns\")\n",
    "    \n",
    "    # Verificar distribuição da variável alvo\n",
    "    if 'target' in merged_data.columns:\n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        print(f\"  - Train: {train['target'].mean()*100:.2f}% positive\")\n",
    "        print(f\"  - Validation: {val['target'].mean()*100:.2f}% positive\")\n",
    "        print(f\"  - Test: {test['target'].mean()*100:.2f}% positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfdfe8c-7a29-4aee-81b5-9ce4d17a3cfe",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "### i. Data Quality Assessment\n",
    "1. Check dataset sizes and completeness  \n",
    "2. Count null values and missing data  \n",
    "3. Identify duplicates  \n",
    "4. List *unique* values and their percentages  \n",
    "5. Flag extremely low/unusual values  \n",
    "\n",
    "### ii. Univariate Analysis\n",
    "1. Analyze distributions for each variable  \n",
    "2. Identify outliers  \n",
    "\n",
    "### iii. Relationship Analysis\n",
    "1. Correlation analysis between features  \n",
    "2. Feature-target relationships  \n",
    "3. Initial feature importance assessment  \n",
    "\n",
    "### iv. Segment Analysis\n",
    "1. Country-specific patterns  \n",
    "2. UTM source/campaign analysis  \n",
    "3. Temporal patterns (day, time)  \n",
    "4. Time-to-conversion metrics  \n",
    "5. Launch/cohort analysis (L16–L21)  \n",
    "6. Leads that buy in other launches  \n",
    "\n",
    "### v. Check for category shifts over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727a2fc-46a1-42a4-a71d-a5b2ce03f64f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from io import StringIO\n",
    "!pip install -U kaleido\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure better plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Preparação para capturar a saída em um arquivo\n",
    "old_stdout = sys.stdout\n",
    "report_content = StringIO()\n",
    "sys.stdout = report_content\n",
    "\n",
    "# Load the merged dataset\n",
    "print(\"Loading merged dataset...\")\n",
    "try:\n",
    "    # Try to load the data with launch information from the previous step\n",
    "    merged_data = pd.read_csv('01_data_collection_and_integration.csv')\n",
    "    print(f\"Loaded dataset with launch information: {merged_data.shape[0]} rows, {merged_data.shape[1]} columns\")\n",
    "except:\n",
    "    try:\n",
    "        # Fallback to original processed data\n",
    "        merged_data = pd.read_csv('smart_ads_processed_data.csv')\n",
    "        print(f\"Loaded processed dataset: {merged_data.shape[0]} rows, {merged_data.shape[1]} columns\")\n",
    "    except:\n",
    "        print(\"ERROR: No processed dataset found. Please run the data collection and integration step first.\")\n",
    "        # For demonstration, we'll create a dummy message\n",
    "        print(\"Creating dummy dataset for demonstration...\")\n",
    "        merged_data = pd.DataFrame()\n",
    "\n",
    "if not merged_data.empty:\n",
    "    # Create a results directory if it doesn't exist\n",
    "    os.makedirs('eda_results', exist_ok=True)\n",
    "    \n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"1. DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=============================================\\n\")\n",
    "    \n",
    "    # Dataset sizes and completeness\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Total records: {len(merged_data)}\")\n",
    "    print(f\"Total features: {merged_data.shape[1]}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData types:\")\n",
    "    dtype_counts = merged_data.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Completion Rate\n",
    "    completion_rates = (merged_data.count() / len(merged_data) * 100).sort_values()\n",
    "    print(\"\\nCompletion rates (lowest 10):\")\n",
    "    for col, rate in completion_rates.head(10).items():\n",
    "        print(f\"  {col}: {rate:.2f}%\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_vals = merged_data.isnull().sum()\n",
    "    missing_vals = missing_vals[missing_vals > 0].sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nColumns with missing values: {len(missing_vals)} out of {len(merged_data.columns)}\")\n",
    "    if len(missing_vals) > 0:\n",
    "        print(\"Top 10 columns with most missing values:\")\n",
    "        for col, count in missing_vals.head(10).items():\n",
    "            print(f\"  {col}: {count} missing ({count/len(merged_data)*100:.2f}%)\")\n",
    "            \n",
    "        # Plot missing values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = missing_vals.head(15).plot(kind='bar')\n",
    "        plt.title('Missing Values by Column (Top 15)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_results/missing_values.png')\n",
    "    \n",
    "    # Check for duplicates in ID columns only\n",
    "    id_cols = [col for col in merged_data.columns if ('id' in col.lower() or 'email' in col.lower()) \n",
    "               and not any(text in col.lower() for text in ['message', 'mensaje', 'fluid', 'opportunities', 'learn'])]\n",
    "    \n",
    "    if id_cols:\n",
    "        print(\"\\nChecking for duplicates in ID columns:\")\n",
    "        for col in id_cols:\n",
    "            if col in merged_data.columns:\n",
    "                unique_count = merged_data[col].nunique()\n",
    "                duplicate_pct = (1 - unique_count / merged_data[col].count()) * 100\n",
    "                print(f\"  {col}: {unique_count} unique values, {duplicate_pct:.2f}% duplicates\")\n",
    "    \n",
    "    # Full duplicate rows\n",
    "    duplicate_rows = merged_data.duplicated().sum()\n",
    "    print(f\"\\nFull duplicate rows: {duplicate_rows} ({duplicate_rows/len(merged_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Identify free text columns\n",
    "    free_text_cols = [col for col in merged_data.columns if any(pattern in col.lower() for pattern in \n",
    "                    ['mensaje', 'message', 'cambiar', 'esperas', 'oportunidades', 'fluido', 'fluidez'])]\n",
    "    \n",
    "    if free_text_cols:\n",
    "        print(\"\\nFree text columns identified (excluded from duplicates analysis):\")\n",
    "        for col in free_text_cols:\n",
    "            # Show basic statistics for text fields\n",
    "            not_null = merged_data[col].notna().sum()\n",
    "            avg_length = merged_data[col].astype(str).str.len().mean()\n",
    "            print(f\"  {col}: {not_null} non-null values, avg length: {avg_length:.1f} characters\")\n",
    "    \n",
    "    # Unique values analysis for categorical variables\n",
    "    categorical_cols = [col for col in merged_data.select_dtypes(include=['object']).columns \n",
    "                      if col not in free_text_cols]\n",
    "    \n",
    "    print(\"\\nUnique value analysis for categorical columns:\")\n",
    "    high_cardinality_cols = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_vals = merged_data[col].nunique()\n",
    "        not_null_count = merged_data[col].count()\n",
    "        unique_pct = unique_vals / not_null_count * 100 if not_null_count > 0 else 0\n",
    "        \n",
    "        # Detect high cardinality columns (excluding free text fields)\n",
    "        if unique_pct > 50 and unique_vals > 100 and col not in free_text_cols:\n",
    "            high_cardinality_cols.append((col, unique_vals, unique_pct))\n",
    "        \n",
    "        # Print details for columns with reasonable cardinality\n",
    "        if unique_vals < 50:\n",
    "            print(f\"\\n  {col}: {unique_vals} unique values\")\n",
    "            # Show value distribution\n",
    "            val_counts = merged_data[col].value_counts(normalize=True, dropna=False).head(5)\n",
    "            for val, pct in val_counts.items():\n",
    "                val_display = str(val)[:30] + '...' if isinstance(val, str) and len(str(val)) > 30 else val\n",
    "                print(f\"    - {val_display}: {pct*100:.2f}%\")\n",
    "    \n",
    "    # Report high cardinality columns (excluding known free text fields)\n",
    "    if high_cardinality_cols:\n",
    "        print(\"\\nHigh cardinality columns (likely IDs or unique identifiers):\")\n",
    "        for col, unique_vals, unique_pct in sorted(high_cardinality_cols, key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {col}: {unique_vals} unique values ({unique_pct:.2f}% unique)\")\n",
    "    \n",
    "    # Identify extremely low/unusual values in numeric columns\n",
    "    numeric_cols = merged_data.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    print(\"\\nChecking for unusual values in numeric columns:\")\n",
    "    for col in numeric_cols:\n",
    "        # Skip likely ID columns\n",
    "        if 'id' in col.lower() or col.endswith('_id'):\n",
    "            continue\n",
    "            \n",
    "        # Get basic stats\n",
    "        col_min = merged_data[col].min()\n",
    "        col_max = merged_data[col].max()\n",
    "        col_mean = merged_data[col].mean()\n",
    "        zeros_count = (merged_data[col] == 0).sum()\n",
    "        zeros_pct = zeros_count / merged_data[col].count() * 100\n",
    "        \n",
    "        # Check for unusual patterns\n",
    "        if zeros_pct > 90:\n",
    "            print(f\"  {col}: {zeros_pct:.2f}% zeros (potential issue)\")\n",
    "        elif col_min < 0 and col_max > 0 and abs(col_min) > col_max * 10:\n",
    "            print(f\"  {col}: Extreme negative value ({col_min}) compared to max ({col_max})\")\n",
    "        elif col_max > col_mean * 100:\n",
    "            print(f\"  {col}: Extreme positive value ({col_max}) compared to mean ({col_mean:.2f})\")\n",
    "    \n",
    "    # Target variable analysis\n",
    "    if 'target' in merged_data.columns:\n",
    "        target_counts = merged_data['target'].value_counts()\n",
    "        target_pct = target_counts / target_counts.sum() * 100\n",
    "        \n",
    "        print(\"\\nTarget variable distribution:\")\n",
    "        for val, count in target_counts.items():\n",
    "            print(f\"  {val}: {count} records ({target_pct[val]:.2f}%)\")\n",
    "        \n",
    "        print(\"Note: Low conversion rate (~1.5%) is expected for this type of digital campaign data\")\n",
    "        \n",
    "        # Plot target distribution\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(x='target', data=merged_data)\n",
    "        plt.title('Target Variable Distribution')\n",
    "        plt.xlabel('Target (0=No Conversion, 1=Conversion)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('eda_results/target_distribution.png')\n",
    "        \n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"2. UNIVARIATE ANALYSIS\")\n",
    "    print(\"=============================================\\n\")\n",
    "    \n",
    "    # Numeric variable distributions\n",
    "    numeric_cols = merged_data.select_dtypes(include=['number']).columns\n",
    "    print(f\"Analyzing distributions for {len(numeric_cols)} numeric variables...\")\n",
    "    \n",
    "    # Exclude ID columns and the target\n",
    "    analysis_cols = [col for col in numeric_cols if not ('id' in col.lower() or col == 'target')]\n",
    "    \n",
    "    # Create histograms for numeric variables\n",
    "    if analysis_cols:\n",
    "        print(\"Creating distribution plots for numeric variables...\")\n",
    "        for i, col in enumerate(analysis_cols[:10]):  # Limit to first 10 for brevity\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(merged_data[col].dropna(), kde=True)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'eda_results/dist_{col}.png')\n",
    "            \n",
    "            # Print basic statistics\n",
    "            stats = merged_data[col].describe()\n",
    "            print(f\"\\n{col} statistics:\")\n",
    "            print(f\"  Mean: {stats['mean']:.2f}\")\n",
    "            print(f\"  Std: {stats['std']:.2f}\")\n",
    "            print(f\"  Min: {stats['min']:.2f}\")\n",
    "            print(f\"  25%: {stats['25%']:.2f}\")\n",
    "            print(f\"  Median: {stats['50%']:.2f}\")\n",
    "            print(f\"  75%: {stats['75%']:.2f}\")\n",
    "            print(f\"  Max: {stats['max']:.2f}\")\n",
    "            \n",
    "            # Detect outliers using IQR method\n",
    "            Q1 = stats['25%']\n",
    "            Q3 = stats['75%']\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_low = Q1 - 1.5 * IQR\n",
    "            outlier_high = Q3 + 1.5 * IQR\n",
    "            outliers = merged_data[(merged_data[col] < outlier_low) | (merged_data[col] > outlier_high)][col]\n",
    "            print(f\"  Outliers: {len(outliers)} ({len(outliers)/merged_data[col].count()*100:.2f}%)\")\n",
    "    \n",
    "    # Categorical variable distributions\n",
    "    cat_cols = [col for col in categorical_cols if merged_data[col].nunique() < 30]  # Exclude high cardinality\n",
    "    \n",
    "    if cat_cols:\n",
    "        print(\"\\nAnalyzing distributions for categorical variables...\")\n",
    "        for col in cat_cols[:5]:  # Limit to first 5 for brevity\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            val_counts = merged_data[col].value_counts().head(15)  # Top 15 categories\n",
    "            ax = val_counts.plot(kind='bar')\n",
    "            plt.title(f'Distribution of {col} (Top 15 values)')\n",
    "            plt.tight_layout()\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.savefig(f'eda_results/cat_dist_{col}.png')\n",
    "            \n",
    "            # Print distribution statistics\n",
    "            print(f\"\\n{col} - top 5 values:\")\n",
    "            top_vals = merged_data[col].value_counts(normalize=True).head(5)\n",
    "            for val, pct in top_vals.items():\n",
    "                val_display = str(val)[:30] + '...' if isinstance(val, str) and len(str(val)) > 30 else val\n",
    "                print(f\"  {val_display}: {pct*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"3. RELATIONSHIP ANALYSIS\")\n",
    "    print(\"=============================================\\n\")\n",
    "    \n",
    "    # Target-related analysis (if target exists)\n",
    "    if 'target' in merged_data.columns:\n",
    "        print(\"Analyzing relationships with target variable...\")\n",
    "        \n",
    "        # Numeric features vs target\n",
    "        numeric_features = [col for col in numeric_cols if col != 'target' and not ('id' in col.lower())]\n",
    "        \n",
    "        if numeric_features:\n",
    "            # Calculate correlation with target\n",
    "            target_corr = merged_data[numeric_features + ['target']].corr()['target'].sort_values(ascending=False)\n",
    "            \n",
    "            print(\"\\nCorrelation of numeric features with target:\")\n",
    "            for feature, corr in target_corr.items():\n",
    "                if feature != 'target':\n",
    "                    print(f\"  {feature}: {corr:.4f}\")\n",
    "            \n",
    "            # Plot top correlations with target\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_corrs = target_corr[1:11]  # Skip target itself, take top 10\n",
    "            top_corrs.plot(kind='bar')\n",
    "            plt.title('Top 10 Correlations with Target')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_results/target_correlation.png')\n",
    "        \n",
    "        # Categorical features vs target\n",
    "        cat_features_for_analysis = [col for col in categorical_cols \n",
    "                                  if merged_data[col].nunique() < 20 and merged_data[col].nunique() > 1]\n",
    "        \n",
    "        if cat_features_for_analysis:\n",
    "            print(\"\\nCategorical features vs target:\")\n",
    "            for col in cat_features_for_analysis[:5]:  # Limit to first 5\n",
    "                cross_tab = pd.crosstab(\n",
    "                    merged_data[col], \n",
    "                    merged_data['target'], \n",
    "                    normalize='index'\n",
    "                )\n",
    "                \n",
    "                # Print conversion rate by category\n",
    "                if 1 in cross_tab.columns:  # If there are positive conversions\n",
    "                    print(f\"\\n  {col} - conversion rates:\")\n",
    "                    conversion_rates = cross_tab[1].sort_values(ascending=False)\n",
    "                    for cat, rate in conversion_rates.head(5).items():\n",
    "                        cat_display = str(cat)[:30] + '...' if isinstance(cat, str) and len(str(cat)) > 30 else cat\n",
    "                        print(f\"    {cat_display}: {rate*100:.2f}%\")\n",
    "                \n",
    "                # Plot categorical feature vs target\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                pd.crosstab(merged_data[col], merged_data['target']).plot(kind='bar', stacked=True)\n",
    "                plt.title(f'{col} vs Target')\n",
    "                plt.ylabel('Count')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'eda_results/{col}_vs_target.png')\n",
    "        \n",
    "        # Feature importance using Random Forest\n",
    "        print(\"\\nCalculating feature importance using Random Forest...\")\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        features_for_rf = []\n",
    "        \n",
    "        # Add numeric features\n",
    "        for col in numeric_features:\n",
    "            if merged_data[col].isnull().sum() < len(merged_data) * 0.3:  # Less than 30% missing\n",
    "                features_for_rf.append(col)\n",
    "        \n",
    "        # Add encoded categorical features (low cardinality)\n",
    "        cat_for_rf = [col for col in categorical_cols \n",
    "                    if merged_data[col].nunique() < 20 and merged_data[col].nunique() > 1 \n",
    "                    and merged_data[col].isnull().sum() < len(merged_data) * 0.3]\n",
    "        \n",
    "        # Encode categorical features\n",
    "        X_rf = merged_data[features_for_rf].copy()\n",
    "        encoders = {}\n",
    "        \n",
    "        for col in cat_for_rf:\n",
    "            le = LabelEncoder()\n",
    "            # Fit only on non-null values\n",
    "            valid_idx = ~merged_data[col].isnull()\n",
    "            X_rf[col] = pd.Series(index=merged_data.index)\n",
    "            X_rf.loc[valid_idx, col] = le.fit_transform(merged_data.loc[valid_idx, col])\n",
    "            encoders[col] = le\n",
    "            features_for_rf.append(col)\n",
    "        \n",
    "        # Fill missing values with median/mode\n",
    "        for col in features_for_rf:\n",
    "            if X_rf[col].isnull().any():\n",
    "                if X_rf[col].dtype.kind in 'ifc':  # numeric\n",
    "                    X_rf[col].fillna(X_rf[col].median(), inplace=True)\n",
    "                else:\n",
    "                    X_rf[col].fillna(X_rf[col].mode()[0], inplace=True)\n",
    "        \n",
    "        # Train a simple RF model to get feature importance\n",
    "        if len(features_for_rf) > 0 and 'target' in merged_data.columns:\n",
    "            try:\n",
    "                rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "                rf.fit(X_rf[features_for_rf], merged_data['target'])\n",
    "                \n",
    "                # Get feature importances\n",
    "                importances = pd.Series(rf.feature_importances_, index=features_for_rf)\n",
    "                top_features = importances.sort_values(ascending=False)\n",
    "                \n",
    "                print(\"\\nTop 10 important features:\")\n",
    "                for feature, importance in top_features.head(10).items():\n",
    "                    print(f\"  {feature}: {importance:.4f}\")\n",
    "                \n",
    "                # Plot feature importances\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                top_features.head(15).plot(kind='bar')\n",
    "                plt.title('Feature Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('eda_results/feature_importance.png')\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating feature importance: {e}\")\n",
    "    \n",
    "    # Correlation matrix for numeric features\n",
    "    corr_features = [col for col in numeric_cols if not ('id' in col.lower())]\n",
    "    if len(corr_features) > 1:\n",
    "        print(\"\\nCalculating correlation matrix for numeric features...\")\n",
    "        corr_matrix = merged_data[corr_features].corr()\n",
    "        \n",
    "        # Plot correlation matrix\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_results/correlation_matrix.png')\n",
    "        \n",
    "        # Find highly correlated features\n",
    "        high_corr_threshold = 0.7\n",
    "        high_corr_pairs = []\n",
    "        \n",
    "        for i in range(len(corr_features)):\n",
    "            for j in range(i+1, len(corr_features)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "                    high_corr_pairs.append((\n",
    "                        corr_features[i], \n",
    "                        corr_features[j], \n",
    "                        corr_matrix.iloc[i, j]\n",
    "                    ))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(\"\\nHighly correlated feature pairs:\")\n",
    "            for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "                print(f\"  {feat1} — {feat2}: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"4. SEGMENT ANALYSIS\")\n",
    "    print(\"=============================================\\n\")\n",
    "    \n",
    "    # Country-specific patterns\n",
    "    country_col = next((col for col in merged_data.columns \n",
    "                      if any(pattern in col.lower() for pattern in ['pais', 'país', 'country'])), None)\n",
    "    \n",
    "    if country_col and 'target' in merged_data.columns:\n",
    "        print(\"Analyzing country-specific patterns...\")\n",
    "        \n",
    "        # Display raw counts by country\n",
    "        country_counts = merged_data[country_col].value_counts()\n",
    "        print(\"\\nDistribution of records by country:\")\n",
    "        for country, count in country_counts.head(10).items():\n",
    "            print(f\"  {country}: {count} records ({count/len(merged_data)*100:.2f}%)\")\n",
    "        \n",
    "        # Conversion rate by country\n",
    "        country_conversion = merged_data.groupby(country_col)['target'].agg(['sum', 'count', 'mean'])\n",
    "        country_conversion['conversion_rate'] = country_conversion['mean'] * 100\n",
    "        country_conversion = country_conversion.sort_values('conversion_rate', ascending=False)\n",
    "        \n",
    "        # Filter to countries with at least 100 records\n",
    "        country_conversion_filtered = country_conversion[country_conversion['count'] >= 100]\n",
    "        \n",
    "        print(\"\\nConversion rates by country (min 100 records):\")\n",
    "        for country, data in country_conversion_filtered.iterrows():\n",
    "            print(f\"  {country}: {data['conversion_rate']:.2f}% ({data['sum']} conversions out of {data['count']} records)\")\n",
    "        \n",
    "        # Plot conversion rate by country\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        country_conversion_filtered['conversion_rate'].plot(kind='bar')\n",
    "        plt.title('Conversion Rate by Country')\n",
    "        plt.ylabel('Conversion Rate (%)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_results/country_conversion.png')\n",
    "        \n",
    "        # Double check Spain and USA specifically\n",
    "        print(\"\\nDetailed analysis for Spain and USA:\")\n",
    "        for country in ['España', 'Spain', 'Estados Unidos', 'USA', 'United States']:\n",
    "            if country in merged_data[country_col].values:\n",
    "                conversions = merged_data[merged_data[country_col] == country]['target'].sum()\n",
    "                total = merged_data[merged_data[country_col] == country].shape[0]\n",
    "                rate = conversions / total * 100 if total > 0 else 0\n",
    "                print(f\"  {country}: {conversions} conversions out of {total} records ({rate:.2f}%)\")\n",
    "    \n",
    "    # UTM source/campaign analysis\n",
    "    utm_cols = [col for col in merged_data.columns \n",
    "               if any(pattern in col.lower() for pattern in ['utm', 'source', 'campaign', 'medium'])]\n",
    "    \n",
    "    if utm_cols and 'target' in merged_data.columns:\n",
    "        print(\"\\nAnalyzing UTM parameters...\")\n",
    "    \n",
    "    for utm_col in utm_cols:\n",
    "        if merged_data[utm_col].nunique() < 100:  # Skip if too many unique values\n",
    "            try:\n",
    "                # Calculate conversion rate by UTM value\n",
    "                # Mudando a forma de criar o DataFrame agregado\n",
    "                utm_agg = merged_data.groupby(utm_col)['target'].agg(['sum', 'count'])\n",
    "                utm_agg['mean'] = utm_agg['sum'] / utm_agg['count']\n",
    "                utm_agg['conversion_rate'] = utm_agg['mean'] * 100\n",
    "                \n",
    "                # Filter to values with at least 50 records\n",
    "                utm_filtered = utm_agg[utm_agg['count'] >= 50]\n",
    "                utm_filtered = utm_filtered.sort_values('conversion_rate', ascending=False)\n",
    "                \n",
    "                # Plot top UTM values by conversion rate\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                utm_filtered['conversion_rate'].head(10).plot(kind='bar')\n",
    "                plt.title(f'Conversion Rate by {utm_col} (Top 10)')\n",
    "                plt.ylabel('Conversion Rate (%)')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'eda_results/{utm_col}_conversion.png')\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing UTM column {utm_col}: {e}\")\n",
    "                \n",
    "    # Alternativa caso a abordagem anterior falhe\n",
    "    # Esta é uma abordagem mais robusta para processamento de UTMs\n",
    "    def analyze_utm_safely(df, utm_col):\n",
    "        if utm_col not in df.columns or df[utm_col].nunique() >= 100:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Agrupar de forma mais segura\n",
    "            grouped = df.groupby(utm_col)\n",
    "\n",
    "            # Calcular estatísticas de forma explícita\n",
    "            counts = grouped.size()\n",
    "            converted = grouped['target'].sum()\n",
    "\n",
    "            # Criar dataframe com as métricas\n",
    "            result_df = pd.DataFrame({\n",
    "                'count': counts,\n",
    "                'conversions': converted\n",
    "            })\n",
    "\n",
    "            # Adicionar taxa de conversão\n",
    "            result_df['conversion_rate'] = (result_df['conversions'] / result_df['count']) * 100\n",
    "\n",
    "            # Filtrar por contagem mínima\n",
    "            filtered = result_df[result_df['count'] >= 50]\n",
    "\n",
    "            # Ordenar por taxa de conversão\n",
    "            filtered = filtered.sort_values('conversion_rate', ascending=False)\n",
    "\n",
    "            if not filtered.empty:\n",
    "                print(f\"\\nConversion rates by {utm_col} (alternative method, min 50 records):\")\n",
    "                for utm_val, row in filtered.head(10).iterrows():\n",
    "                    utm_display = str(utm_val)[:30] + '...' if isinstance(utm_val, str) and len(str(utm_val)) > 30 else utm_val\n",
    "                    if pd.isna(utm_display):\n",
    "                        utm_display = 'NaN'\n",
    "                    print(f\"  {utm_display}: {row['conversion_rate']:.2f}% ({row['conversions']} conversions out of {row['count']} records)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Alternative UTM analysis for {utm_col} failed: {e}\")\n",
    "            \n",
    "            if not utm_filtered.empty:\n",
    "                print(f\"\\nConversion rates by {utm_col} (min 50 records):\")\n",
    "                for utm_val, row in utm_filtered.head(10).iterrows():\n",
    "                    utm_display = str(utm_val)[:30] + '...' if isinstance(utm_val, str) and len(str(utm_val)) > 30 else utm_val\n",
    "                    if pd.isna(utm_display):\n",
    "                        utm_display = 'NaN'\n",
    "                    print(f\"  {utm_display}: {row['conversion_rate']:.2f}% ({row['sum']} conversions out of {row['count']} records)\")\n",
    "    \n",
    "    # Temporal patterns analysis\n",
    "    date_cols = [col for col in merged_data.columns \n",
    "                if any(pattern in col.lower() for pattern in ['date', 'time', 'fecha', 'data', 'timestamp', 'marca'])]\n",
    "    \n",
    "    if date_cols:\n",
    "        print(\"\\nAnalyzing temporal patterns...\")\n",
    "        \n",
    "        # Process timestamp columns\n",
    "        for date_col in date_cols:\n",
    "            try:\n",
    "                # Try to convert to datetime\n",
    "                if merged_data[date_col].dtype == 'object':\n",
    "                    merged_data[f'{date_col}_dt'] = pd.to_datetime(merged_data[date_col], errors='coerce')\n",
    "                else:\n",
    "                    # If already numeric, might be a timestamp\n",
    "                    merged_data[f'{date_col}_dt'] = pd.to_datetime(merged_data[date_col], unit='s', errors='coerce')\n",
    "                \n",
    "                if merged_data[f'{date_col}_dt'].notna().any():\n",
    "                    print(f\"Successfully converted {date_col} to datetime\")\n",
    "                    \n",
    "                    # Extract time components\n",
    "                    merged_data[f'{date_col}_hour'] = merged_data[f'{date_col}_dt'].dt.hour\n",
    "                    merged_data[f'{date_col}_day'] = merged_data[f'{date_col}_dt'].dt.day_name()\n",
    "                    merged_data[f'{date_col}_month'] = merged_data[f'{date_col}_dt'].dt.month_name()\n",
    "                    \n",
    "                    # Analyze hourly pattern\n",
    "                    if 'target' in merged_data.columns:\n",
    "                        hour_conversion = merged_data.groupby(f'{date_col}_hour')['target'].agg(['sum', 'count', 'mean'])\n",
    "                        hour_conversion['conversion_rate'] = hour_conversion['mean'] * 100\n",
    "                        \n",
    "                        print(f\"\\nHourly conversion rates from {date_col}:\")\n",
    "                        for hour in sorted(hour_conversion.index):\n",
    "                            data = hour_conversion.loc[hour]\n",
    "                            print(f\"  Hour {hour}: {data['conversion_rate']:.2f}% ({data['sum']} conversions out of {data['count']} records)\")\n",
    "                        \n",
    "                        # Plot hourly conversion rate\n",
    "                        plt.figure(figsize=(12, 6))\n",
    "                        hour_conversion['conversion_rate'].plot()\n",
    "                        plt.title(f'Conversion Rate by Hour ({date_col})')\n",
    "                        plt.xlabel('Hour of Day')\n",
    "                        plt.ylabel('Conversion Rate (%)')\n",
    "                        plt.xticks(range(0, 24, 2))\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(f'eda_results/{date_col}_hourly_conversion.png')\n",
    "                        \n",
    "                        # Analyze day of week pattern\n",
    "                        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "                        day_conversion = merged_data.groupby(f'{date_col}_day')['target'].agg(['sum', 'count', 'mean'])\n",
    "                        day_conversion['conversion_rate'] = day_conversion['mean'] * 100\n",
    "                        \n",
    "                        # Reindex to get days in correct order\n",
    "                        day_conversion = day_conversion.reindex(day_order)\n",
    "                        \n",
    "                        print(f\"\\nDay of week conversion rates from {date_col}:\")\n",
    "                        for day in day_order:\n",
    "                            if day in day_conversion.index:\n",
    "                                data = day_conversion.loc[day]\n",
    "                                print(f\"  {day}: {data['conversion_rate']:.2f}% ({data['sum']} conversions out of {data['count']} records)\")\n",
    "                        \n",
    "                        # Plot day of week conversion rate\n",
    "                        plt.figure(figsize=(12, 6))\n",
    "                        day_conversion['conversion_rate'].plot(kind='bar')\n",
    "                        plt.title(f'Conversion Rate by Day of Week ({date_col})')\n",
    "                        plt.ylabel('Conversion Rate (%)')\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(f'eda_results/{date_col}_daily_conversion.png')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing date column {date_col}: {e}\")\n",
    "    \n",
    "    # Enhanced Launch Analysis (L16-L21)\n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"5. ENHANCED LAUNCH/COHORT ANALYSIS\")\n",
    "    print(\"=============================================\\n\")\n",
    "    \n",
    "    # Check if we have the launch column from previous processing\n",
    "    launch_col = None\n",
    "    if 'lançamento' in merged_data.columns:\n",
    "        launch_col = 'lançamento'\n",
    "    elif 'lancamento' in merged_data.columns:\n",
    "        launch_col = 'lancamento'\n",
    "    \n",
    "    if launch_col is None:\n",
    "        # Try to infer launch information from other columns\n",
    "        # Look for patterns L16, L17, etc. in columns\n",
    "        merged_data['launch_cohort'] = None\n",
    "        launch_patterns = ['L16', 'L17', 'L18', 'L19', 'L20', 'L21']\n",
    "        \n",
    "        for pattern in launch_patterns:\n",
    "            pattern_mask = False\n",
    "            for col in merged_data.columns:\n",
    "                if merged_data[col].dtype == 'object':\n",
    "                    pattern_mask |= merged_data[col].astype(str).str.contains(pattern, case=False, regex=False).fillna(False)\n",
    "            \n",
    "            if pattern_mask.any():\n",
    "                merged_data.loc[pattern_mask, 'launch_cohort'] = pattern\n",
    "        \n",
    "        launch_col = 'launch_cohort'\n",
    "    \n",
    "    # Ensure we have a valid launch column with data\n",
    "    if launch_col in merged_data.columns and merged_data[launch_col].notna().any():\n",
    "        print(f\"Using launch column: {launch_col}\")\n",
    "        \n",
    "        # Basic statistics by launch\n",
    "        launch_stats = merged_data.groupby(launch_col).size().reset_index(name='records')\n",
    "        launch_stats = launch_stats.sort_values(by=launch_col)\n",
    "        \n",
    "        print(\"\\nRecord distribution by launch:\")\n",
    "        for _, row in launch_stats.iterrows():\n",
    "            launch = row[launch_col]\n",
    "            count = row['records']\n",
    "            if pd.notna(launch):\n",
    "                print(f\"  {launch}: {count} records ({count/len(merged_data)*100:.2f}%)\")\n",
    "        \n",
    "        # Plot launch distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.barplot(x=launch_col, y='records', data=launch_stats[launch_stats[launch_col].notna()])\n",
    "        plt.title('Record Distribution by Launch')\n",
    "        plt.ylabel('Number of Records')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_results/launch_distribution.png')\n",
    "        \n",
    "        # Target conversion by launch\n",
    "        if 'target' in merged_data.columns:\n",
    "            launch_conversion = merged_data.groupby(launch_col)['target'].agg(['sum', 'count', 'mean'])\n",
    "            launch_conversion['conversion_rate'] = launch_conversion['mean'] * 100\n",
    "            \n",
    "            print(\"\\nConversion rates by launch:\")\n",
    "            for launch, data in launch_conversion.iterrows():\n",
    "                if pd.notna(launch):\n",
    "                    print(f\"  {launch}: {data['conversion_rate']:.2f}% ({data['sum']} conversions out of {data['count']} records)\")\n",
    "            \n",
    "            # Plot conversion rate by launch\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            launch_conversion = launch_conversion.sort_index()\n",
    "            launch_conversion['conversion_rate'].plot(kind='bar')\n",
    "            plt.title('Conversion Rate by Launch')\n",
    "            plt.ylabel('Conversion Rate (%)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_results/launch_conversion_rate.png')\n",
    "        \n",
    "        # Category shifts over time (across launches)\n",
    "        print(\"\\nAnalyzing category shifts across launches...\")\n",
    "        \n",
    "        # Select categorical columns for trend analysis\n",
    "        trend_cols = [col for col in categorical_cols if merged_data[col].nunique() > 1 and merged_data[col].nunique() < 20]\n",
    "        \n",
    "        for col in trend_cols[:5]:  # Limit to top 5 to avoid too many charts\n",
    "            try:\n",
    "                # Calculate value distribution by launch\n",
    "                cat_by_launch = pd.crosstab(merged_data[launch_col], merged_data[col], normalize='index') * 100\n",
    "                \n",
    "                # Skip if too sparse\n",
    "                if cat_by_launch.shape[1] <= 1 or cat_by_launch.dropna().empty:\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"\\nCategory shifts for {col} across launches:\")\n",
    "                \n",
    "                # Print top categories for each launch\n",
    "                for launch in cat_by_launch.index:\n",
    "                    if pd.isna(launch):\n",
    "                        continue\n",
    "                    \n",
    "                    top_cats = cat_by_launch.loc[launch].nlargest(5)\n",
    "                    print(f\"  {launch} top categories:\")\n",
    "                    for cat, pct in top_cats.items():\n",
    "                        cat_display = str(cat)[:30] + '...' if isinstance(cat, str) and len(str(cat)) > 30 else cat\n",
    "                        print(f\"    - {cat_display}: {pct:.2f}%\")\n",
    "                \n",
    "                # Plot trend of top categories\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                for cat in cat_by_launch.columns[:5]:  # Top 5 categories\n",
    "                    cat_by_launch[cat].plot(marker='o', label=str(cat)[:20])\n",
    "                \n",
    "                plt.title(f'Category Shift: {col} Distribution by Launch')\n",
    "                plt.ylabel('Percentage (%)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend(title='Categories')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'eda_results/category_shift_{col}.png')\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing category shift for {col}: {e}\")\n",
    "        \n",
    "        # Profile differences between launches\n",
    "        print(\"\\nProfiling differences between launches...\")\n",
    "        \n",
    "        numeric_by_launch = {}\n",
    "        for col in numeric_cols:\n",
    "            if col != 'target' and not ('id' in col.lower()):\n",
    "                try:\n",
    "                    # Calculate mean of numeric column by launch\n",
    "                    means = merged_data.groupby(launch_col)[col].mean()\n",
    "                    if not means.isna().all():\n",
    "                        numeric_by_launch[col] = means\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        \n",
    "        if numeric_by_launch:\n",
    "            numeric_trends = pd.DataFrame(numeric_by_launch)\n",
    "            \n",
    "            # Print significant shifts\n",
    "            print(\"\\nSignificant numeric shifts between launches:\")\n",
    "            for col in numeric_trends.columns:\n",
    "                min_val = numeric_trends[col].min()\n",
    "                max_val = numeric_trends[col].max()\n",
    "                \n",
    "                # Check if there's meaningful variation (avoid near-constant values)\n",
    "                if max_val - min_val > 0.1 * min_val:\n",
    "                    print(f\"\\n  {col}:\")\n",
    "                    for launch, val in numeric_trends[col].items():\n",
    "                        if pd.notna(launch) and pd.notna(val):\n",
    "                            print(f\"    - {launch}: {val:.2f}\")\n",
    "            \n",
    "            # Create a heatmap of numeric trends\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            \n",
    "            # Normalize data for better visualization\n",
    "            norm_data = (numeric_trends - numeric_trends.min()) / (numeric_trends.max() - numeric_trends.min())\n",
    "            \n",
    "            sns.heatmap(norm_data.T, annot=False, cmap='viridis', cbar_kws={'label': 'Normalized Value'})\n",
    "            plt.title('Heatmap of Numeric Features by Launch')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_results/launch_numeric_heatmap.png')\n",
    "        \n",
    "        # Time-to-conversion metrics\n",
    "        if 'target' in merged_data.columns and date_cols:\n",
    "            print(\"\\nAnalyzing time-to-conversion metrics...\")\n",
    "            \n",
    "            # Find survey date and purchase date columns\n",
    "            survey_date_col = None\n",
    "            purchase_date_col = None\n",
    "            \n",
    "            # Try to identify appropriate date columns\n",
    "            for col in date_cols:\n",
    "                if any(term in col.lower() for term in ['survey', 'pesquisa', 'respuesta']):\n",
    "                    survey_date_col = f\"{col}_dt\" if f\"{col}_dt\" in merged_data.columns else col\n",
    "                elif any(term in col.lower() for term in ['purchase', 'venda', 'compra']):\n",
    "                    purchase_date_col = f\"{col}_dt\" if f\"{col}_dt\" in merged_data.columns else col\n",
    "            \n",
    "            # If we couldn't identify specifically, use the first date column as survey date\n",
    "            if survey_date_col is None and date_cols:\n",
    "                survey_date_col = f\"{date_cols[0]}_dt\" if f\"{date_cols[0]}_dt\" in merged_data.columns else date_cols[0]\n",
    "            \n",
    "            # Calculate time to conversion if we have both dates\n",
    "            if survey_date_col and purchase_date_col and merged_data['target'].sum() > 0:\n",
    "                # Make sure both are datetime\n",
    "                if merged_data[survey_date_col].dtype != 'datetime64[ns]':\n",
    "                    merged_data[survey_date_col] = pd.to_datetime(merged_data[survey_date_col], errors='coerce')\n",
    "                if merged_data[purchase_date_col].dtype != 'datetime64[ns]':\n",
    "                    merged_data[purchase_date_col] = pd.to_datetime(merged_data[purchase_date_col], errors='coerce')\n",
    "                \n",
    "                # Calculate time to conversion for converted users\n",
    "                converters = merged_data[merged_data['target'] == 1].copy()\n",
    "                converters['time_to_conversion'] = (converters[purchase_date_col] - converters[survey_date_col]).dt.total_seconds() / (3600 * 24)  # in days\n",
    "                \n",
    "                # Filter out invalid values\n",
    "                valid_ttc = converters[converters['time_to_conversion'] >= 0]\n",
    "                \n",
    "                if len(valid_ttc) > 0:\n",
    "                    ttc_stats = valid_ttc['time_to_conversion'].describe()\n",
    "                    \n",
    "                    print(\"\\nTime to conversion statistics (in days):\")\n",
    "                    print(f\"  Mean: {ttc_stats['mean']:.2f}\")\n",
    "                    print(f\"  Median: {ttc_stats['50%']:.2f}\")\n",
    "                    print(f\"  Min: {ttc_stats['min']:.2f}\")\n",
    "                    print(f\"  Max: {ttc_stats['max']:.2f}\")\n",
    "                    \n",
    "                    # Plot time to conversion distribution\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    sns.histplot(valid_ttc['time_to_conversion'], bins=20, kde=True)\n",
    "                    plt.title('Time to Conversion Distribution (Days)')\n",
    "                    plt.xlabel('Days')\n",
    "                    plt.xlim(0, min(ttc_stats['75%'] * 3, ttc_stats['max']))  # Limit x-axis to 3x the 75th percentile\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('eda_results/time_to_conversion.png')\n",
    "                    \n",
    "                    # Time to conversion by launch\n",
    "                    if launch_col in valid_ttc.columns:\n",
    "                        ttc_by_launch = valid_ttc.groupby(launch_col)['time_to_conversion'].agg(['mean', 'median', 'count'])\n",
    "                        \n",
    "                        print(\"\\nTime to conversion by launch (in days):\")\n",
    "                        for launch, data in ttc_by_launch.iterrows():\n",
    "                            if pd.notna(launch) and data['count'] >= 5:  # At least 5 conversions\n",
    "                                print(f\"  {launch}: Mean = {data['mean']:.2f}, Median = {data['median']:.2f}, Count = {data['count']}\")\n",
    "                        \n",
    "                        # Plot median time to conversion by launch\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        ttc_by_launch = ttc_by_launch.sort_index()\n",
    "                        ttc_by_launch['median'].plot(kind='bar')\n",
    "                        plt.title('Median Time to Conversion by Launch')\n",
    "                        plt.ylabel('Days')\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig('eda_results/ttc_by_launch.png')\n",
    "        \n",
    "                # Análise de fluxo de leads entre lançamentos usando Sankey\n",
    "        print(\"\\nAnalisando fluxo de leads entre lançamentos...\")\n",
    "\n",
    "        # Verificar se temos informações suficientes para a análise\n",
    "        if 'email_norm' in merged_data.columns and 'lançamento' in merged_data.columns and 'target' in merged_data.columns:\n",
    "            try:\n",
    "                import plotly.graph_objects as go\n",
    "                import networkx as nx\n",
    "\n",
    "                # 1. Identificar primeira aparição de cada lead\n",
    "                # Ordenar dados para garantir que encontramos a primeira aparição\n",
    "                # Assumindo que temos alguma coluna de data ou que os índices são cronológicos\n",
    "                if date_cols:\n",
    "                    date_col = date_cols[0]\n",
    "                    if f\"{date_col}_dt\" in merged_data.columns:\n",
    "                        date_col = f\"{date_col}_dt\"\n",
    "\n",
    "                    # Criar coluna para ordenação\n",
    "                    if merged_data[date_col].dtype != 'datetime64[ns]':\n",
    "                        merged_data['temp_date'] = pd.to_datetime(merged_data[date_col], errors='coerce')\n",
    "                    else:\n",
    "                        merged_data['temp_date'] = merged_data[date_col]\n",
    "                else:\n",
    "                    # Se não temos data, usamos o índice como proxy para ordem cronológica\n",
    "                    merged_data['temp_date'] = merged_data.index\n",
    "\n",
    "                # Ordenar por email e data para pegar a primeira e última aparição de cada lead\n",
    "                sorted_data = merged_data.sort_values(by=['email_norm', 'temp_date'])\n",
    "\n",
    "                # Pegar a primeira aparição de cada lead em qualquer lançamento\n",
    "                first_appearance = sorted_data.drop_duplicates(subset='email_norm', keep='first')[['email_norm', 'lançamento']]\n",
    "                first_appearance.columns = ['email_norm', 'primeiro_lancamento']\n",
    "\n",
    "                # Identificar onde cada lead converteu\n",
    "                conversions = merged_data[merged_data['target'] == 1][['email_norm', 'lançamento']]\n",
    "                conversions.columns = ['email_norm', 'lancamento_conversao']\n",
    "\n",
    "                # Juntar informações de primeira aparição e conversão\n",
    "                flow_data = pd.merge(first_appearance, conversions, on='email_norm', how='inner')\n",
    "\n",
    "                # 2. Criar matriz de fluxo\n",
    "                flow_matrix = pd.crosstab(\n",
    "                    flow_data['primeiro_lancamento'], \n",
    "                    flow_data['lancamento_conversao'], \n",
    "                    dropna=False\n",
    "                )\n",
    "\n",
    "                # Preencher com zeros onde estiver faltando\n",
    "                launch_list = sorted(merged_data['lançamento'].dropna().unique())\n",
    "                for launch in launch_list:\n",
    "                    if launch not in flow_matrix.index:\n",
    "                        flow_matrix.loc[launch] = 0\n",
    "                    if launch not in flow_matrix.columns:\n",
    "                        flow_matrix[launch] = 0\n",
    "\n",
    "                # Ordenar índices e colunas para melhor visualização\n",
    "                flow_matrix = flow_matrix.reindex(index=launch_list, columns=launch_list, fill_value=0)\n",
    "\n",
    "                # Mostrar matriz de fluxo\n",
    "                print(\"\\nMatriz de fluxo de leads entre lançamentos (origem → conversão):\")\n",
    "                print(flow_matrix)\n",
    "\n",
    "                # 3. Criar dados para o diagrama Sankey\n",
    "                source = []  # Lançamento de origem\n",
    "                target = []  # Lançamento de conversão\n",
    "                value = []   # Quantidade de leads\n",
    "\n",
    "                # Criar mapeamento de lançamentos para índices\n",
    "                launch_to_idx = {launch: i for i, launch in enumerate(launch_list)}\n",
    "\n",
    "                # Preencher listas para o Sankey\n",
    "                for origem in flow_matrix.index:\n",
    "                    for destino in flow_matrix.columns:\n",
    "                        if flow_matrix.loc[origem, destino] > 0:\n",
    "                            source.append(launch_to_idx[origem])\n",
    "                            target.append(launch_to_idx[destino] + len(launch_list))  # Offset para os nós de destino\n",
    "                            value.append(float(flow_matrix.loc[origem, destino]))\n",
    "\n",
    "                # Criar rótulos: primeiro os lançamentos de origem, depois os de destino\n",
    "                labels = [f\"{l} (Origem)\" for l in launch_list] + [f\"{l} (Conversão)\" for l in launch_list]\n",
    "\n",
    "                # Criar figura Sankey\n",
    "                fig = go.Figure(data=[go.Sankey(\n",
    "                    node=dict(\n",
    "                        pad=15,\n",
    "                        thickness=20,\n",
    "                        line=dict(color=\"black\", width=0.5),\n",
    "                        label=labels,\n",
    "                        color=\"blue\"\n",
    "                    ),\n",
    "                    link=dict(\n",
    "                        source=source,\n",
    "                        target=target,\n",
    "                        value=value\n",
    "                    )\n",
    "                )])\n",
    "\n",
    "                fig.update_layout(\n",
    "                    title_text=\"Fluxo de Leads entre Lançamentos\",\n",
    "                    font_size=10,\n",
    "                    width=1000,\n",
    "                    height=600\n",
    "                )\n",
    "\n",
    "                # Salvar como HTML interativo\n",
    "                fig.write_html(\"eda_results/lead_flow_sankey.html\")\n",
    "\n",
    "                # Tentar salvar como imagem estática, com tratamento de erro robusto\n",
    "                try:\n",
    "                    fig.write_image(\"eda_results/lead_flow_sankey.png\")\n",
    "                    print(\"Diagrama Sankey salvo como 'eda_results/lead_flow_sankey.html' (interativo) e '.png' (estático)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Não foi possível salvar a imagem estática do diagrama Sankey: {e}\")\n",
    "                    print(\"O diagrama interativo foi salvo como 'eda_results/lead_flow_sankey.html'\")\n",
    "                    \n",
    "                # 4. Análise adicional de conversão entre lançamentos\n",
    "                print(\"\\nResumo de conversões entre lançamentos:\")\n",
    "\n",
    "                # Total de conversões no mesmo lançamento (diagonal principal)\n",
    "                same_launch_conv = sum(flow_matrix[col][col] for col in flow_matrix.columns if col in flow_matrix.index)\n",
    "\n",
    "                # Total de conversões em lançamentos diferentes\n",
    "                diff_launch_conv = flow_matrix.sum().sum() - same_launch_conv\n",
    "\n",
    "                print(f\"- Leads que converteram no mesmo lançamento: {same_launch_conv}\")\n",
    "                print(f\"- Leads que converteram em um lançamento diferente: {diff_launch_conv}\")\n",
    "\n",
    "                if diff_launch_conv > 0:\n",
    "                    # Para cada lançamento de origem, mostrar para onde foram os leads\n",
    "                    for origem in launch_list:\n",
    "                        destinos = flow_matrix.loc[origem]\n",
    "                        total_from_origin = destinos.sum()\n",
    "\n",
    "                        if total_from_origin > 0:\n",
    "                            # Filtrar apenas conversões em outros lançamentos\n",
    "                            other_launches = destinos[destinos.index != origem]\n",
    "                            if other_launches.sum() > 0:\n",
    "                                print(f\"\\nDe {origem}, {other_launches.sum()} lead(s) converteram em outros lançamentos:\")\n",
    "                                for destino, count in other_launches.items():\n",
    "                                    if count > 0:\n",
    "                                        pct = (count / total_from_origin) * 100\n",
    "                                        print(f\"  → {destino}: {count} leads ({pct:.1f}% dos leads de {origem})\")\n",
    "\n",
    "            except ImportError as e:\n",
    "                print(f\"Visualização Sankey requer bibliotecas adicionais: {e}\")\n",
    "                print(\"Execute 'pip install plotly' para habilitar esta visualização\")\n",
    "\n",
    "                # Cair para uma versão simplificada sem Sankey\n",
    "                # Criar e mostrar apenas a matriz de fluxo\n",
    "                flow_matrix = pd.crosstab(\n",
    "                    flow_data['primeiro_lancamento'], \n",
    "                    flow_data['lancamento_conversao'], \n",
    "                    dropna=False\n",
    "                )\n",
    "                print(\"\\nMatriz de fluxo de leads entre lançamentos (origem → conversão):\")\n",
    "                print(flow_matrix)\n",
    "\n",
    "        else:\n",
    "            print(\"Faltam colunas necessárias (email_norm, lançamento ou target) para análise de fluxo de leads.\")\n",
    "    \n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"EDA SUMMARY\")\n",
    "    print(\"=============================================\\n\")\n",
    "    \n",
    "    print(\"Exploratory Data Analysis completed successfully!\")\n",
    "    print(f\"Generated visualizations and insights saved to 'eda_results' directory.\")\n",
    "    \n",
    "    if 'target' in merged_data.columns:\n",
    "        conv_rate = merged_data['target'].mean() * 100\n",
    "        print(f\"\\nOverall conversion rate: {conv_rate:.2f}%\")\n",
    "        print(f\"Total conversions: {merged_data['target'].sum()} out of {len(merged_data)} records\")\n",
    "        \n",
    "        # Summarize key insights\n",
    "        print(\"\\nKey insights:\")\n",
    "        \n",
    "        # Summarize high correlation features\n",
    "        if 'target_corr' in locals() and not target_corr.empty:\n",
    "            top_positive = target_corr[target_corr > 0].drop('target', errors='ignore').nlargest(3)\n",
    "            if not top_positive.empty:\n",
    "                print(\"Top positive correlations with conversion:\")\n",
    "                for feat, corr in top_positive.items():\n",
    "                    print(f\"  - {feat}: {corr:.4f}\")\n",
    "        \n",
    "        # Análise de países expandida (top 5)\n",
    "        if 'country_conversion_filtered' in locals() and not country_conversion_filtered.empty:\n",
    "            top_countries = country_conversion_filtered.head(5)  # Alterado de 3 para 5\n",
    "            if not top_countries.empty:\n",
    "                print(\"\\nPaíses com melhor conversão (top 5):\")\n",
    "                for country, data in top_countries.iterrows():\n",
    "                    print(f\"  - {country}: {data['conversion_rate']:.2f}% ({data['sum']} conversões em {data['count']} registros)\")\n",
    "        \n",
    "        # Summarize UTM insights\n",
    "        utm_insights = []\n",
    "        for utm_col in utm_cols:\n",
    "            utm_var_name = f\"{utm_col}_conversion\"\n",
    "            if utm_var_name in locals() and not locals()[utm_var_name].empty:\n",
    "                utm_data = locals()[utm_var_name]\n",
    "                if not utm_data.empty:\n",
    "                    top_utm = utm_data.head(1)\n",
    "                    for utm_val, data in top_utm.iterrows():\n",
    "                        utm_insights.append((utm_col, utm_val, data['conversion_rate'], data['count']))\n",
    "        \n",
    "        if utm_insights:\n",
    "            print(\"\\nTop converting UTM values:\")\n",
    "            for utm_col, utm_val, rate, count in sorted(utm_insights, key=lambda x: x[2], reverse=True)[:3]:\n",
    "                utm_display = str(utm_val)[:30] + '...' if isinstance(utm_val, str) and len(str(utm_val)) > 30 else utm_val\n",
    "                print(f\"  - {utm_col}: {utm_display} ({rate:.2f}%, {count} records)\")\n",
    "        \n",
    "        # Summarize launch insights\n",
    "        if 'launch_col' in locals() and launch_col in merged_data.columns:\n",
    "            if 'launch_conversion' in locals() and not launch_conversion.empty:\n",
    "                top_launches = launch_conversion.sort_values('conversion_rate', ascending=False).head(3)\n",
    "                \n",
    "                print(\"\\nBest converting launches:\")\n",
    "                for launch, data in top_launches.iterrows():\n",
    "                    if pd.notna(launch) and pd.notna(data['conversion_rate']):\n",
    "                        print(f\"  - {launch}: {data['conversion_rate']:.2f}% ({data['sum']} conversions out of {data['count']} records)\")\n",
    "else:\n",
    "    print(\"No data to analyze. Please ensure the dataset is loaded properly.\")\n",
    "\n",
    "# Restaurar a saída original e salvar o relatório\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "# Salvar relatório como arquivo de texto\n",
    "with open('eda_results/eda_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report_content.getvalue())\n",
    "\n",
    "print(f\"EDA Report saved to 'eda_results/eda_report.txt'\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
