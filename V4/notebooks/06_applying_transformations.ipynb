{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4940ed-31ed-4855-8445-bf2dd864c190",
   "metadata": {},
   "source": [
    "# Aplicação das transformações no CV e Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b48329-5ad2-4d26-bcd5-188f476c5a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando existência dos arquivos de entrada:\n",
      "  Train path: /home/jupyter/smart_ads/data/split2/train0.csv - Existe: True\n",
      "  CV path: /home/jupyter/smart_ads/data/split2/validation.csv - Existe: True\n",
      "  Test path: /home/jupyter/smart_ads/data/split2/test.csv - Existe: True\n",
      "Carregando datasets de /home/jupyter/smart_ads/data/split2...\n",
      "Datasets carregados: treino (97174, 48), validação (20823, 48), teste (20823, 48)\n",
      "\n",
      "--- Processando conjunto de treinamento ---\n",
      "Iniciando pipeline de pré-processamento para DataFrame: (97174, 48)\n",
      "1. Normalizando emails...\n",
      "2. Consolidando colunas de qualidade...\n",
      "3. Tratando valores ausentes...\n",
      "4. Tratando outliers...\n",
      "5. Normalizando valores numéricos...\n",
      "6. Convertendo tipos de dados...\n",
      "7. Aplicando feature engineering não-textual...\n",
      "8. Processando features textuais...\n",
      "Pipeline concluída! Dimensões finais: (97174, 404)\n",
      "Parâmetros de pré-processamento salvos em /home/jupyter/smart_ads/notebooks/models/preprocessing_params/all_preprocessing_params.joblib\n",
      "Dataset de treino processado e salvo em /home/jupyter/smart_ads/notebooks/datasets/split2/train.csv\n",
      "\n",
      "--- Processando conjunto de validação ---\n",
      "Iniciando pipeline de pré-processamento para DataFrame: (20823, 48)\n",
      "1. Normalizando emails...\n",
      "2. Consolidando colunas de qualidade...\n",
      "3. Tratando valores ausentes...\n",
      "4. Tratando outliers...\n",
      "5. Normalizando valores numéricos...\n",
      "6. Convertendo tipos de dados...\n",
      "7. Aplicando feature engineering não-textual...\n",
      "8. Processando features textuais...\n",
      "Pipeline concluída! Dimensões finais: (20823, 404)\n",
      "Alinhando colunas entre conjuntos de dados...\n",
      "  Adicionada coluna ausente: utm_month_encoded\n",
      "  Removidas colunas extras: utm_month_freq\n",
      "Alinhamento concluído: 1 colunas adicionadas, 1 removidas\n",
      "Dataset de validação processado e salvo em /home/jupyter/smart_ads/notebooks/datasets/split2/validation.csv\n",
      "\n",
      "--- Processando conjunto de teste ---\n",
      "Iniciando pipeline de pré-processamento para DataFrame: (20823, 48)\n",
      "1. Normalizando emails...\n",
      "2. Consolidando colunas de qualidade...\n",
      "3. Tratando valores ausentes...\n",
      "4. Tratando outliers...\n",
      "5. Normalizando valores numéricos...\n",
      "6. Convertendo tipos de dados...\n",
      "7. Aplicando feature engineering não-textual...\n",
      "8. Processando features textuais...\n",
      "Pipeline concluída! Dimensões finais: (20823, 404)\n",
      "Alinhando colunas entre conjuntos de dados...\n",
      "  Adicionada coluna ausente: utm_month_encoded\n",
      "  Removidas colunas extras: utm_month_freq\n",
      "Alinhamento concluído: 1 colunas adicionadas, 1 removidas\n",
      "Dataset de teste processado e salvo em /home/jupyter/smart_ads/notebooks/datasets/split2/test.csv\n",
      "\n",
      "Pré-processamento dos conjuntos concluído com sucesso!\n",
      "Os datasets processados foram salvos em /home/jupyter/smart_ads/notebooks/datasets/split2/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script para aplicar a pipeline de pré-processamento nos conjuntos de treino, validação e teste,\n",
    "garantindo que as mesmas transformações sejam aplicadas em todos.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Adiciona diretório pai ao path para encontrar os módulos\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports dos módulos de pré-processamento\n",
    "from src.preprocessing.email_processing import normalize_emails_in_dataframe\n",
    "from src.preprocessing.data_cleaning import (\n",
    "    consolidate_quality_columns,\n",
    "    handle_missing_values,\n",
    "    handle_outliers,\n",
    "    normalize_values,\n",
    "    convert_data_types\n",
    ")\n",
    "from src.preprocessing.feature_engineering import feature_engineering\n",
    "from src.preprocessing.text_processing import text_feature_engineering\n",
    "\n",
    "def apply_preprocessing_pipeline(df, params=None, fit=False):\n",
    "    \"\"\"\n",
    "    Aplica a pipeline completa de pré-processamento.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a ser processado\n",
    "        params: Parâmetros para transformações (None para começar do zero)\n",
    "        fit: Se True, ajusta as transformações, se False, apenas aplica\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame processado e parâmetros atualizados\n",
    "    \"\"\"\n",
    "    # Inicializar parâmetros se não fornecidos\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    print(f\"Iniciando pipeline de pré-processamento para DataFrame: {df.shape}\")\n",
    "    \n",
    "    # 1. Normalizar emails\n",
    "    print(\"1. Normalizando emails...\")\n",
    "    df = normalize_emails_in_dataframe(df, email_col='email')\n",
    "    \n",
    "    # 2. Consolidar colunas de qualidade\n",
    "    print(\"2. Consolidando colunas de qualidade...\")\n",
    "    quality_params = params.get('quality_columns', {})\n",
    "    df, quality_params = consolidate_quality_columns(df, fit=fit, params=quality_params)\n",
    "    \n",
    "    # 3. Tratamento de valores ausentes\n",
    "    print(\"3. Tratando valores ausentes...\")\n",
    "    missing_params = params.get('missing_values', {})\n",
    "    df, missing_params = handle_missing_values(df, fit=fit, params=missing_params)\n",
    "    \n",
    "    # 4. Tratamento de outliers\n",
    "    print(\"4. Tratando outliers...\")\n",
    "    outlier_params = params.get('outliers', {})\n",
    "    df, outlier_params = handle_outliers(df, fit=fit, params=outlier_params)\n",
    "    \n",
    "    # 5. Normalização de valores\n",
    "    print(\"5. Normalizando valores numéricos...\")\n",
    "    norm_params = params.get('normalization', {})\n",
    "    df, norm_params = normalize_values(df, fit=fit, params=norm_params)\n",
    "    \n",
    "    # 6. Converter tipos de dados\n",
    "    print(\"6. Convertendo tipos de dados...\")\n",
    "    df, _ = convert_data_types(df, fit=fit)\n",
    "    \n",
    "    # 7. Feature engineering não-textual\n",
    "    print(\"7. Aplicando feature engineering não-textual...\")\n",
    "    feature_params = params.get('feature_engineering', {})\n",
    "    df, feature_params = feature_engineering(df, fit=fit, params=feature_params)\n",
    "    \n",
    "    # 8. Processamento de texto\n",
    "    print(\"8. Processando features textuais...\")\n",
    "    text_params = params.get('text_processing', {})\n",
    "    df, text_params = text_feature_engineering(df, fit=fit, params=text_params)\n",
    "    \n",
    "    # 9. Compilar parâmetros atualizados\n",
    "    updated_params = {\n",
    "        'quality_columns': quality_params,\n",
    "        'missing_values': missing_params,\n",
    "        'outliers': outlier_params,\n",
    "        'normalization': norm_params,\n",
    "        'feature_engineering': feature_params,\n",
    "        'text_processing': text_params\n",
    "    }\n",
    "    \n",
    "    print(f\"Pipeline concluída! Dimensões finais: {df.shape}\")\n",
    "    return df, updated_params\n",
    "\n",
    "def ensure_column_consistency(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Garante que o DataFrame de teste tenha as mesmas colunas que o de treinamento.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame de treinamento\n",
    "        test_df: DataFrame de teste para alinhar\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame de teste com colunas alinhadas\n",
    "    \"\"\"\n",
    "    print(\"Alinhando colunas entre conjuntos de dados...\")\n",
    "    \n",
    "    # Colunas presentes no treino, mas ausentes no teste\n",
    "    missing_cols = set(train_df.columns) - set(test_df.columns)\n",
    "    \n",
    "    # Adicionar colunas faltantes com valores padrão\n",
    "    for col in missing_cols:\n",
    "        if col in train_df.select_dtypes(include=['number']).columns:\n",
    "            test_df[col] = 0\n",
    "        else:\n",
    "            test_df[col] = None\n",
    "        print(f\"  Adicionada coluna ausente: {col}\")\n",
    "    \n",
    "    # Remover colunas extras no teste não presentes no treino\n",
    "    extra_cols = set(test_df.columns) - set(train_df.columns)\n",
    "    if extra_cols:\n",
    "        test_df = test_df.drop(columns=list(extra_cols))\n",
    "        print(f\"  Removidas colunas extras: {', '.join(list(extra_cols)[:5])}\" + \n",
    "              (f\" e mais {len(extra_cols)-5} outras\" if len(extra_cols) > 5 else \"\"))\n",
    "    \n",
    "    # Garantir a mesma ordem de colunas\n",
    "    test_df = test_df[train_df.columns]\n",
    "    \n",
    "    print(f\"Alinhamento concluído: {len(missing_cols)} colunas adicionadas, {len(extra_cols)} removidas\")\n",
    "    return test_df\n",
    "\n",
    "# Função principal - adaptada para notebook\n",
    "def process_datasets():\n",
    "    \"\"\"\n",
    "    Função principal que processa todos os conjuntos na ordem correta.\n",
    "    \"\"\"\n",
    "    # 1. Definir caminhos dos datasets\n",
    "    # Considerando que estamos em smart_ads/notebooks e precisamos acessar smart_ads/data/split2\n",
    "    base_dir = os.path.dirname(os.getcwd())  # Sobe um nível para smart_ads\n",
    "    base_data_dir = os.path.join(base_dir, \"data\", \"split2\")\n",
    "    \n",
    "    train_path = os.path.join(base_data_dir, \"train0.csv\")\n",
    "    cv_path = os.path.join(base_data_dir, \"validation.csv\")\n",
    "    test_path = os.path.join(base_data_dir, \"test.csv\")\n",
    "    \n",
    "    # Garantir que o diretório de saída existe\n",
    "    output_dir = os.path.join(os.getcwd(), \"datasets\", \"split2\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Verificar se os arquivos existem\n",
    "    print(f\"Verificando existência dos arquivos de entrada:\")\n",
    "    print(f\"  Train path: {train_path} - Existe: {os.path.exists(train_path)}\")\n",
    "    print(f\"  CV path: {cv_path} - Existe: {os.path.exists(cv_path)}\")\n",
    "    print(f\"  Test path: {test_path} - Existe: {os.path.exists(test_path)}\")\n",
    "    \n",
    "    # 2. Carregar os datasets\n",
    "    print(f\"Carregando datasets de {base_data_dir}...\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    cv_df = pd.read_csv(cv_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    print(f\"Datasets carregados: treino {train_df.shape}, validação {cv_df.shape}, teste {test_df.shape}\")\n",
    "    \n",
    "    # 3. Processar o conjunto de treinamento com fit=True para aprender parâmetros\n",
    "    print(\"\\n--- Processando conjunto de treinamento ---\")\n",
    "    train_processed, params = apply_preprocessing_pipeline(train_df, fit=True)\n",
    "    \n",
    "    # 4. Salvar parâmetros aprendidos\n",
    "    params_dir = os.path.join(current_dir, \"models\", \"preprocessing_params\")\n",
    "    os.makedirs(params_dir, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(params, os.path.join(params_dir, \"all_preprocessing_params.joblib\"))\n",
    "    print(f\"Parâmetros de pré-processamento salvos em {params_dir}/all_preprocessing_params.joblib\")\n",
    "    \n",
    "    # 5. Salvar conjunto de treino processado\n",
    "    train_processed.to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
    "    print(f\"Dataset de treino processado e salvo em {output_dir}/train.csv\")\n",
    "    \n",
    "    # 6. Processar o conjunto de validação com fit=False para aplicar parâmetros aprendidos\n",
    "    print(\"\\n--- Processando conjunto de validação ---\")\n",
    "    cv_processed, _ = apply_preprocessing_pipeline(cv_df, params=params, fit=False)\n",
    "    \n",
    "    # 7. Garantir consistência de colunas com o treino\n",
    "    cv_processed = ensure_column_consistency(train_processed, cv_processed)\n",
    "    \n",
    "    # 8. Salvar conjunto de validação processado\n",
    "    cv_processed.to_csv(os.path.join(output_dir, \"validation.csv\"), index=False)\n",
    "    print(f\"Dataset de validação processado e salvo em {output_dir}/validation.csv\")\n",
    "    \n",
    "    # 9. Processar o conjunto de teste com fit=False para aplicar parâmetros aprendidos\n",
    "    print(\"\\n--- Processando conjunto de teste ---\")\n",
    "    test_processed, _ = apply_preprocessing_pipeline(test_df, params=params, fit=False)\n",
    "    \n",
    "    # 10. Garantir consistência de colunas com o treino\n",
    "    test_processed = ensure_column_consistency(train_processed, test_processed)\n",
    "    \n",
    "    # 11. Salvar conjunto de teste processado\n",
    "    test_processed.to_csv(os.path.join(output_dir, \"test.csv\"), index=False)\n",
    "    print(f\"Dataset de teste processado e salvo em {output_dir}/test.csv\")\n",
    "    \n",
    "    print(\"\\nPré-processamento dos conjuntos concluído com sucesso!\")\n",
    "    print(f\"Os datasets processados foram salvos em {output_dir}/\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_processed,\n",
    "        'cv': cv_processed,\n",
    "        'test': test_processed,\n",
    "        'params': params\n",
    "    }\n",
    "\n",
    "# Execute a função principal\n",
    "results = process_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9f7cf-fe2e-4901-b5ca-da2f7a0e79c1",
   "metadata": {},
   "source": [
    "# Feature importance and selection no cv set e training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b626bd-9c3f-4152-bde5-5b42f1d3b55e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'reports/eda_results/feature_importance_results' já existe\n",
      "Carregando dataset...\n",
      "Dataset de treino carregado: (97174, 404)\n",
      "Dataset de validação carregado: (20823, 404)\n",
      "Dataset de teste carregado: (20823, 404)\n",
      "Dataset combinado: 138820 linhas, 404 colunas\n",
      "\n",
      "Identificando colunas importantes...\n",
      "Coluna de lançamento encontrada: 'lançamento'\n",
      "Número de lançamentos: 6\n",
      "Lançamentos identificados: ['L16', 'L17', 'L18', 'L19', 'L20', 'L21']\n",
      "Distribuição de lançamentos:\n",
      "lançamento\n",
      "L21    22.577438\n",
      "L16    20.629592\n",
      "L17    16.944965\n",
      "L20    14.325746\n",
      "L19    12.839648\n",
      "L18    12.682611\n",
      "Name: proportion, dtype: float64\n",
      "Removendo 2 colunas com variância zero\n",
      "Features derivadas de texto identificadas: 270\n",
      "Exemplos de features textuais:\n",
      "  - name_length\n",
      "  - name_word_count\n",
      "  - Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?_length\n",
      "  - Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?_word_count\n",
      "  - Cuando hables inglés con fluidez, ¿qué cambiará en tu vida? ¿Qué oportunidades se abrirán para ti?_has_question\n",
      "  - ... e mais 265 features textuais\n",
      "Sanitizando nomes das features para evitar problemas com caracteres especiais...\n",
      "Renomeando 329 colunas para evitar erros com caracteres especiais\n",
      "Usando 368 features numéricas para análise\n",
      "Distribuição do target: target\n",
      "0    98.645728\n",
      "1     1.354272\n",
      "Name: proportion, dtype: float64\n",
      "--- Análise de Multicolinearidade ---\n",
      "Encontrados 82 pares de features com correlação > 0.8:\n",
      "1. utm_year & utm_hour_encoded: -1.0000\n",
      "2. D_jame_un_mensaje_length & D_jame_un_mensaje_word_count: 0.9925\n",
      "3. _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__length & _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__word_count: 0.9901\n",
      "4. _Qu__esperas_aprender_en_la_Semana_de_Cero_a_Ingl_s_Fluido__tfidf_hablar_entender & _Qu__esperas_aprender_en_la_Semana_de_Cero_a_Ingl_s_Fluido__high_conv_term_hablar_entender: 0.9889\n",
      "5. Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__tfidf_mejores_oportunidades & Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__high_conv_term_mejores_oportunidades: 0.9887\n",
      "6. _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__tfidf_hablar_fluido & _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__high_conv_term_hablar_fluido: 0.9867\n",
      "7. Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__length & Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__word_count: 0.9857\n",
      "8. _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__tfidf_entender_ingl_s & _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__low_conv_term_entender_ingl_s: 0.9852\n",
      "9. _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__tfidf_aprender_ingl_s & _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__low_conv_term_aprender_ingl_s: 0.9846\n",
      "10. _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__has_any_high_conv_term & _Qu__esperas_aprender_en_la_Inmersi_n_Desbloquea_Tu_Ingl_s_En_72_horas__num_high_conv_terms: 0.9845\n",
      "... e mais 72 pares.\n",
      "--- Análise de Redundância: country_freq vs country_encoded ---\n",
      "Correlação entre country_freq e country_encoded: -0.0797 (p-value: 0.0000)\n",
      "Correlação com target:\n",
      "- country_freq: -0.0231\n",
      "- country_encoded: -0.0126\n",
      "Recomendação: country_freq parece ter maior valor preditivo.\n",
      "Treinamento: 111056 amostras, Validação: 27764 amostras\n",
      "Proporção da classe positiva no treino: 1.35%\n",
      "Proporção da classe positiva na validação: 1.35%\n",
      "\n",
      "--- Iniciando análise de importância de features ---\n",
      "Analisando com RandomForest e validação cruzada para dados desbalanceados...\n",
      "\n",
      "Fold 1/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7849\n",
      "  Average Precision: 0.0680\n",
      "  Melhor F1-Score: 0.1346 (threshold: 0.6123)\n",
      "\n",
      "Fold 2/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7858\n",
      "  Average Precision: 0.0565\n",
      "  Melhor F1-Score: 0.1222 (threshold: 0.6131)\n",
      "\n",
      "Fold 3/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7863\n",
      "  Average Precision: 0.0630\n",
      "  Melhor F1-Score: 0.1146 (threshold: 0.5965)\n",
      "\n",
      "Fold 4/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7939\n",
      "  Average Precision: 0.0610\n",
      "  Melhor F1-Score: 0.1243 (threshold: 0.6309)\n",
      "\n",
      "Fold 5/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7880\n",
      "  Average Precision: 0.0631\n",
      "  Melhor F1-Score: 0.1190 (threshold: 0.6377)\n",
      "\n",
      "Métricas médias da validação cruzada (RandomForest):\n",
      "  AUC: 0.7878 (±0.0032)\n",
      "  AP: 0.0623 (±0.0037)\n",
      "  F1: 0.1229 (±0.0067)\n",
      "\n",
      "Top 15 features (RandomForest):\n",
      "                                              Feature  Importance_RF\n",
      "16                                        age_encoded       0.075632\n",
      "19                             current_salary_encoded       0.075089\n",
      "24                                       country_freq       0.043926\n",
      "20                             desired_salary_encoded       0.033427\n",
      "50                           D_jame_un_mensaje_length       0.027908\n",
      "25                                    country_encoded       0.025709\n",
      "26                                    profession_freq       0.025072\n",
      "29                                    UTM_SOURCE_freq       0.023772\n",
      "31                                   UTM_CONTENT_freq       0.023479\n",
      "17                                 time_known_encoded       0.022519\n",
      "51                       D_jame_un_mensaje_word_count       0.021042\n",
      "54                  D_jame_un_mensaje_avg_word_length       0.018800\n",
      "44  Cuando_hables_ingl_s_con_fluidez___qu__cambiar...       0.018702\n",
      "28                                  UTM_CAMPAING_freq       0.017796\n",
      "1                                         name_length       0.017656\n",
      "Analisando com LightGBM...\n",
      "\n",
      "Fold 1/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7949\n",
      "  Average Precision: 0.0779\n",
      "  Melhor F1-Score: 0.1608 (threshold: 0.7870)\n",
      "\n",
      "Fold 2/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7953\n",
      "  Average Precision: 0.1545\n",
      "  Melhor F1-Score: 0.2137 (threshold: 0.7677)\n",
      "\n",
      "Fold 3/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7979\n",
      "  Average Precision: 0.0787\n",
      "  Melhor F1-Score: 0.1528 (threshold: 0.7483)\n",
      "\n",
      "Fold 4/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7967\n",
      "  Average Precision: 0.0893\n",
      "  Melhor F1-Score: 0.1657 (threshold: 0.7733)\n",
      "\n",
      "Fold 5/5\n",
      "Desempenho do modelo:\n",
      "  AUC: 0.7830\n",
      "  Average Precision: 0.0633\n",
      "  Melhor F1-Score: 0.1444 (threshold: 0.7662)\n",
      "\n",
      "Métricas médias da validação cruzada (LightGBM):\n",
      "  AUC: 0.7936 (±0.0054)\n",
      "  AP: 0.0927 (±0.0320)\n",
      "  F1: 0.1675 (±0.0242)\n",
      "\n",
      "Top 15 features (LightGBM):\n",
      "                                              Feature  Importance_LGB\n",
      "16                                        age_encoded   401548.037900\n",
      "19                             current_salary_encoded   182607.984705\n",
      "29                                    UTM_SOURCE_freq   102745.138720\n",
      "24                                       country_freq    95366.776885\n",
      "25                                    country_encoded    62089.465253\n",
      "31                                   UTM_CONTENT_freq    41463.569665\n",
      "51                       D_jame_un_mensaje_word_count    38446.808719\n",
      "50                           D_jame_un_mensaje_length    36530.471339\n",
      "17                                 time_known_encoded    36037.577145\n",
      "45  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...    35311.670696\n",
      "1                                         name_length    30310.682621\n",
      "26                                    profession_freq    29687.351316\n",
      "54                  D_jame_un_mensaje_avg_word_length    29256.248953\n",
      "28                                  UTM_CAMPAING_freq    28911.664179\n",
      "44  Cuando_hables_ingl_s_con_fluidez___qu__cambiar...    26536.802519\n",
      "Analisando com XGBoost...\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "Métricas médias da validação cruzada (XGBoost):\n",
      "  AUC: 0.7919 (±0.0051)\n",
      "  AP: 0.0804 (±0.0065)\n",
      "  F1: 0.1459 (±0.0033)\n",
      "\n",
      "Top 15 features (XGBoost):\n",
      "                                              Feature  Importance_XGB\n",
      "0                                         age_encoded      664.810193\n",
      "1                                     UTM_SOURCE_freq      628.296924\n",
      "2     D_jame_un_mensaje_low_conv_term_espero_aprender      407.156750\n",
      "3                                  time_known_encoded      389.361218\n",
      "4   _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...      364.583136\n",
      "5                                     country_encoded      363.264600\n",
      "6                                           has_gclid      358.136737\n",
      "7            D_jame_un_mensaje_has_any_high_conv_term      353.312805\n",
      "8                                            utm_year      335.734113\n",
      "9                              current_salary_encoded      331.670203\n",
      "10                       D_jame_un_mensaje_word_count      318.960602\n",
      "11                                       country_freq      318.143958\n",
      "12            D_jame_un_mensaje_has_any_low_conv_term      306.028772\n",
      "13  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...      294.038565\n",
      "14  Cuando_hables_ingl_s_con_fluidez___qu__cambiar...      289.123279\n",
      "Combinando resultados de diferentes métodos...\n",
      "\n",
      "Importância combinada (top 20 features):\n",
      "                                               Feature  Mean_Importance  \\\n",
      "335                                        age_encoded         9.300117   \n",
      "341                             current_salary_encoded         5.605519   \n",
      "340                                       country_freq         3.181804   \n",
      "169                                    UTM_SOURCE_freq         2.834618   \n",
      "339                                    country_encoded         2.080059   \n",
      "92                            D_jame_un_mensaje_length         1.642224   \n",
      "167                                   UTM_CONTENT_freq         1.601832   \n",
      "358                                 time_known_encoded         1.580397   \n",
      "345                             desired_salary_encoded         1.525778   \n",
      "163                       D_jame_un_mensaje_word_count         1.521902   \n",
      "356                                    profession_freq         1.421864   \n",
      "82                   D_jame_un_mensaje_avg_word_length         1.207277   \n",
      "166                                  UTM_CAMPAING_freq         1.182071   \n",
      "353                                        name_length         1.172511   \n",
      "0    Cuando_hables_ingl_s_con_fluidez___qu__cambiar...         1.149911   \n",
      "263  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         1.104423   \n",
      "10   Cuando_hables_ingl_s_con_fluidez___qu__cambiar...         0.999106   \n",
      "168                                    UTM_MEDIUM_freq         0.960198   \n",
      "352                                              month         0.953362   \n",
      "253  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         0.869683   \n",
      "\n",
      "     Std_Importance        CV  \n",
      "335        8.956707  0.963075  \n",
      "341        4.313166  0.769450  \n",
      "340        2.201364  0.691860  \n",
      "169        1.840892  0.649432  \n",
      "339        1.182013  0.568259  \n",
      "92         1.193942  0.727027  \n",
      "167        0.977064  0.609967  \n",
      "358        0.741709  0.469318  \n",
      "345        1.577808  1.034101  \n",
      "163        0.774850  0.509133  \n",
      "356        1.076786  0.757306  \n",
      "82         0.776452  0.643143  \n",
      "166        0.708890  0.599701  \n",
      "353        0.758516  0.646915  \n",
      "0          0.778526  0.677032  \n",
      "263        0.677391  0.613343  \n",
      "10         0.575687  0.576203  \n",
      "168        0.616218  0.641761  \n",
      "352        0.414697  0.434984  \n",
      "253        0.457799  0.526398  \n",
      "\n",
      "Importância das features salva em reports/eda_results/feature_importance_results/feature_importance_combined.csv\n",
      "\n",
      "--- Análise de Robustez entre Lançamentos ---\n",
      "\n",
      "Analisando lançamento: L21 (31342 amostras)\n",
      "\n",
      "Analisando lançamento: L16 (28638 amostras)\n",
      "\n",
      "Analisando lançamento: L17 (23523 amostras)\n",
      "\n",
      "Analisando lançamento: L20 (19887 amostras)\n",
      "\n",
      "Analisando lançamento: L19 (17824 amostras)\n",
      "\n",
      "Analisando lançamento: L18 (17606 amostras)\n",
      "\n",
      "Importância média entre lançamentos (top 15 features):\n",
      "                                               Feature  Mean_Launch_Imp  \\\n",
      "341                             current_salary_encoded         7.178437   \n",
      "335                                        age_encoded         5.471585   \n",
      "339                                    country_encoded         3.499213   \n",
      "340                                       country_freq         3.105987   \n",
      "167                                   UTM_CONTENT_freq         2.687921   \n",
      "92                            D_jame_un_mensaje_length         2.583926   \n",
      "263  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         2.457472   \n",
      "82                   D_jame_un_mensaje_avg_word_length         2.377212   \n",
      "356                                    profession_freq         2.326728   \n",
      "345                             desired_salary_encoded         2.301521   \n",
      "163                       D_jame_un_mensaje_word_count         2.042895   \n",
      "352                                              month         2.002477   \n",
      "253  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         1.916116   \n",
      "166                                  UTM_CAMPAING_freq         1.850755   \n",
      "334  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         1.838355   \n",
      "\n",
      "     Std_Launch_Imp  CV_Launch  \n",
      "341        3.139509   0.437353  \n",
      "335        1.845584   0.337303  \n",
      "339        2.334607   0.667181  \n",
      "340        1.556596   0.501160  \n",
      "167        1.086000   0.404030  \n",
      "92         0.784567   0.303634  \n",
      "263        1.105336   0.449786  \n",
      "82         1.420175   0.597412  \n",
      "356        0.683465   0.293745  \n",
      "345        1.298319   0.564114  \n",
      "163        0.609848   0.298521  \n",
      "352        1.719079   0.858476  \n",
      "253        0.494243   0.257940  \n",
      "166        0.624568   0.337467  \n",
      "334        1.361641   0.740684  \n",
      "\n",
      "Features com alta variabilidade entre lançamentos:\n",
      "                                               Feature  Mean_Launch_Imp  \\\n",
      "346                                     gender_encoded         0.510633   \n",
      "338                                belief_work_encoded         0.542774   \n",
      "337                              belief_salary_encoded         0.840664   \n",
      "252  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...         1.253164   \n",
      "367                                               year         1.038398   \n",
      "171  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...         0.869993   \n",
      "181  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...         1.030878   \n",
      "\n",
      "     CV_Launch  \n",
      "346   1.906687  \n",
      "338   1.883540  \n",
      "337   1.742069  \n",
      "252   1.518557  \n",
      "367   1.505097  \n",
      "171   1.429163  \n",
      "181   1.237231  \n",
      "\n",
      "Features consistentemente importantes entre lançamentos:\n",
      "                                              Feature  Mean_Importance  \\\n",
      "1                                         age_encoded         9.300117   \n",
      "0                              current_salary_encoded         5.605519   \n",
      "3                                        country_freq         3.181804   \n",
      "17                                    UTM_SOURCE_freq         2.834618   \n",
      "2                                     country_encoded         2.080059   \n",
      "5                            D_jame_un_mensaje_length         1.642224   \n",
      "4                                    UTM_CONTENT_freq         1.601832   \n",
      "9                              desired_salary_encoded         1.525778   \n",
      "10                       D_jame_un_mensaje_word_count         1.521902   \n",
      "8                                     profession_freq         1.421864   \n",
      "7                   D_jame_un_mensaje_avg_word_length         1.207277   \n",
      "13                                  UTM_CAMPAING_freq         1.182071   \n",
      "15                                        name_length         1.172511   \n",
      "19  Cuando_hables_ingl_s_con_fluidez___qu__cambiar...         1.149911   \n",
      "6   _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         1.104423   \n",
      "\n",
      "    Mean_Launch_Imp  CV_Launch  \n",
      "1          5.471585   0.337303  \n",
      "0          7.178437   0.437353  \n",
      "3          3.105987   0.501160  \n",
      "17         1.560052   0.629633  \n",
      "2          3.499213   0.667181  \n",
      "5          2.583926   0.303634  \n",
      "4          2.687921   0.404030  \n",
      "9          2.301521   0.564114  \n",
      "10         2.042895   0.298521  \n",
      "8          2.326728   0.293745  \n",
      "7          2.377212   0.597412  \n",
      "13         1.850755   0.337467  \n",
      "15         1.684801   0.558332  \n",
      "19         1.380611   0.792526  \n",
      "6          2.457472   0.449786  \n",
      "\n",
      "Análise de robustez entre lançamentos salva em reports/eda_results/feature_importance_results/feature_robustness_analysis.csv\n",
      "\n",
      "Identificando features potencialmente irrelevantes...\n",
      "\n",
      "Features potencialmente irrelevantes (119):\n",
      "                                               Feature  Mean_Importance  \\\n",
      "193  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     2.219257e-19   \n",
      "175  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     1.416190e-18   \n",
      "194  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     6.257881e-17   \n",
      "4    Cuando_hables_ingl_s_con_fluidez___qu__cambiar...     7.209112e-17   \n",
      "198  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     4.896953e-06   \n",
      "183  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     9.333958e-05   \n",
      "174  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     1.714198e-04   \n",
      "214  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     2.562666e-04   \n",
      "185  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     2.731979e-04   \n",
      "182  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     3.362268e-04   \n",
      "186  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     5.379568e-04   \n",
      "176  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     6.334523e-04   \n",
      "184  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     7.919459e-04   \n",
      "265  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...     1.201410e-03   \n",
      "251  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     1.366929e-03   \n",
      "257  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...     1.467736e-03   \n",
      "275  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...     1.828967e-03   \n",
      "216  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     1.854077e-03   \n",
      "11   Cuando_hables_ingl_s_con_fluidez___qu__cambiar...     1.910882e-03   \n",
      "195  _Qu__esperas_aprender_en_la_Inmersi_n_Desbloqu...     2.633637e-03   \n",
      "\n",
      "           CV  \n",
      "193  1.732051  \n",
      "175  1.732051  \n",
      "194  1.732051  \n",
      "4    1.732051  \n",
      "198  1.732051  \n",
      "183  1.732051  \n",
      "174  1.732051  \n",
      "214  1.732051  \n",
      "185  1.732051  \n",
      "182  1.732051  \n",
      "186  1.732051  \n",
      "176  1.732051  \n",
      "184  1.732051  \n",
      "265  1.732051  \n",
      "251  1.227464  \n",
      "257  0.866046  \n",
      "275  0.987459  \n",
      "216  1.331991  \n",
      "11   1.732051  \n",
      "195  1.732051  \n",
      "... e mais 99 features.\n",
      "\n",
      "--- Análise Específica de Features Textuais ---\n",
      "\n",
      "Importância das features textuais por categoria:\n",
      "Comprimento Texto: 14 features, importância média: 0.86\n",
      "  Top features nesta categoria:\n",
      "    - D_jame_un_mensaje_length: 1.64\n",
      "    - D_jame_un_mensaje_word_count: 1.52\n",
      "    - D_jame_un_mensaje_avg_word_length: 1.21\n",
      "Sentimento: 4 features, importância média: 0.09\n",
      "  Top features nesta categoria:\n",
      "    - Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__sentiment: 0.12\n",
      "    - D_jame_un_mensaje_sentiment: 0.12\n",
      "    - _Qu__esperas_aprender_en_la_Semana_de_Cero_a_Ingl_s_Fluido__sentiment: 0.09\n",
      "Motivação: 48 features, importância média: 0.17\n",
      "  Top features nesta categoria:\n",
      "    - Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__motiv_work_norm: 0.59\n",
      "    - Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__motiv_improvement_norm: 0.55\n",
      "    - _Qu__esperas_aprender_en_la_Semana_de_Cero_a_Ingl_s_Fluido__motiv_communication_norm: 0.47\n",
      "Características: 4 features, importância média: 0.01\n",
      "  Top features nesta categoria:\n",
      "    - D_jame_un_mensaje_has_question: 0.05\n",
      "    - _Qu__esperas_aprender_en_la_Semana_de_Cero_a_Ingl_s_Fluido__has_question: 0.00\n",
      "    - Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__has_question: 0.00\n",
      "TF-IDF: 200 features, importância média: 0.16\n",
      "  Top features nesta categoria:\n",
      "    - D_jame_un_mensaje_tfidf_gracias: 0.51\n",
      "    - D_jame_un_mensaje_tfidf_espero: 0.50\n",
      "    - Cuando_hables_ingl_s_con_fluidez___qu__cambiar__en_tu_vida___Qu__oportunidades_se_abrir_n_para_ti__tfidf_trabajo: 0.48\n",
      "\n",
      "Top 10 features textuais:\n",
      "                                               Feature  Mean_Importance\n",
      "92                            D_jame_un_mensaje_length         1.642224\n",
      "163                       D_jame_un_mensaje_word_count         1.521902\n",
      "82                   D_jame_un_mensaje_avg_word_length         1.207277\n",
      "353                                        name_length         1.172511\n",
      "0    Cuando_hables_ingl_s_con_fluidez___qu__cambiar...         1.149911\n",
      "263  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         1.104423\n",
      "10   Cuando_hables_ingl_s_con_fluidez___qu__cambiar...         0.999106\n",
      "253  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         0.869683\n",
      "334  _Qu__esperas_aprender_en_la_Semana_de_Cero_a_I...         0.637883\n",
      "27   Cuando_hables_ingl_s_con_fluidez___qu__cambiar...         0.593646\n",
      "\n",
      "Contribuição total das features textuais: 52.46% da importância total\n",
      "Análise de features textuais salva em reports/eda_results/feature_importance_results/text_features_importance.csv\n",
      "\n",
      "--- Preparando Recomendações Finais ---\n",
      "\n",
      "Total de features analisadas: 368\n",
      "Features recomendadas após filtragem: 226\n",
      "\n",
      "Lista de 226 features recomendadas salva em reports/eda_results/feature_importance_results/recommended_features.txt\n",
      "Documentação detalhada de features não recomendadas salva em reports/eda_results/feature_importance_results/unrecommended_features_explanation.txt\n",
      "\n",
      "--- Gerando Dataset com Features Selecionadas ---\n",
      "Selecionando 226 features + target para o novo dataset\n",
      "Dataset com features selecionadas salvo em data/split3/train_selected.csv\n",
      "Dataset de treino com features selecionadas salvo em 'data/split3/train_selected.csv'\n",
      "\n",
      "--- Gerando Dataset com Features Selecionadas ---\n",
      "Selecionando 226 features + target para o novo dataset\n",
      "Dataset com features selecionadas salvo em data/split3/validation_selected.csv\n",
      "Dataset de validação com features selecionadas salvo em 'data/split3/validation_selected.csv'\n",
      "\n",
      "--- Gerando Dataset com Features Selecionadas ---\n",
      "Selecionando 226 features + target para o novo dataset\n",
      "Dataset com features selecionadas salvo em data/split3/test_selected.csv\n",
      "Dataset de teste com features selecionadas salvo em 'data/split3/test_selected.csv'\n",
      "\n",
      "=== RESUMO DAS ANÁLISES ===\n",
      "Total de features analisadas: 368\n",
      "Features textuais processadas: 270\n",
      "\n",
      "Top 10 features mais importantes para previsão de conversão:\n",
      "336. age_encoded: 9.30\n",
      "342. current_salary_encoded: 5.61\n",
      "341. country_freq: 3.18\n",
      "170. UTM_SOURCE_freq: 2.83\n",
      "340. country_encoded: 2.08\n",
      "93. D_jame_un_mensaje_length: 1.64\n",
      "168. UTM_CONTENT_freq: 1.60\n",
      "359. time_known_encoded: 1.58\n",
      "346. desired_salary_encoded: 1.53\n",
      "164. D_jame_un_mensaje_word_count: 1.52\n",
      "\n",
      "Importância por categoria de features:\n",
      "Features Textuais: 52.46 (52.5%)\n",
      "Demografia: 9.72 (9.7%)\n",
      "UTM/Campaign: 9.35 (9.3%)\n",
      "Tempo/Data: 8.15 (8.1%)\n",
      "Dados Geográficos: 5.26 (5.3%)\n",
      "Profissão: 3.93 (3.9%)\n",
      "\n",
      "Análise de importância de features concluída com sucesso!\n",
      "Todos os resultados foram salvos na pasta 'reports/eda_results/feature_importance_results'\n",
      "Datasets com features selecionadas salvos em 'data/split3'\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance e Feature Selection Pipeline\n",
    "# Adaptado do script feature_importance_pipeline.py para uso em notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports dos módulos de avaliação\n",
    "from src.evaluation import feature_importance as fi\n",
    "from src.evaluation import feature_selector as fs\n",
    "\n",
    "# Criar pasta para resultados\n",
    "output_dir = 'reports/eda_results/feature_importance_results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Pasta '{output_dir}' criada para salvar resultados\")\n",
    "else:\n",
    "    print(f\"Pasta '{output_dir}' já existe\")\n",
    "\n",
    "# 1 - Carregar dataset\n",
    "print(\"Carregando dataset...\")\n",
    "data_dir = '../data/split3'\n",
    "train_path = os.path.join(data_dir, \"train.csv\")\n",
    "cv_path = os.path.join(data_dir, \"validation.csv\") \n",
    "test_path = os.path.join(data_dir, \"test.csv\")\n",
    "\n",
    "try:\n",
    "    # Tentar carregar os arquivos individualmente\n",
    "    df_list = []\n",
    "    if os.path.exists(train_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        df_list.append(train_df)\n",
    "        print(f\"Dataset de treino carregado: {train_df.shape}\")\n",
    "    \n",
    "    if os.path.exists(cv_path):\n",
    "        cv_df = pd.read_csv(cv_path)\n",
    "        df_list.append(cv_df)\n",
    "        print(f\"Dataset de validação carregado: {cv_df.shape}\")\n",
    "    \n",
    "    if os.path.exists(test_path):\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        df_list.append(test_df)\n",
    "        print(f\"Dataset de teste carregado: {test_df.shape}\")\n",
    "    \n",
    "    # Se pelo menos um arquivo foi carregado, combine-os\n",
    "    if df_list:\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"Dataset combinado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Nenhum arquivo encontrado em data/split3\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar datasets: {e}\")\n",
    "    # Tentar carregar arquivos da pasta notebooks/datasets\n",
    "    try:\n",
    "        notebook_train_path = \"datasets/split/train.csv\"\n",
    "        if os.path.exists(notebook_train_path):\n",
    "            df = pd.read_csv(notebook_train_path)\n",
    "            print(f\"Dataset alternativo carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "        else:\n",
    "            # Última tentativa - arquivos originais\n",
    "            alt_paths = [\"smart_ads_all_features.csv\", \"smart_ads_cleaned.csv\"]\n",
    "            df_loaded = False\n",
    "            for path in alt_paths:\n",
    "                try:\n",
    "                    df = pd.read_csv(path)\n",
    "                    df_loaded = True\n",
    "                    print(f\"Arquivo alternativo {path} carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "                    break\n",
    "                except:\n",
    "                    print(f\"Arquivo alternativo {path} não encontrado.\")\n",
    "            \n",
    "            if not df_loaded:\n",
    "                raise FileNotFoundError(\"Não foi possível encontrar nenhum dataset para análise\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Erro final ao tentar carregar qualquer dataset: {e2}\")\n",
    "        raise\n",
    "\n",
    "# 2 - Identificar colunas de lançamento e target\n",
    "print(\"\\nIdentificando colunas importantes...\")\n",
    "launch_col = fi.identify_launch_column(df)\n",
    "target_col = fi.identify_target_column(df)\n",
    "\n",
    "# 3 - Selecionar features numéricas para análise\n",
    "numeric_cols = fi.select_numeric_features(df, target_col)\n",
    "\n",
    "# 4 - Identificar colunas derivadas de texto\n",
    "text_derived_cols = fi.identify_text_derived_columns(numeric_cols)\n",
    "\n",
    "# 5 - Sanitizar nomes de colunas\n",
    "rename_dict = fi.sanitize_column_names(numeric_cols)\n",
    "\n",
    "# Aplicar renomeação se necessário\n",
    "if rename_dict:\n",
    "    print(f\"Renomeando {len(rename_dict)} colunas para evitar erros com caracteres especiais\")\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Atualizar listas\n",
    "    numeric_cols = [rename_dict.get(col, col) for col in numeric_cols]\n",
    "    text_derived_cols = [rename_dict.get(col, col) for col in text_derived_cols]\n",
    "\n",
    "# 6 - Preparar dados para modelagem\n",
    "X = df[numeric_cols].fillna(0)\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Usando {len(numeric_cols)} features numéricas para análise\")\n",
    "print(f\"Distribuição do target: {y.value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# 7 - Análise de multicolinearidade\n",
    "high_corr_pairs = fi.analyze_multicollinearity(X)\n",
    "\n",
    "# 8 - Análise específica: comparação de encodings de país\n",
    "fi.compare_country_encodings(X, y)\n",
    "\n",
    "# 9 - Separar dados para treinamento e validação\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Treinamento: {X_train.shape[0]} amostras, Validação: {X_val.shape[0]} amostras\")\n",
    "print(f\"Proporção da classe positiva no treino: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Proporção da classe positiva na validação: {y_val.mean()*100:.2f}%\")\n",
    "\n",
    "# 10 - Análise de importância com múltiplos modelos\n",
    "print(\"\\n--- Iniciando análise de importância de features ---\")\n",
    "\n",
    "# 10.1 - RandomForest\n",
    "rf_importance, rf_metrics = fi.analyze_rf_importance(X, y, numeric_cols)\n",
    "\n",
    "# 10.2 - LightGBM\n",
    "lgb_importance, lgb_metrics = fi.analyze_lgb_importance(X, y, numeric_cols)\n",
    "\n",
    "# 10.3 - XGBoost\n",
    "xgb_importance, xgb_metrics = fi.analyze_xgb_importance(X, y, numeric_cols)\n",
    "\n",
    "# 11 - Combinar resultados de importância\n",
    "final_importance = fi.combine_importance_results(rf_importance, lgb_importance, xgb_importance)\n",
    "\n",
    "# 12 - Salvar resultados combinados\n",
    "final_importance.to_csv(os.path.join(output_dir, 'feature_importance_combined.csv'), index=False)\n",
    "print(f\"\\nImportância das features salva em {os.path.join(output_dir, 'feature_importance_combined.csv')}\")\n",
    "\n",
    "# 13 - Análise de robustez entre lançamentos\n",
    "launch_importance, unstable_features, launch_vs_global, consistent_features = fi.analyze_launch_robustness(\n",
    "    df, X, y, numeric_cols, launch_col, rename_dict, final_importance\n",
    ")\n",
    "\n",
    "# Salvar análise de robustez entre lançamentos se disponível\n",
    "if launch_vs_global is not None:\n",
    "    launch_vs_global.to_csv(os.path.join(output_dir, 'feature_robustness_analysis.csv'), index=False)\n",
    "    print(f\"\\nAnálise de robustez entre lançamentos salva em {os.path.join(output_dir, 'feature_robustness_analysis.csv')}\")\n",
    "\n",
    "# 14 - Identificar features potencialmente irrelevantes\n",
    "potentially_irrelevant, irrelevant_by_importance, irrelevant_by_variance, irrelevant_by_correlation = fs.identify_irrelevant_features(\n",
    "    final_importance, high_corr_pairs\n",
    ")\n",
    "\n",
    "# 15 - Análise de features textuais\n",
    "text_importance = fs.analyze_text_features(final_importance, text_derived_cols)\n",
    "\n",
    "# Salvar análise de features textuais se disponível\n",
    "if text_importance is not None:\n",
    "    text_importance.to_csv(os.path.join(output_dir, 'text_features_importance.csv'), index=False)\n",
    "    print(f\"Análise de features textuais salva em {os.path.join(output_dir, 'text_features_importance.csv')}\")\n",
    "\n",
    "# 16 - Selecionar features finais e criar recomendações\n",
    "original_relevant_features, features_to_remove_corr, unrecommended_features = fs.select_final_features(\n",
    "    final_importance, high_corr_pairs, numeric_cols, rename_dict\n",
    ")\n",
    "\n",
    "# 17 - Documentar seleções de features\n",
    "fs.document_feature_selections(\n",
    "    original_relevant_features, unrecommended_features, \n",
    "    final_importance, high_corr_pairs, rename_dict, output_dir\n",
    ")\n",
    "\n",
    "# 18 - Criar datasets com features selecionadas\n",
    "# Determinar caminhos de output para datasets processados\n",
    "output_data_dir = 'data/split3'\n",
    "if not os.path.exists(output_data_dir):\n",
    "    os.makedirs(output_data_dir, exist_ok=True)\n",
    "\n",
    "# Conjunto de treino (se existir)\n",
    "if os.path.exists(train_path):\n",
    "    train_output_path = os.path.join(output_data_dir, \"train_selected.csv\")\n",
    "    fs.create_selected_dataset(\n",
    "        original_relevant_features, target_col, train_path, train_output_path\n",
    "    )\n",
    "    print(f\"Dataset de treino com features selecionadas salvo em '{train_output_path}'\")\n",
    "\n",
    "# Conjunto de validação (se existir)\n",
    "if os.path.exists(cv_path):\n",
    "    cv_output_path = os.path.join(output_data_dir, \"validation_selected.csv\")\n",
    "    fs.create_selected_dataset(\n",
    "        original_relevant_features, target_col, cv_path, cv_output_path\n",
    "    )\n",
    "    print(f\"Dataset de validação com features selecionadas salvo em '{cv_output_path}'\")\n",
    "\n",
    "# Conjunto de teste (se existir)\n",
    "if os.path.exists(test_path):\n",
    "    test_output_path = os.path.join(output_data_dir, \"test_selected.csv\")\n",
    "    fs.create_selected_dataset(\n",
    "        original_relevant_features, target_col, test_path, test_output_path\n",
    "    )\n",
    "    print(f\"Dataset de teste com features selecionadas salvo em '{test_output_path}'\")\n",
    "\n",
    "# 19 - Resumir categorias de features\n",
    "fs.summarize_feature_categories(numeric_cols, final_importance, text_derived_cols)\n",
    "\n",
    "print(\"\\nAnálise de importância de features concluída com sucesso!\")\n",
    "print(f\"Todos os resultados foram salvos na pasta '{output_dir}'\")\n",
    "print(f\"Datasets com features selecionadas salvos em '{output_data_dir}'\")\n",
    "\n",
    "# Variáveis importantes para uso posterior no notebook\n",
    "results = {\n",
    "    'df': df,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'text_derived_cols': text_derived_cols,\n",
    "    'final_importance': final_importance,\n",
    "    'selected_features': original_relevant_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb1c4e-be26-42b2-a061-4ddca140e9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
