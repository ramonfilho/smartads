{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f10c00-3bd1-46e6-b287-24761a3c855d",
   "metadata": {},
   "source": [
    "# Feature importance and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ce1b3-222e-400d-bdc8-870c35d16a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "import re\n",
    "import shap\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Criar pasta para resultados\n",
    "output_dir = 'eda_results/feature_importance_results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Pasta '{output_dir}' criada para salvar resultados\")\n",
    "else:\n",
    "    print(f\"Pasta '{output_dir}' já existe\")\n",
    "\n",
    "# 1 - Carregar dataset com features textuais incluídas\n",
    "print(\"Carregando dataset com features textuais...\")\n",
    "try:\n",
    "    df = pd.read_csv(\"datasets/02_3_data_with_feature_treatment_nlp.csv\")\n",
    "    print(f\"Dataset com features textuais carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "except:\n",
    "    print(\"Arquivo não encontrado, tentando alternativa...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"smart_ads_all_features.csv\")\n",
    "        print(f\"Dataset alternativo carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "    except:\n",
    "        print(\"Arquivo alternativo não encontrado, tentando arquivo original...\")\n",
    "        df = pd.read_csv(\"smart_ads_cleaned.csv\")\n",
    "        print(f\"Dataset original carregado: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
    "\n",
    "# 2 - Identificar coluna de lançamento (usando especificamente 'lançamento')\n",
    "print(\"\\nIdentificando coluna de lançamento...\")\n",
    "launch_col = 'lançamento'  # Nome específico conforme indicado\n",
    "\n",
    "if launch_col in df.columns:\n",
    "    print(f\"Coluna de lançamento encontrada: '{launch_col}'\")\n",
    "    n_launches = df[launch_col].nunique()\n",
    "    print(f\"Número de lançamentos: {n_launches}\")\n",
    "    print(f\"Lançamentos identificados: {sorted(df[launch_col].unique())}\")\n",
    "    print(f\"Distribuição de lançamentos:\\n{df[launch_col].value_counts(normalize=True)*100}\")\n",
    "else:\n",
    "    print(f\"Coluna '{launch_col}' não encontrada. Verificando alternativas...\")\n",
    "    # Procurar colunas alternativas\n",
    "    alt_launch_cols = [col for col in df.columns if 'lanc' in col.lower() or 'launch' in col.lower()]\n",
    "    if alt_launch_cols:\n",
    "        launch_col = alt_launch_cols[0]\n",
    "        print(f\"Usando coluna alternativa: '{launch_col}'\")\n",
    "        print(f\"Número de lançamentos: {df[launch_col].nunique()}\")\n",
    "        print(f\"Distribuição de lançamentos:\\n{df[launch_col].value_counts(normalize=True)*100}\")\n",
    "    else:\n",
    "        launch_col = None\n",
    "        print(\"Nenhuma coluna de lançamento identificada.\")\n",
    "\n",
    "# 3 - Preparar dados para modelagem\n",
    "print(\"\\nPreparando dados para análise de importância...\")\n",
    "# Verificar coluna target\n",
    "if 'target' not in df.columns:\n",
    "    print(\"Coluna 'target' não encontrada. Verificando alternativas...\")\n",
    "    target_cols = [col for col in df.columns if col.lower() in ['target', 'comprou', 'converted', 'conversion']]\n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "        print(f\"Usando '{target_col}' como target.\")\n",
    "    else:\n",
    "        raise ValueError(\"Não foi possível encontrar uma coluna target.\")\n",
    "else:\n",
    "    target_col = 'target'\n",
    "\n",
    "# Selecionar colunas numéricas para análise\n",
    "numeric_cols = df.select_dtypes(include=['number', 'bool']).columns.tolist()\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "\n",
    "# Remover colunas com mais de 90% de valores ausentes\n",
    "missing_pct = df[numeric_cols].isna().mean()\n",
    "high_missing_cols = missing_pct[missing_pct > 0.9].index.tolist()\n",
    "if high_missing_cols:\n",
    "    print(f\"Removendo {len(high_missing_cols)} colunas com mais de 90% de valores ausentes\")\n",
    "    numeric_cols = [col for col in numeric_cols if col not in high_missing_cols]\n",
    "\n",
    "# Remover colunas com variância zero\n",
    "try:\n",
    "    selector = VarianceThreshold(threshold=0)\n",
    "    selector.fit(df[numeric_cols].fillna(0))\n",
    "    zero_var_cols = [numeric_cols[i] for i, var in enumerate(selector.variances_) if var == 0]\n",
    "    if zero_var_cols:\n",
    "        print(f\"Removendo {len(zero_var_cols)} colunas com variância zero\")\n",
    "        numeric_cols = [col for col in numeric_cols if col not in zero_var_cols]\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar variância: {e}\")\n",
    "\n",
    "# Verificar se há features textuais no dataset\n",
    "text_derived_cols = [col for col in numeric_cols if any(text_indicator in col for text_indicator in \n",
    "                                                     ['_tfidf_', '_sentiment', '_word_count', '_length', '_motiv_', '_has_question'])]\n",
    "print(f\"Features derivadas de texto identificadas: {len(text_derived_cols)}\")\n",
    "if text_derived_cols:\n",
    "    print(\"Exemplos de features textuais:\")\n",
    "    for col in text_derived_cols[:5]:  # Mostrar alguns exemplos\n",
    "        print(f\"  - {col}\")\n",
    "    if len(text_derived_cols) > 5:\n",
    "        print(f\"  - ... e mais {len(text_derived_cols) - 5} features textuais\")\n",
    "\n",
    "# IMPORTANTE: Sanitizar nomes de colunas para evitar erro de caracteres especiais JSON\n",
    "# Isso resolve o erro \"Do not support special JSON characters in feature name\"\n",
    "print(\"\\nSanitizando nomes das features para evitar problemas com caracteres especiais...\")\n",
    "rename_dict = {}\n",
    "for col in numeric_cols:\n",
    "    # Substituir caracteres especiais e espaços por underscores\n",
    "    new_col = re.sub(r'[^0-9a-zA-Z_]', '_', col)\n",
    "    # Garantir que não comece com número\n",
    "    if new_col[0].isdigit():\n",
    "        new_col = 'f_' + new_col\n",
    "    # Verificar se já existe esse novo nome\n",
    "    i = 1\n",
    "    temp_col = new_col\n",
    "    while temp_col in rename_dict.values():\n",
    "        temp_col = f\"{new_col}_{i}\"\n",
    "        i += 1\n",
    "    new_col = temp_col\n",
    "    \n",
    "    # Só adicionar ao dicionário se o nome mudou\n",
    "    if col != new_col:\n",
    "        rename_dict[col] = new_col\n",
    "\n",
    "# Aplicar renomeação\n",
    "if rename_dict:\n",
    "    print(f\"Renomeando {len(rename_dict)} colunas para evitar erros com caracteres especiais\")\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Atualizar lista de colunas numéricas\n",
    "    numeric_cols = [rename_dict.get(col, col) for col in numeric_cols]\n",
    "    \n",
    "    # Atualizar lista de colunas textuais\n",
    "    text_derived_cols = [rename_dict.get(col, col) for col in text_derived_cols]\n",
    "\n",
    "X = df[numeric_cols].fillna(0)\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Usando {len(numeric_cols)} features numéricas para análise\")\n",
    "print(f\"Distribuição do target: {y.value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# 4 - Análise de Multicolinearidade\n",
    "print(\"\\n--- Análise de Multicolinearidade ---\")\n",
    "# Identificar pares de features com correlação alta\n",
    "corr_matrix = X.corr()\n",
    "high_corr_pairs = []\n",
    "\n",
    "# Limiar de correlação (ajustável)\n",
    "corr_threshold = 0.8\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > corr_threshold:\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': corr_matrix.columns[i],\n",
    "                'feature2': corr_matrix.columns[j],\n",
    "                'correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "# Ordenar por correlação absoluta\n",
    "high_corr_pairs = sorted(high_corr_pairs, key=lambda x: abs(x['correlation']), reverse=True)\n",
    "\n",
    "print(f\"Encontrados {len(high_corr_pairs)} pares de features com correlação > {corr_threshold}:\")\n",
    "for i, pair in enumerate(high_corr_pairs[:10]):  # Mostrar os 10 primeiros\n",
    "    print(f\"{i+1}. {pair['feature1']} & {pair['feature2']}: {pair['correlation']:.4f}\")\n",
    "\n",
    "if len(high_corr_pairs) > 10:\n",
    "    print(f\"... e mais {len(high_corr_pairs) - 10} pares.\")\n",
    "\n",
    "# 5 - Verificação Específica: country_freq vs country_encoded\n",
    "print(\"\\n--- Análise de Redundância: country_freq vs country_encoded ---\")\n",
    "# Procurar versões sanitizadas dos nomes\n",
    "country_freq_col = next((col for col in X.columns if 'country_freq' in col), None)\n",
    "country_encoded_col = next((col for col in X.columns if 'country_encoded' in col), None)\n",
    "\n",
    "if country_freq_col and country_encoded_col:\n",
    "    # Calcular correlação\n",
    "    corr, p_value = pearsonr(X[country_freq_col].fillna(0), X[country_encoded_col].fillna(0))\n",
    "    print(f\"Correlação entre {country_freq_col} e {country_encoded_col}: {corr:.4f} (p-value: {p_value:.4f})\")\n",
    "    \n",
    "    # Avaliar valor preditivo relativo para o target\n",
    "    corr_target_freq = pearsonr(X[country_freq_col].fillna(0), y)[0]\n",
    "    corr_target_encoded = pearsonr(X[country_encoded_col].fillna(0), y)[0]\n",
    "    \n",
    "    print(f\"Correlação com target:\")\n",
    "    print(f\"- {country_freq_col}: {corr_target_freq:.4f}\")\n",
    "    print(f\"- {country_encoded_col}: {corr_target_encoded:.4f}\")\n",
    "    \n",
    "    recommendation = f\"{country_freq_col if abs(corr_target_freq) > abs(corr_target_encoded) else country_encoded_col} parece ter maior valor preditivo.\"\n",
    "    print(f\"Recomendação: {recommendation}\")\n",
    "else:\n",
    "    print(\"Colunas country_freq e/ou country_encoded não encontradas.\")\n",
    "\n",
    "# 6 - Separar dados para treinamento e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Treinamento: {X_train.shape[0]} amostras, Validação: {X_val.shape[0]} amostras\")\n",
    "print(f\"Proporção da classe positiva no treino: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Proporção da classe positiva na validação: {y_val.mean()*100:.2f}%\")\n",
    "\n",
    "# 7 - Definir funções de avaliação para dados desbalanceados\n",
    "def evaluate_model(model, X, y, feature_names):\n",
    "    \"\"\"Avalia o modelo usando métricas adequadas para dados desbalanceados\"\"\"\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "    else:  # Para modelos como XGBoost com DMatrix\n",
    "        y_proba = model.predict(X)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    # AUC - avalia ranking independente do threshold\n",
    "    auc = roc_auc_score(y, y_proba)\n",
    "    \n",
    "    # Average Precision - média ponderada de precisões em diferentes thresholds\n",
    "    ap = average_precision_score(y, y_proba)\n",
    "    \n",
    "    # Encontrar melhor F1-score ajustando threshold\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y, y_proba)\n",
    "    f1_scores = 2 * recalls * precisions / (recalls + precisions + 1e-10)\n",
    "    best_threshold_idx = np.argmax(f1_scores)\n",
    "    best_threshold = 0 if len(thresholds) == 0 else thresholds[min(best_threshold_idx, len(thresholds)-1)]\n",
    "    best_f1 = f1_scores[best_threshold_idx]\n",
    "    \n",
    "    print(f\"Desempenho do modelo:\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Average Precision: {ap:.4f}\")\n",
    "    print(f\"  Melhor F1-Score: {best_f1:.4f} (threshold: {best_threshold:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'ap': ap,\n",
    "        'f1': best_f1,\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "\n",
    "# 8 - Análise de importância com múltiplos modelos\n",
    "print(\"\\n--- Iniciando análise de importância de features ---\")\n",
    "\n",
    "# 8.1 - RandomForest com validação cruzada para dados desbalanceados\n",
    "print(\"\\nAnalisando com RandomForest e validação cruzada para dados desbalanceados...\")\n",
    "try:\n",
    "    # Usar validação cruzada estratificada para lidar com o desbalanceamento\n",
    "    n_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    rf_importances = np.zeros(len(numeric_cols))\n",
    "    rf_metrics = {'auc': [], 'ap': [], 'f1': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold+1}/{n_folds}\")\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Calcular class_weight\n",
    "        n_samples = len(y_fold_train)\n",
    "        n_pos = y_fold_train.sum()\n",
    "        n_neg = n_samples - n_pos\n",
    "        weight_pos = (n_samples / (2 * n_pos)) if n_pos > 0 else 1.0\n",
    "        weight_neg = (n_samples / (2 * n_neg)) if n_neg > 0 else 1.0\n",
    "        \n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            class_weight={0: weight_neg, 1: weight_pos},\n",
    "            max_depth=10,\n",
    "            random_state=42 + fold,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Avaliar modelo\n",
    "        metrics = evaluate_model(rf_model, X_fold_val, y_fold_val, numeric_cols)\n",
    "        for key in rf_metrics:\n",
    "            rf_metrics[key].append(metrics[key])\n",
    "        \n",
    "        # Acumular importâncias\n",
    "        rf_importances += rf_model.feature_importances_\n",
    "    \n",
    "    # Calcular média das importâncias\n",
    "    rf_importances /= n_folds\n",
    "    \n",
    "    # Criar dataframe de importância\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance_RF': rf_importances\n",
    "    }).sort_values(by='Importance_RF', ascending=False)\n",
    "    \n",
    "    print(\"\\nMétricas médias da validação cruzada (RandomForest):\")\n",
    "    for key, values in rf_metrics.items():\n",
    "        print(f\"  {key.upper()}: {np.mean(values):.4f} (±{np.std(values):.4f})\")\n",
    "    \n",
    "    print(\"\\nTop 15 features (RandomForest):\")\n",
    "    print(rf_importance.head(15))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao executar RandomForest: {e}\")\n",
    "    # Criar dataframe vazio em caso de erro\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance_RF': [0] * len(numeric_cols)\n",
    "    })\n",
    "\n",
    "# 8.2 - LightGBM (com cuidado para evitar erros)\n",
    "print(\"\\nAnalisando com LightGBM...\")\n",
    "try:\n",
    "    # Validação cruzada com LightGBM para dados desbalanceados\n",
    "    lgb_importances = np.zeros(len(numeric_cols))\n",
    "    lgb_metrics = {'auc': [], 'ap': [], 'f1': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold+1}/{n_folds}\")\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Calcular scale_pos_weight\n",
    "        pos_scale = (y_fold_train == 0).sum() / max(1, (y_fold_train == 1).sum())\n",
    "        \n",
    "        train_data = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "        val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'seed': 42 + fold,\n",
    "            'learning_rate': 0.05,\n",
    "            'scale_pos_weight': pos_scale,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Treinando modelo\n",
    "        callbacks = [lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                    lgb.log_evaluation(period=0)]\n",
    "        \n",
    "        lgb_model = lgb.train(\n",
    "            params, \n",
    "            train_data, \n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # Avaliar modelo\n",
    "        metrics = evaluate_model(lgb_model, X_fold_val, y_fold_val, numeric_cols)\n",
    "        for key in lgb_metrics:\n",
    "            lgb_metrics[key].append(metrics[key])\n",
    "        \n",
    "        # Acumular importâncias\n",
    "        fold_importance = lgb_model.feature_importance(importance_type='gain')\n",
    "        lgb_importances += fold_importance\n",
    "    \n",
    "    # Calcular média das importâncias\n",
    "    lgb_importances /= n_folds\n",
    "    \n",
    "    # Criar dataframe de importância\n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance_LGB': lgb_importances\n",
    "    }).sort_values(by='Importance_LGB', ascending=False)\n",
    "    \n",
    "    print(\"\\nMétricas médias da validação cruzada (LightGBM):\")\n",
    "    for key, values in lgb_metrics.items():\n",
    "        print(f\"  {key.upper()}: {np.mean(values):.4f} (±{np.std(values):.4f})\")\n",
    "\n",
    "    print(\"\\nTop 15 features (LightGBM):\")\n",
    "    print(lgb_importance.head(15))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao executar LightGBM: {e}\")\n",
    "    print(\"Criando dataframe de importância vazio para LightGBM\")\n",
    "    # Criar dataframe vazio em caso de erro\n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance_LGB': [0] * len(numeric_cols)\n",
    "    })\n",
    "\n",
    "# 8.3 - XGBoost\n",
    "print(\"\\nAnalisando com XGBoost...\")\n",
    "try:\n",
    "    # Validação cruzada com XGBoost para dados desbalanceados\n",
    "    xgb_importances = {}  # Dicionário para acumular importâncias\n",
    "    xgb_metrics = {'auc': [], 'ap': [], 'f1': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold+1}/{n_folds}\")\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Calcular scale_pos_weight\n",
    "        pos_scale = (y_fold_train == 0).sum() / max(1, (y_fold_train == 1).sum())\n",
    "        \n",
    "        # Preparar dados\n",
    "        dtrain = xgb.DMatrix(X_fold_train, label=y_fold_train, feature_names=numeric_cols)\n",
    "        dval = xgb.DMatrix(X_fold_val, label=y_fold_val, feature_names=numeric_cols)\n",
    "        \n",
    "        # Configuração para dados desbalanceados\n",
    "        xgb_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'scale_pos_weight': pos_scale,\n",
    "            'learning_rate': 0.05,\n",
    "            'seed': 42 + fold,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        # Treinando o modelo\n",
    "        xgb_model = xgb.train(\n",
    "            xgb_params,\n",
    "            dtrain,\n",
    "            num_boost_round=500,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Avaliar modelo\n",
    "        dval_pred = xgb.DMatrix(X_fold_val, feature_names=numeric_cols)\n",
    "        y_pred = xgb_model.predict(dval_pred)\n",
    "        auc = roc_auc_score(y_fold_val, y_pred)\n",
    "        ap = average_precision_score(y_fold_val, y_pred)\n",
    "        \n",
    "        # Encontrar melhor F1-score ajustando threshold\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_fold_val, y_pred)\n",
    "        f1_scores = 2 * recalls * precisions / (recalls + precisions + 1e-10)\n",
    "        best_threshold_idx = np.argmax(f1_scores)\n",
    "        best_threshold = 0 if len(thresholds) == 0 else thresholds[min(best_threshold_idx, len(thresholds)-1)]\n",
    "        best_f1 = f1_scores[best_threshold_idx]\n",
    "        \n",
    "        xgb_metrics['auc'].append(auc)\n",
    "        xgb_metrics['ap'].append(ap)\n",
    "        xgb_metrics['f1'].append(best_f1)\n",
    "        \n",
    "        # Acumular importâncias\n",
    "        importance_dict = xgb_model.get_score(importance_type='gain')\n",
    "        for feat, score in importance_dict.items():\n",
    "            if feat in xgb_importances:\n",
    "                xgb_importances[feat] += score\n",
    "            else:\n",
    "                xgb_importances[feat] = score\n",
    "    \n",
    "    # Calcular média das importâncias\n",
    "    for feat in xgb_importances:\n",
    "        xgb_importances[feat] /= n_folds\n",
    "    \n",
    "    # Criar dataframe de importância\n",
    "    xgb_features = []\n",
    "    xgb_scores = []\n",
    "    \n",
    "    for feat, score in xgb_importances.items():\n",
    "        xgb_features.append(feat)\n",
    "        xgb_scores.append(score)\n",
    "    \n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'Feature': xgb_features,\n",
    "        'Importance_XGB': xgb_scores\n",
    "    }).sort_values(by='Importance_XGB', ascending=False)\n",
    "    \n",
    "    # Adicionar features ausentes\n",
    "    missing_features = set(numeric_cols) - set(xgb_importance['Feature'])\n",
    "    for feat in missing_features:\n",
    "        xgb_importance = pd.concat([xgb_importance, \n",
    "                                   pd.DataFrame({'Feature': [feat], 'Importance_XGB': [0]})],\n",
    "                                  ignore_index=True)\n",
    "    \n",
    "    print(\"\\nMétricas médias da validação cruzada (XGBoost):\")\n",
    "    for key, values in xgb_metrics.items():\n",
    "        print(f\"  {key.upper()}: {np.mean(values):.4f} (±{np.std(values):.4f})\")\n",
    "    \n",
    "    print(\"\\nTop 15 features (XGBoost):\")\n",
    "    print(xgb_importance.head(15))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao executar XGBoost: {e}\")\n",
    "    print(\"Criando dataframe de importância vazio para XGBoost\")\n",
    "    # Criar dataframe vazio em caso de erro\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance_XGB': [0] * len(numeric_cols)\n",
    "    })\n",
    "\n",
    "# 9 - Combinando importâncias de múltiplos modelos\n",
    "print(\"\\nCombinando resultados de diferentes métodos...\")\n",
    "\n",
    "# Normalizar importâncias para comparabilidade\n",
    "for df_imp, col in [(rf_importance, 'Importance_RF'), \n",
    "                    (lgb_importance, 'Importance_LGB'), \n",
    "                    (xgb_importance, 'Importance_XGB')]:\n",
    "    if df_imp[col].sum() > 0:  # Evitar divisão por zero\n",
    "        df_imp[col] = df_imp[col] / df_imp[col].sum() * 100\n",
    "\n",
    "# Mesclar resultados\n",
    "try:\n",
    "    combined = pd.merge(rf_importance, lgb_importance, on='Feature', how='outer')\n",
    "    combined = pd.merge(combined, xgb_importance, on='Feature', how='outer')\n",
    "    combined = combined.fillna(0)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao combinar resultados: {e}\")\n",
    "    # Alternativa: usar apenas o modelo que funcionou\n",
    "    if rf_importance['Importance_RF'].sum() > 0:\n",
    "        combined = rf_importance.copy()\n",
    "        if 'Importance_LGB' not in combined.columns:\n",
    "            combined['Importance_LGB'] = 0\n",
    "        if 'Importance_XGB' not in combined.columns:\n",
    "            combined['Importance_XGB'] = 0\n",
    "    elif lgb_importance['Importance_LGB'].sum() > 0:\n",
    "        combined = lgb_importance.copy()\n",
    "        if 'Importance_RF' not in combined.columns:\n",
    "            combined['Importance_RF'] = 0\n",
    "        if 'Importance_XGB' not in combined.columns:\n",
    "            combined['Importance_XGB'] = 0\n",
    "    else:\n",
    "        combined = xgb_importance.copy()\n",
    "        if 'Importance_RF' not in combined.columns:\n",
    "            combined['Importance_RF'] = 0\n",
    "        if 'Importance_LGB' not in combined.columns:\n",
    "            combined['Importance_LGB'] = 0\n",
    "\n",
    "# Calcular média e desvio padrão das importâncias\n",
    "combined['Mean_Importance'] = combined[['Importance_RF', 'Importance_LGB', 'Importance_XGB']].mean(axis=1)\n",
    "combined['Std_Importance'] = combined[['Importance_RF', 'Importance_LGB', 'Importance_XGB']].std(axis=1)\n",
    "combined['CV'] = combined['Std_Importance'] / combined['Mean_Importance'].replace(0, 1e-10)\n",
    "\n",
    "# Ordenar por importância média\n",
    "final_importance = combined.sort_values(by='Mean_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportância combinada (top 20 features):\")\n",
    "print(final_importance[['Feature', 'Mean_Importance', 'Std_Importance', 'CV']].head(20))\n",
    "\n",
    "# Salvar importância das features\n",
    "final_importance.to_csv(os.path.join(output_dir, 'feature_importance_combined.csv'), index=False)\n",
    "print(f\"\\nImportância das features salva em {os.path.join(output_dir, 'feature_importance_combined.csv')}\")\n",
    "\n",
    "# 10 - Análise de Robustez entre Lançamentos\n",
    "if launch_col and df[launch_col].nunique() >= 2:\n",
    "    print(\"\\n--- Análise de Robustez entre Lançamentos ---\")\n",
    "    \n",
    "    # Verificar se as colunas de lançamento foram renomeadas\n",
    "    if launch_col in rename_dict:\n",
    "        launch_col = rename_dict[launch_col]\n",
    "    \n",
    "    launch_imp_results = {}\n",
    "    \n",
    "    # Selecionar os principais lançamentos para análise (top 6)\n",
    "    main_launches = df[launch_col].value_counts().nlargest(6).index.tolist()\n",
    "    \n",
    "    for launch in main_launches:\n",
    "        launch_mask = df[launch_col] == launch\n",
    "        if launch_mask.sum() < 100:  # Pular lançamentos muito pequenos\n",
    "            print(f\"Pulando lançamento {launch} (menos de 100 amostras)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nAnalisando lançamento: {launch} ({launch_mask.sum()} amostras)\")\n",
    "        \n",
    "        # Separar dados deste lançamento\n",
    "        X_launch = X[launch_mask]\n",
    "        y_launch = y[launch_mask]\n",
    "        \n",
    "        # Verificar se há amostras positivas suficientes\n",
    "        n_pos = y_launch.sum()\n",
    "        if n_pos < 5:\n",
    "            print(f\"  Pulando: apenas {n_pos} amostras positivas\")\n",
    "            continue\n",
    "            \n",
    "        # Criar split proporcional a quantidade de dados\n",
    "        test_size = min(0.2, 100/len(y_launch))\n",
    "        X_tr, X_vl, y_tr, y_vl = train_test_split(X_launch, y_launch, \n",
    "                                                  test_size=test_size, \n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_launch)\n",
    "        \n",
    "        # Tentar RandomForest (mais robusto a erros)\n",
    "        try:\n",
    "            # Calcular class_weight\n",
    "            n_samples = len(y_tr)\n",
    "            n_pos = y_tr.sum()\n",
    "            n_neg = n_samples - n_pos\n",
    "            weight_pos = (n_samples / (2 * n_pos)) if n_pos > 0 else 1.0\n",
    "            weight_neg = (n_samples / (2 * n_neg)) if n_neg > 0 else 1.0\n",
    "            \n",
    "            # Treinar modelo apenas para este lançamento\n",
    "            rf_model_launch = RandomForestClassifier(\n",
    "                n_estimators=50, \n",
    "                class_weight={0: weight_neg, 1: weight_pos},\n",
    "                max_depth=6,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf_model_launch.fit(X_tr, y_tr)\n",
    "            \n",
    "            # Obter importância\n",
    "            launch_imp = pd.DataFrame({\n",
    "                'Feature': numeric_cols,\n",
    "                f'Imp_{launch}': rf_model_launch.feature_importances_\n",
    "            })\n",
    "            \n",
    "            # Normalizar importância\n",
    "            if launch_imp[f'Imp_{launch}'].sum() > 0:\n",
    "                launch_imp[f'Imp_{launch}'] = launch_imp[f'Imp_{launch}'] / launch_imp[f'Imp_{launch}'].sum() * 100\n",
    "            \n",
    "            # Guardar resultados\n",
    "            launch_imp_results[launch] = launch_imp\n",
    "        except Exception as e:\n",
    "            print(f\"  Erro ao analisar lançamento {launch}: {e}\")\n",
    "    \n",
    "    # Combinar resultados de diferentes lançamentos\n",
    "    if launch_imp_results:\n",
    "        combined_launch_imp = launch_imp_results[list(launch_imp_results.keys())[0]].copy()\n",
    "        \n",
    "        for launch, imp_df in list(launch_imp_results.items())[1:]:\n",
    "            combined_launch_imp = pd.merge(combined_launch_imp, imp_df, on='Feature', how='outer')\n",
    "        \n",
    "        combined_launch_imp = combined_launch_imp.fillna(0)\n",
    "        \n",
    "        # Calcular média e desvio padrão entre lançamentos\n",
    "        imp_cols = [col for col in combined_launch_imp.columns if col.startswith('Imp_')]\n",
    "        combined_launch_imp['Mean_Launch_Imp'] = combined_launch_imp[imp_cols].mean(axis=1)\n",
    "        combined_launch_imp['Std_Launch_Imp'] = combined_launch_imp[imp_cols].std(axis=1)\n",
    "        combined_launch_imp['CV_Launch'] = combined_launch_imp['Std_Launch_Imp'] / combined_launch_imp['Mean_Launch_Imp'].replace(0, 1e-10)\n",
    "        \n",
    "        # Ordenar por importância média\n",
    "        launch_importance = combined_launch_imp.sort_values(by='Mean_Launch_Imp', ascending=False)\n",
    "        \n",
    "        print(\"\\nImportância média entre lançamentos (top 15 features):\")\n",
    "        print(launch_importance[['Feature', 'Mean_Launch_Imp', 'Std_Launch_Imp', 'CV_Launch']].head(15))\n",
    "        \n",
    "        # Identificar features com alta variabilidade entre lançamentos\n",
    "        unstable_features = launch_importance[\n",
    "            (launch_importance['CV_Launch'] > 1.2) & \n",
    "            (launch_importance['Mean_Launch_Imp'] > 0.5)\n",
    "        ].sort_values(by='CV_Launch', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeatures com alta variabilidade entre lançamentos:\")\n",
    "        print(unstable_features[['Feature', 'Mean_Launch_Imp', 'CV_Launch']].head(10))\n",
    "        \n",
    "        # Merge com importância geral para comparação\n",
    "        launch_vs_global = pd.merge(\n",
    "            launch_importance[['Feature', 'Mean_Launch_Imp', 'CV_Launch']],\n",
    "            final_importance[['Feature', 'Mean_Importance']],\n",
    "            on='Feature', how='inner'\n",
    "        )\n",
    "        \n",
    "        # Identificar features consistentemente importantes\n",
    "        consistent_features = launch_vs_global[\n",
    "            (launch_vs_global['Mean_Launch_Imp'] > launch_vs_global['Mean_Launch_Imp'].median()) &\n",
    "            (launch_vs_global['Mean_Importance'] > launch_vs_global['Mean_Importance'].median()) &\n",
    "            (launch_vs_global['CV_Launch'] < 1.0)\n",
    "        ].sort_values(by='Mean_Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeatures consistentemente importantes entre lançamentos:\")\n",
    "        print(consistent_features[['Feature', 'Mean_Importance', 'Mean_Launch_Imp', 'CV_Launch']].head(15))\n",
    "        \n",
    "        # Salvar análise de robustez\n",
    "        launch_vs_global.to_csv(os.path.join(output_dir, 'feature_robustness_analysis.csv'), index=False)\n",
    "        print(f\"\\nAnálise de robustez entre lançamentos salva em {os.path.join(output_dir, 'feature_robustness_analysis.csv')}\")\n",
    "else:\n",
    "    print(\"\\nAnálise de robustez entre lançamentos não realizada (coluna de lançamento não identificada ou insuficiente)\")\n",
    "\n",
    "# 11 - Identificar features potencialmente irrelevantes\n",
    "print(\"\\nIdentificando features potencialmente irrelevantes...\")\n",
    "\n",
    "# Critérios para considerar uma feature como potencialmente irrelevante:\n",
    "# 1. Baixa importância média (< 0.1% da importância total média)\n",
    "threshold_importance = 0.1\n",
    "irrelevant_by_importance = final_importance[final_importance['Mean_Importance'] < threshold_importance]\n",
    "\n",
    "# 2. Alta variabilidade entre modelos (coef. variação > 1.5)\n",
    "threshold_cv = 1.5\n",
    "irrelevant_by_variance = final_importance[(final_importance['CV'] > threshold_cv) & \n",
    "                                         (final_importance['Mean_Importance'] < final_importance['Mean_Importance'].median())]\n",
    "\n",
    "# 3. Features altamente correlacionadas com outras mais importantes\n",
    "irrelevant_by_correlation = []\n",
    "for pair in high_corr_pairs:\n",
    "    f1, f2 = pair['feature1'], pair['feature2']\n",
    "    f1_imp = final_importance[final_importance['Feature'] == f1]['Mean_Importance'].values[0] if f1 in final_importance['Feature'].values else 0\n",
    "    f2_imp = final_importance[final_importance['Feature'] == f2]['Mean_Importance'].values[0] if f2 in final_importance['Feature'].values else 0\n",
    "    \n",
    "    # A feature menos importante é considerada irrelevante\n",
    "    if f1_imp < f2_imp:\n",
    "        irrelevant_by_correlation.append({\n",
    "            'Feature': f1, \n",
    "            'Correlation_With': f2, \n",
    "            'Correlation': pair['correlation'],\n",
    "            'Mean_Importance': f1_imp,\n",
    "            'Better_Feature_Importance': f2_imp\n",
    "        })\n",
    "    else:\n",
    "        irrelevant_by_correlation.append({\n",
    "            'Feature': f2, \n",
    "            'Correlation_With': f1, \n",
    "            'Correlation': pair['correlation'],\n",
    "            'Mean_Importance': f2_imp,\n",
    "            'Better_Feature_Importance': f1_imp\n",
    "        })\n",
    "\n",
    "# Converter para DataFrame\n",
    "irrelevant_by_correlation_df = pd.DataFrame(irrelevant_by_correlation)\n",
    "\n",
    "# Combinando critérios\n",
    "potentially_irrelevant = pd.concat([\n",
    "    irrelevant_by_importance[['Feature', 'Mean_Importance', 'CV']],\n",
    "    irrelevant_by_variance[['Feature', 'Mean_Importance', 'CV']]\n",
    "]).drop_duplicates().sort_values(by='Mean_Importance')\n",
    "\n",
    "print(f\"\\nFeatures potencialmente irrelevantes ({len(potentially_irrelevant)}):\")\n",
    "print(potentially_irrelevant[['Feature', 'Mean_Importance', 'CV']].head(20))\n",
    "\n",
    "if len(potentially_irrelevant) > 20:\n",
    "    print(f\"... e mais {len(potentially_irrelevant) - 20} features.\")\n",
    "\n",
    "# 12 - Análise específica de features textuais\n",
    "if text_derived_cols:\n",
    "    print(\"\\n--- Análise Específica de Features Textuais ---\")\n",
    "    \n",
    "    # Extrair apenas features textuais do dataframe de importância\n",
    "    text_importance = final_importance[final_importance['Feature'].isin(text_derived_cols)].copy()\n",
    "    \n",
    "    # Agrupar por tipo de feature textual\n",
    "    text_feature_types = {\n",
    "        'Comprimento Texto': [col for col in text_derived_cols if '_length' in col or '_word_count' in col],\n",
    "        'Sentimento': [col for col in text_derived_cols if '_sentiment' in col],\n",
    "        'Motivação': [col for col in text_derived_cols if '_motiv_' in col],\n",
    "        'Características': [col for col in text_derived_cols if '_has_' in col],\n",
    "        'TF-IDF': [col for col in text_derived_cols if '_tfidf_' in col]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nImportância das features textuais por categoria:\")\n",
    "    for category, cols in text_feature_types.items():\n",
    "        if cols:\n",
    "            category_importance = text_importance[text_importance['Feature'].isin(cols)]\n",
    "            avg_importance = category_importance['Mean_Importance'].mean() if not category_importance.empty else 0\n",
    "            print(f\"{category}: {len(cols)} features, importância média: {avg_importance:.2f}\")\n",
    "            \n",
    "            # Top 3 features nesta categoria\n",
    "            top_features = category_importance.head(3)\n",
    "            if not top_features.empty:\n",
    "                print(\"  Top features nesta categoria:\")\n",
    "                for i, row in top_features.iterrows():\n",
    "                    print(f\"    - {row['Feature']}: {row['Mean_Importance']:.2f}\")\n",
    "    \n",
    "    # Top 10 features textuais gerais\n",
    "    print(\"\\nTop 10 features textuais:\")\n",
    "    print(text_importance[['Feature', 'Mean_Importance']].head(10))\n",
    "    \n",
    "    # Proporção de importância das features textuais\n",
    "    text_importance_sum = text_importance['Mean_Importance'].sum()\n",
    "    total_importance_sum = final_importance['Mean_Importance'].sum()\n",
    "    text_proportion = (text_importance_sum / total_importance_sum) * 100 if total_importance_sum > 0 else 0\n",
    "    \n",
    "    print(f\"\\nContribuição total das features textuais: {text_proportion:.2f}% da importância total\")\n",
    "    \n",
    "    # Salvar análise de features textuais\n",
    "    text_importance.to_csv(os.path.join(output_dir, 'text_features_importance.csv'), index=False)\n",
    "    print(f\"Análise de features textuais salva em {os.path.join(output_dir, 'text_features_importance.csv')}\")\n",
    "\n",
    "# 13 - Recomendações finais e criação da lista de features não recomendadas com justificativas\n",
    "print(\"\\n--- Preparando Recomendações Finais e Documentação ---\")\n",
    "\n",
    "# Definir um limiar de importância\n",
    "importance_threshold = final_importance['Mean_Importance'].sum() * 0.001  # 0.1% da importância total\n",
    "\n",
    "# Filtrar features relevantes e não redundantes\n",
    "relevant_features = final_importance[final_importance['Mean_Importance'] > importance_threshold]['Feature'].tolist()\n",
    "\n",
    "# Remover uma feature de cada par altamente correlacionado (manter a mais importante)\n",
    "features_to_remove_corr = []\n",
    "if high_corr_pairs:\n",
    "    for pair in high_corr_pairs:\n",
    "        f1, f2 = pair['feature1'], pair['feature2']\n",
    "        if f1 in relevant_features and f2 in relevant_features:\n",
    "            f1_imp = final_importance[final_importance['Feature'] == f1]['Mean_Importance'].values[0] \n",
    "            f2_imp = final_importance[final_importance['Feature'] == f2]['Mean_Importance'].values[0]\n",
    "            # Remover a feature menos importante\n",
    "            if f1_imp < f2_imp and f1 in relevant_features:\n",
    "                relevant_features.remove(f1)\n",
    "                features_to_remove_corr.append((f1, f2, pair['correlation'], f1_imp, f2_imp))\n",
    "            elif f2 in relevant_features:\n",
    "                relevant_features.remove(f2)\n",
    "                features_to_remove_corr.append((f2, f1, pair['correlation'], f2_imp, f1_imp))\n",
    "\n",
    "# Criar conjunto de features recomendadas\n",
    "# Converter para dicionário para facilitar a conversão de volta aos nomes originais\n",
    "reverse_rename_dict = {v: k for k, v in rename_dict.items()}\n",
    "\n",
    "# Recuperar os nomes originais das features relevantes\n",
    "original_relevant_features = [reverse_rename_dict.get(feature, feature) for feature in relevant_features]\n",
    "\n",
    "# Salvar lista de features recomendadas\n",
    "with open(os.path.join(output_dir, 'recommended_features.txt'), 'w') as f:\n",
    "    for feature in original_relevant_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(f\"\\nLista de {len(original_relevant_features)} features recomendadas salva em {os.path.join(output_dir, 'recommended_features.txt')}\")\n",
    "\n",
    "# Criar lista e documentação de features não recomendadas\n",
    "unrecommended_features = set(numeric_cols) - set(relevant_features)\n",
    "unrecommended_features_original = [reverse_rename_dict.get(feature, feature) for feature in unrecommended_features]\n",
    "\n",
    "# Preparar justificativas para cada feature não recomendada\n",
    "unrecommended_with_reasons = []\n",
    "\n",
    "for feature in unrecommended_features:\n",
    "    original_feature = reverse_rename_dict.get(feature, feature)\n",
    "    reasons = []\n",
    "    \n",
    "    # Verificar se tem baixa importância\n",
    "    if feature in irrelevant_by_importance['Feature'].values:\n",
    "        imp = final_importance[final_importance['Feature'] == feature]['Mean_Importance'].values[0]\n",
    "        reasons.append(f\"Baixa importância preditiva ({imp:.4f})\")\n",
    "    \n",
    "    # Verificar se tem alta variabilidade entre modelos\n",
    "    if feature in irrelevant_by_variance['Feature'].values:\n",
    "        cv = final_importance[final_importance['Feature'] == feature]['CV'].values[0]\n",
    "        reasons.append(f\"Alta variabilidade entre modelos (CV={cv:.2f})\")\n",
    "    \n",
    "    # Verificar se é redundante com outra feature\n",
    "    redundant_with = None\n",
    "    for f, better_f, corr, imp, better_imp in features_to_remove_corr:\n",
    "        if f == feature:\n",
    "            original_better_f = reverse_rename_dict.get(better_f, better_f)\n",
    "            reasons.append(f\"Altamente correlacionada (r={corr:.2f}) com {original_better_f} que tem maior importância ({better_imp:.4f} vs {imp:.4f})\")\n",
    "            redundant_with = original_better_f\n",
    "            break\n",
    "    \n",
    "    # Se não encontrou razão específica\n",
    "    if not reasons:\n",
    "        reasons.append(\"Baixa contribuição geral para o modelo\")\n",
    "    \n",
    "    unrecommended_with_reasons.append({\n",
    "        'Feature': original_feature,\n",
    "        'Reasons': \"; \".join(reasons),\n",
    "        'Redundant_With': redundant_with,\n",
    "        'Importance': final_importance[final_importance['Feature'] == feature]['Mean_Importance'].values[0] if feature in final_importance['Feature'].values else 0\n",
    "    })\n",
    "\n",
    "# Converter para DataFrame e salvar\n",
    "unrecommended_df = pd.DataFrame(unrecommended_with_reasons)\n",
    "unrecommended_df = unrecommended_df.sort_values('Importance', ascending=False)\n",
    "unrecommended_df.to_csv(os.path.join(output_dir, 'unrecommended_features.csv'), index=False)\n",
    "\n",
    "# Criar arquivo de texto com explicações detalhadas\n",
    "with open(os.path.join(output_dir, 'unrecommended_features_explanation.txt'), 'w') as f:\n",
    "    f.write(\"# Features Não Recomendadas e Justificativas\\n\\n\")\n",
    "    f.write(f\"Total de features analisadas: {len(numeric_cols)}\\n\")\n",
    "    f.write(f\"Features recomendadas: {len(relevant_features)}\\n\")\n",
    "    f.write(f\"Features não recomendadas: {len(unrecommended_features)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Razões para remoção:\\n\\n\")\n",
    "    f.write(\"1. **Baixa importância preditiva**: Features com importância menor que 0.1% da importância total.\\n\")\n",
    "    f.write(\"2. **Alta variabilidade entre modelos**: Features cujo coeficiente de variação entre diferentes modelos é maior que 1.5.\\n\")\n",
    "    f.write(\"3. **Redundância**: Features altamente correlacionadas (r > 0.8) com outras de maior importância.\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Lista de features não recomendadas:\\n\\n\")\n",
    "    \n",
    "    for i, row in unrecommended_df.iterrows():\n",
    "        f.write(f\"### {i+1}. {row['Feature']}\\n\")\n",
    "        f.write(f\"   - **Razões**: {row['Reasons']}\\n\")\n",
    "        if pd.notna(row['Redundant_With']):\n",
    "            f.write(f\"   - **Redundante com**: {row['Redundant_With']}\\n\")\n",
    "        f.write(f\"   - **Importância**: {row['Importance']:.6f}\\n\\n\")\n",
    "\n",
    "print(f\"Documentação detalhada de features não recomendadas salva em {os.path.join(output_dir, 'unrecommended_features_explanation.txt')}\")\n",
    "\n",
    "# 14 - Criar dataset com features selecionadas\n",
    "print(\"\\n--- Gerando Dataset com Features Selecionadas ---\")\n",
    "\n",
    "# Recuperar os nomes originais das features (não sanitizadas) para seleção no dataset original\n",
    "original_df = pd.read_csv(\"datasets/02_3_data_with_feature_treatment_nlp.csv\")\n",
    "target_col_original = target_col  # Assumindo que o target não foi renomeado\n",
    "\n",
    "# Verificar se as features relevantes existem no dataset original\n",
    "available_features = [col for col in original_relevant_features if col in original_df.columns]\n",
    "missing_features = set(original_relevant_features) - set(available_features)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Aviso: {len(missing_features)} features recomendadas não foram encontradas no dataset original.\")\n",
    "    print(f\"Exemplos: {list(missing_features)[:5]}...\")\n",
    "\n",
    "# Selecionar features relevantes + target\n",
    "selected_columns = available_features + [target_col_original]\n",
    "print(f\"Selecionando {len(available_features)} features + target para o novo dataset\")\n",
    "\n",
    "# Criar novo dataset\n",
    "selected_df = original_df[selected_columns]\n",
    "\n",
    "# Salvar dataset com features selecionadas\n",
    "selected_df.to_csv(\"datasets/02_4_data_with_selected_features.csv\", index=False)\n",
    "print(f\"Dataset com features selecionadas salvo em '02_4_data_with_selected_features.csv'\")\n",
    "\n",
    "# 15 - Resumo das análises realizadas\n",
    "print(\"\\n=== RESUMO DAS ANÁLISES ===\")\n",
    "print(f\"Total de features analisadas: {len(numeric_cols)}\")\n",
    "print(f\"Features textuais processadas: {len(text_derived_cols)}\")\n",
    "print(f\"Pares de features altamente correlacionadas: {len(high_corr_pairs)}\")\n",
    "print(f\"Features potencialmente irrelevantes: {len(potentially_irrelevant)}\")\n",
    "print(f\"Features recomendadas após filtragem: {len(relevant_features)}\")\n",
    "print(f\"Features disponíveis no dataset final: {len(available_features)}\")\n",
    "\n",
    "# Exibir top 10 features mais importantes\n",
    "print(\"\\nTop 10 features mais importantes para previsão de conversão:\")\n",
    "for i, row in final_importance.head(10).iterrows():\n",
    "    print(f\"{i+1}. {row['Feature']}: {row['Mean_Importance']:.2f}\")\n",
    "\n",
    "# Categorizar features por tipo\n",
    "feature_categories = {\n",
    "    'Features Textuais': text_derived_cols,\n",
    "    'UTM/Campaign': [col for col in numeric_cols if any(term in col.lower() for term in ['utm', 'campaign', 'camp'])],\n",
    "    'Dados Geográficos': [col for col in numeric_cols if any(term in col.lower() for term in ['country', 'pais'])],\n",
    "    'Tempo/Data': [col for col in numeric_cols if any(term in col.lower() for term in ['time', 'hour', 'day', 'month', 'year'])],\n",
    "    'Demografia': [col for col in numeric_cols if any(term in col.lower() for term in ['age', 'gender', 'edad'])],\n",
    "    'Profissão': [col for col in numeric_cols if any(term in col.lower() for term in ['profes', 'profession', 'work'])]\n",
    "}\n",
    "\n",
    "# Calcular importância por categoria\n",
    "categories_importance = {}\n",
    "for category, cols in feature_categories.items():\n",
    "    category_features = [col for col in cols if col in final_importance['Feature'].values]\n",
    "    if category_features:\n",
    "        category_importance = final_importance[final_importance['Feature'].isin(category_features)]['Mean_Importance'].sum()\n",
    "        categories_importance[category] = category_importance\n",
    "\n",
    "# Ordenar categorias por importância\n",
    "sorted_categories = sorted(categories_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nImportância por categoria de features:\")\n",
    "for category, importance in sorted_categories:\n",
    "    category_pct = (importance / final_importance['Mean_Importance'].sum()) * 100\n",
    "    print(f\"{category}: {importance:.2f} ({category_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nAnálise de importância de features concluída com sucesso!\")\n",
    "print(f\"Todos os resultados foram salvos na pasta '{output_dir}'\")\n",
    "print(f\"Dataset com features selecionadas salvo como '02_4_data_with_selected_features.csv'\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
