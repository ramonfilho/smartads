#!/usr/bin/env python
"""
Feature Selection Script for Smart Ads Project

Este script realiza sele√ß√£o de features usando m√∫ltiplos modelos e t√©cnicas
para identificar as features mais relevantes para predi√ß√£o de convers√£o.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, f1_score
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from scipy.stats import pearsonr
import re
import warnings
import argparse
import json
from datetime import datetime
import logging
import time

warnings.filterwarnings('ignore')

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configurar projeto
PROJECT_ROOT = "/Users/ramonmoreira/desktop/smart_ads"
sys.path.insert(0, PROJECT_ROOT)
sys.path.insert(0, os.path.join(PROJECT_ROOT, "src"))

# Importar m√≥dulos do projeto se dispon√≠veis
try:
    from src.evaluation.feature_importance import (
        create_output_directory,
        load_dataset,
        identify_launch_column,
        identify_target_column,
        select_numeric_features,
        identify_text_derived_columns,
        sanitize_column_names,
        analyze_multicollinearity,
        compare_country_encodings,
        evaluate_model,
        analyze_rf_importance,
        analyze_lgb_importance,
        analyze_xgb_importance,
        combine_importance_results,
        analyze_launch_robustness
    )
    from src.evaluation.feature_selector import (
        identify_irrelevant_features,
        analyze_text_features,
        select_final_features,
        document_feature_selections,
        create_selected_dataset,
        summarize_feature_categories
    )
    logger.info("M√≥dulos do projeto carregados com sucesso")
    USE_PROJECT_MODULES = True
except ImportError as e:
    logger.warning(f"N√£o foi poss√≠vel importar m√≥dulos do projeto: {e}")
    logger.info("Usando implementa√ß√£o local")
    USE_PROJECT_MODULES = False

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='Feature Selection for Smart Ads Project',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Diret√≥rios
    parser.add_argument(
        '--input-dir',
        type=str,
        default=os.path.join(PROJECT_ROOT, 'data/new/03_feature_engineering_1'),
        help='Diret√≥rio de entrada com os datasets processados'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=os.path.join(PROJECT_ROOT, 'data/new/04_feature_selection'),
        help='Diret√≥rio de sa√≠da para datasets com features selecionadas'
    )
    
    parser.add_argument(
        '--analysis-dir',
        type=str,
        default=os.path.join(PROJECT_ROOT, 'reports/feature_importance_results'),
        help='Diret√≥rio para salvar resultados de an√°lise'
    )
    
    # Par√¢metros
    parser.add_argument(
        '--importance-threshold',
        type=float,
        default=0.1,
        help='Threshold de import√¢ncia m√≠nima (em %% da import√¢ncia total)'
    )
    
    parser.add_argument(
        '--cv-threshold',
        type=float,
        default=1.5,
        help='Threshold de coeficiente de varia√ß√£o para features inst√°veis'
    )
    
    parser.add_argument(
        '--correlation-threshold',
        type=float,
        default=0.95,
        help='Threshold de correla√ß√£o para identificar features redundantes'
    )
    
    parser.add_argument(
        '--n-folds',
        type=int,
        default=3,  # Reduzido de 5 para 3 para acelerar
        help='N√∫mero de folds para cross-validation'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10000,
        help='Tamanho do batch para processamento (economia de mem√≥ria)'
    )
    
    parser.add_argument(
        '--max-features',
        type=int,
        default=300,  # Mudado de 700 para 300 como padr√£o
        help='N√∫mero m√°ximo de features a selecionar'
    )
    
    parser.add_argument(
        '--fast-mode',
        action='store_true',
        help='Modo r√°pido: usa apenas RandomForest sem CV'
    )
    
    # Flags
    parser.add_argument(
        '--use-cache',
        action='store_true',
        help='Usar cache de an√°lises anteriores se dispon√≠vel'
    )
    
    parser.add_argument(
        '--skip-launch-analysis',
        action='store_true',
        default=True,
        help='Pular an√°lise de robustez entre lan√ßamentos'
    )
    
    parser.add_argument(
        '--keep-all-features',
        action='store_true',
        help='Manter todas as features (apenas gerar an√°lise)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        help='Arquivo JSON com configura√ß√µes (sobrescreve outros argumentos)'
    )
    
    args = parser.parse_args()
    
    # Carregar configura√ß√µes de arquivo se fornecido
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config = json.load(f)
            for key, value in config.items():
                if hasattr(args, key):
                    setattr(args, key, value)
        logger.info(f"Configura√ß√µes carregadas de {args.config}")
    
    return args

def create_directories(args):
    """Criar diret√≥rios necess√°rios."""
    for dir_path in [args.output_dir, args.analysis_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
            logger.info(f"Diret√≥rio criado: {dir_path}")
        else:
            logger.info(f"Diret√≥rio j√° existe: {dir_path}")

def load_datasets(args):
    """Carregar datasets de treino, valida√ß√£o e teste."""
    datasets = {}
    
    for dataset_name in ['train', 'validation', 'test']:
        file_path = os.path.join(args.input_dir, f"{dataset_name}.csv")
        
        try:
            # Tentar carregar em chunks se o arquivo for muito grande
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            
            if file_size_mb > 500:  # Se maior que 500MB
                logger.info(f"Arquivo {dataset_name}.csv grande ({file_size_mb:.1f}MB), carregando em chunks...")
                
                # Ler em chunks e concatenar
                chunks = []
                for chunk in pd.read_csv(file_path, chunksize=args.batch_size):
                    chunks.append(chunk)
                
                datasets[dataset_name] = pd.concat(chunks, ignore_index=True)
            else:
                datasets[dataset_name] = pd.read_csv(file_path)
            
            logger.info(f"Dataset {dataset_name} carregado: {datasets[dataset_name].shape}")
            
        except Exception as e:
            logger.error(f"Erro ao carregar {dataset_name}: {e}")
            raise
    
    return datasets['train'], datasets['validation'], datasets['test']

def save_configuration(args, output_info):
    """Salvar configura√ß√£o usada e informa√ß√µes de output."""
    config = {
        'timestamp': datetime.now().isoformat(),
        'arguments': vars(args),
        'output_info': output_info,
        'project_root': PROJECT_ROOT,
        'python_version': sys.version,
        'modules_used': 'project' if USE_PROJECT_MODULES else 'local'
    }
    
    config_path = os.path.join(args.analysis_dir, 'feature_selection_config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    logger.info(f"Configura√ß√£o salva em {config_path}")

def remove_perfect_correlations(X, threshold=0.999):
    """
    Remove features com correla√ß√£o perfeita ou quase perfeita.
    Mant√©m a primeira de cada par.
    """
    # Calcular matriz de correla√ß√£o
    corr_matrix = X.corr().abs()
    
    # M√°scara triangular superior
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    
    # Encontrar features para remover
    to_drop = [column for column in upper.columns if any(upper[column] >= threshold)]
    
    if to_drop:
        logger.info(f"Removendo {len(to_drop)} features com correla√ß√£o >= {threshold}")
        logger.info(f"Exemplos: {to_drop[:5]}")
    
    return X.drop(columns=to_drop), to_drop

def fast_feature_selection(X, y, n_features=500):
    """
    Sele√ß√£o r√°pida de features usando RandomForest + estat√≠sticas b√°sicas.
    """
    logger.info(f"\nModo r√°pido: selecionando top {n_features} features")
    
    # 1. Remover features com baixa vari√¢ncia
    selector = VarianceThreshold(threshold=0.01)
    X_var = pd.DataFrame(
        selector.fit_transform(X),
        columns=X.columns[selector.get_support()],
        index=X.index
    )
    logger.info(f"Features ap√≥s filtro de vari√¢ncia: {X_var.shape[1]}")
    
    # 2. F-score para ranking inicial r√°pido
    if X_var.shape[1] > n_features * 2:
        selector = SelectKBest(f_classif, k=min(n_features * 2, X_var.shape[1]))
        X_f = pd.DataFrame(
            selector.fit_transform(X_var, y),
            columns=X_var.columns[selector.get_support()],
            index=X_var.index
        )
        logger.info(f"Features ap√≥s F-score: {X_f.shape[1]}")
    else:
        X_f = X_var
    
    # 3. RandomForest para import√¢ncia final
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        n_jobs=-1,
        random_state=42,
        class_weight='balanced'
    )
    
    # Usar amostra se dataset muito grande
    if len(X_f) > 50000:
        X_sample, _, y_sample, _ = train_test_split(
            X_f, y, train_size=50000/len(X_f), stratify=y, random_state=42
        )
        rf.fit(X_sample, y_sample)
    else:
        rf.fit(X_f, y)
    
    # Criar DataFrame de import√¢ncia
    importance_df = pd.DataFrame({
        'feature': X_f.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # Selecionar top N
    selected_features = importance_df.head(n_features)['feature'].tolist()
    
    return selected_features, importance_df

def handle_multicollinearity(X, y, numeric_cols, correlation_threshold=0.95):
    """
    Remove features altamente correlacionadas antes da an√°lise.
    Para pares correlacionados, mant√©m a que tem maior correla√ß√£o com target.
    """
    logger.info(f"\nRemovendo features com correla√ß√£o > {correlation_threshold}")
    start_time = time.time()
    
    # Primeiro, remover correla√ß√µes perfeitas (mais r√°pido)
    X_cleaned, perfect_corr_removed = remove_perfect_correlations(X, threshold=0.999)
    numeric_cols = [col for col in numeric_cols if col in X_cleaned.columns]
    
    # Calcular correla√ß√µes apenas se necess√°rio
    if correlation_threshold < 0.999:
        corr_matrix = X_cleaned.corr().abs()
        upper = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        # Calcular correla√ß√£o com target
        target_corr = X_cleaned.corrwith(y).abs()
        
        features_to_remove = set()
        correlation_groups = {}
        
        # Encontrar pares com alta correla√ß√£o
        high_corr_indices = np.where(upper > correlation_threshold)
        
        for idx in range(len(high_corr_indices[0])):
            i = high_corr_indices[0][idx]
            j = high_corr_indices[1][idx]
            
            col1 = corr_matrix.columns[i]
            col2 = corr_matrix.columns[j]
            corr_value = upper.iloc[i, j]
            
            # Para alta correla√ß√£o (n√£o perfeita), manter a com maior correla√ß√£o com target
            if target_corr[col1] < target_corr[col2]:
                features_to_remove.add(col1)
            else:
                features_to_remove.add(col2)
            
            # Registrar para an√°lise
            key = tuple(sorted([col1, col2]))
            correlation_groups[key] = {
                'correlation': corr_value,
                'target_corr_1': target_corr[col1],
                'target_corr_2': target_corr[col2]
            }
        
        # Remover features adicionais
        if features_to_remove:
            X_cleaned = X_cleaned.drop(columns=list(features_to_remove))
            numeric_cols = [col for col in numeric_cols if col not in features_to_remove]
    else:
        features_to_remove = set()
        correlation_groups = {}
    
    total_removed = len(perfect_corr_removed) + len(features_to_remove)
    elapsed_time = time.time() - start_time
    
    logger.info(f"Correla√ß√µes processadas em {elapsed_time:.1f} segundos")
    logger.info(f"Features removidas: {total_removed} (perfeitas: {len(perfect_corr_removed)}, altas: {len(features_to_remove)})")
    logger.info(f"Features restantes: {len(numeric_cols)}")
    
    return X_cleaned, numeric_cols, features_to_remove.union(set(perfect_corr_removed)), correlation_groups

def main():
    """Fun√ß√£o principal."""
    total_start_time = time.time()
    
    # Parse argumentos
    args = parse_arguments()
    
    logger.info("=== INICIANDO FEATURE SELECTION ===")
    logger.info(f"Input: {args.input_dir}")
    logger.info(f"Output: {args.output_dir}")
    logger.info(f"Modo: {'R√ÅPIDO' if args.fast_mode else 'COMPLETO'}")
    logger.info(f"Max features: {args.max_features}")
    
    # Criar diret√≥rios
    create_directories(args)
    
    # Carregar datasets
    logger.info("\nCarregando datasets...")
    train_df, val_df, test_df = load_datasets(args)
    
    # Usar dataset de treino para an√°lise
    df = train_df
    
    # Identificar coluna de lan√ßamento
    launch_col = identify_launch_column(df) if not args.skip_launch_analysis else None
    
    # Identificar coluna target
    target_col = identify_target_column(df)
    
    # Selecionar features num√©ricas
    logger.info("\nPreparando dados para an√°lise...")
    numeric_cols = select_numeric_features(df, target_col)
    initial_n_features = len(numeric_cols)
    
    # Identificar features de texto
    text_derived_cols = identify_text_derived_columns(numeric_cols)
    logger.info(f"Features derivadas de texto: {len(text_derived_cols)}")
    
    # Sanitizar nomes se necess√°rio
    rename_dict = sanitize_column_names(numeric_cols)
    if rename_dict:
        logger.info(f"Renomeando {len(rename_dict)} colunas")
        df = df.rename(columns=rename_dict)
        numeric_cols = [rename_dict.get(col, col) for col in numeric_cols]
        text_derived_cols = [rename_dict.get(col, col) for col in text_derived_cols]
        if launch_col and launch_col in rename_dict:
            launch_col = rename_dict[launch_col]
    
    # Preparar dados
    X = df[numeric_cols].fillna(0)
    y = df[target_col]
    
    logger.info(f"Analisando {len(numeric_cols)} features")
    logger.info(f"Distribui√ß√£o do target: {y.value_counts(normalize=True) * 100}")
    
    # PASSO 1: Remover correla√ß√µes altas (ANTES de qualquer an√°lise)
    X_cleaned, numeric_cols_cleaned, removed_features, corr_groups = \
        handle_multicollinearity(X, y, numeric_cols, args.correlation_threshold)
    
    X = X_cleaned
    numeric_cols = numeric_cols_cleaned
    text_derived_cols = [col for col in text_derived_cols if col in numeric_cols]
    
    # Salvar features removidas
    if removed_features:
        removed_path = os.path.join(args.analysis_dir, 'removed_correlations.txt')
        with open(removed_path, 'w') as f:
            f.write(f"Total removidas por correla√ß√£o: {len(removed_features)}\n\n")
            for feat in sorted(removed_features):
                f.write(f"- {feat}\n")
    
    # PASSO 2: An√°lise de import√¢ncia
    if args.fast_mode:
        # Modo r√°pido
        selected_features, importance_df = fast_feature_selection(X, y, args.max_features)
        
        # Criar estrutura compat√≠vel
        final_importance = pd.DataFrame({
            'Feature': importance_df['feature'],
            'Mean_Importance': importance_df['importance'] * 100,
            'Importance_RF': importance_df['importance'] * 100,
            'Importance_LGB': 0,
            'Importance_XGB': 0,
            'Std_Importance': 0,
            'CV': 0
        })
        
    else:
        # Modo completo - verificar cache primeiro
        cache_path = os.path.join(args.analysis_dir, 'importance_cache.pkl')
        
        if args.use_cache and os.path.exists(cache_path):
            try:
                import joblib
                cache_data = joblib.load(cache_path)
                logger.info("Cache carregado com sucesso")
                final_importance = cache_data.get('final_importance')
                
                # Filtrar apenas features que ainda existem
                final_importance = final_importance[final_importance['Feature'].isin(numeric_cols)]
                
            except Exception as e:
                logger.warning(f"Erro ao carregar cache: {e}")
                args.use_cache = False
        
        if not args.use_cache or 'final_importance' not in locals():
            # Separar dados
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # An√°lise com m√∫ltiplos modelos
            logger.info("\nIniciando an√°lise de import√¢ncia com m√∫ltiplos modelos...")
            
            # RandomForest
            rf_importance, rf_metrics = analyze_rf_importance(
                X_train, y_train, numeric_cols, n_folds=args.n_folds
            )
            
            # LightGBM
            lgb_importance, lgb_metrics = analyze_lgb_importance(
                X_train, y_train, numeric_cols, n_folds=args.n_folds
            )
            
            # XGBoost
            xgb_importance, xgb_metrics = analyze_xgb_importance(
                X_train, y_train, numeric_cols, n_folds=args.n_folds
            )
            
            # Combinar resultados
            final_importance = combine_importance_results(
                rf_importance, lgb_importance, xgb_importance
            )
            
            # Salvar cache
            if not args.fast_mode:
                try:
                    import joblib
                    cache_data = {
                        'final_importance': final_importance,
                        'timestamp': datetime.now()
                    }
                    joblib.dump(cache_data, cache_path)
                    logger.info("Cache salvo com sucesso")
                except Exception as e:
                    logger.warning(f"Erro ao salvar cache: {e}")
    
    # Salvar import√¢ncia das features
    importance_path = os.path.join(args.analysis_dir, 'feature_importance_combined.csv')
    final_importance.to_csv(importance_path, index=False)
    logger.info(f"Import√¢ncia das features salva em {importance_path}")
    
    # PASSO 3: Sele√ß√£o das top features baseado em import√¢ncia
    logger.info(f"\nüìä Selecionando top {args.max_features} features baseado em Mean_Importance")
    
    # Ordenar por import√¢ncia e pegar as top N
    top_features_df = final_importance.nlargest(args.max_features, 'Mean_Importance')
    top_features_sanitized = top_features_df['Feature'].tolist()
    
    # Converter de volta para nomes originais
    if rename_dict:
        reverse_rename_dict = {v: k for k, v in rename_dict.items()}
        top_features_original = []
        for feat in top_features_sanitized:
            original_name = reverse_rename_dict.get(feat, feat)
            top_features_original.append(original_name)
    else:
        top_features_original = top_features_sanitized
    
    logger.info(f"‚úÖ Top {args.max_features} features selecionadas")
    logger.info(f"   Import√¢ncia m√©dia das top features: {top_features_df['Mean_Importance'].mean():.4f}")
    logger.info(f"   Top 5 features: {top_features_original[:5]}")
    
    # Documentar features selecionadas
    recommended_path = os.path.join(args.analysis_dir, 'recommended_features.txt')
    with open(recommended_path, 'w') as f:
        for feature in top_features_original:
            f.write(f"{feature}\n")
    logger.info(f"Lista de features recomendadas salva em {recommended_path}")
    
    # Salvar tamb√©m um CSV com as top features e suas import√¢ncias
    top_features_info = top_features_df.copy()
    if rename_dict:
        # Adicionar coluna com nomes originais
        top_features_info['Original_Feature'] = [
            reverse_rename_dict.get(feat, feat) for feat in top_features_info['Feature']
        ]
    top_features_info.to_csv(
        os.path.join(args.analysis_dir, 'top_features_importance.csv'), 
        index=False
    )
    
    if not args.keep_all_features:
        # PASSO 4: Aplicar sele√ß√£o aos datasets, salvando APENAS as top features
        logger.info(f"\nüìÅ Salvando datasets com apenas as top {args.max_features} features...")
        
        for df_name, df_data in [('train', train_df), ('validation', val_df), ('test', test_df)]:
            # Verificar quais features existem no dataset
            features_in_dataset = [f for f in top_features_original if f in df_data.columns]
            missing_features = set(top_features_original) - set(features_in_dataset)
            
            if missing_features:
                logger.warning(f"  {df_name}: {len(missing_features)} features n√£o encontradas")
            
            # Adicionar target se existir
            columns_to_keep = features_in_dataset.copy()
            if target_col in df_data.columns:
                columns_to_keep.append(target_col)
            
            # Criar dataset selecionado
            df_selected = df_data[columns_to_keep]
            
            # Salvar
            output_path = os.path.join(args.output_dir, f"{df_name}.csv")
            df_selected.to_csv(output_path, index=False)
            
            logger.info(f"‚úÖ {df_name}: {df_data.shape[1]} ‚Üí {df_selected.shape[1]} colunas")
            logger.info(f"   Features: {len(features_in_dataset)}, Target: {'Sim' if target_col in columns_to_keep else 'N√£o'}")
        
        # Informa√ß√µes de output
        output_info = {
            'features_selected': args.max_features,
            'features_removed': initial_n_features - args.max_features,
            'original_features': initial_n_features,
            'features_after_correlation_removal': len(numeric_cols),
            'datasets_processed': ['train', 'validation', 'test'],
            'mode': 'fast' if args.fast_mode else 'complete',
            'top_5_features': top_features_original[:5]
        }
    else:
        logger.info("\nModo --keep-all-features ativado. Apenas gerando an√°lise.")
        output_info = {
            'analysis_only': True,
            'total_features': len(numeric_cols),
            'text_features': len(text_derived_cols)
        }
    
    # An√°lise de features textuais
    if text_derived_cols and not args.fast_mode:
        text_importance = analyze_text_features(final_importance, text_derived_cols)
        if text_importance is not None:
            text_path = os.path.join(args.analysis_dir, 'text_features_importance.csv')
            text_importance.to_csv(text_path, index=False)
    
    # Sumarizar categorias
    if not args.fast_mode:
        summarize_feature_categories(numeric_cols, final_importance, text_derived_cols)
    
    # Salvar configura√ß√£o
    save_configuration(args, output_info)
    
    # Tempo total
    total_time = time.time() - total_start_time
    logger.info(f"\n=== FEATURE SELECTION CONCLU√çDO EM {total_time:.1f} SEGUNDOS ===")
    logger.info(f"Resultados salvos em: {args.analysis_dir}")
    if not args.keep_all_features:
        logger.info(f"Datasets selecionados salvos em: {args.output_dir}")
        logger.info(f"Cada dataset cont√©m apenas as top {args.max_features} features + target")

if __name__ == "__main__":
    main()