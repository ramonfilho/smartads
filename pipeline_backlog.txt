PARTE 1: COLETA E INTEGRAÇÃO
├── 1.1 Carregamento de Dados
│   ├── load_survey_files() - Carrega pesquisas
│   ├── load_buyer_files() - Carrega compradores  
│   └── load_utm_files() - Carrega dados UTM
│
├── 1.2 Matching e Merge
│   ├── normalize_emails_preserving_originals() - Normaliza emails
│   ├── match_surveys_with_buyers_improved() - Faz matching
│   ├── create_target_variable() - Cria variável alvo
│   └── merge_datasets() - Mescla datasets
│
└── 1.3 Preparação Final
    ├── prepare_final_dataset() - Filtra colunas permitidas
    └── train_test_split() - Divide dados

PARTE 2: PRÉ-PROCESSAMENTO
├── data_cleaning.py
│   ├── consolidate_quality_columns() - Unifica colunas de qualidade
│   ├── handle_missing_values() - Trata valores ausentes
│   ├── handle_outliers() - Trata outliers
│   ├── normalize_values() - Normaliza valores numéricos
│   └── convert_data_types() - Converte tipos de dados
│
├── feature_engineering.py
│   ├── create_identity_features() - Features de identidade
│   ├── create_temporal_features() - Features temporais
│   └── encode_categorical_features() - Codifica categóricas
│
├── text_processing.py
│   ├── extract_basic_text_features() - Comprimento, palavras
│   ├── extract_sentiment_features() - Análise sentimento
│   ├── extract_tfidf_features() - TF-IDF básico
│   ├── extract_motivation_features() - Palavras motivação
│   └── extract_discriminative_features() - Termos discriminativos
│
└── advanced_feature_engineering.py
    ├── create_salary_features() - Features salariais
    ├── create_country_interaction_features() - Interações país
    ├── create_age_interaction_features() - Interações idade
    └── create_temporal_interaction_features() - Interações temporais

PARTE 3: FEATURE ENGINEERING PROFISSIONAL
└── professional_motivation_features.py
    ├── create_professional_motivation_score() - Score motivação
    ├── analyze_aspiration_sentiment() - Sentimento aspiração
    ├── detect_commitment_expressions() - Expressões compromisso
    ├── create_career_term_detector() - Termos carreira
    ├── enhance_tfidf_for_career_terms() - TF-IDF aprimorado
    └── perform_topic_modeling_fixed() - LDA para tópicos

PARTE 4: FEATURE SELECTION
└── feature_importance.py
    ├── analyze_rf_importance() - Random Forest
    ├── analyze_lgb_importance() - LightGBM
    ├── analyze_xgb_importance() - XGBoost
    └── combine_importance_results() - Combina resultados


### Tornar o script robusto e reutilizável:
CHECK - Criar uma lógica universal de reconhecimento de tipos colunas e verificar sua utilização ao longo do pipeline
CHECK - Criar uma lógica universão de nomenclatura de colunas e verificar sua utilização ao longo do pipeline
1 - Unificar a estrutura de parâmetros com o cache do classificador de colunas.

### Perguntar sobre a estrutura geral da pipeline. Comparar com outras pipelines já vistas. Prós e contras e oportunidades de melhoria que não vão "quebrar" o pipeline. Considere o tipo de problema que estamos tratando e a natureza dos dados.

1 -  Unificar Processamento de Texto
text_processing.py e professional_motivation_features.py têm sobreposição
Poderia criar um único módulo text_feature_pipeline.py que:
Processa texto básico
Aplica TF-IDF unificado
Extrai features profissionais
Aplica LDA

2 - Pipeline de Features Numéricas
Unificar advanced_feature_engineering.py com parte de feature_engineering.py
Criar numeric_feature_pipeline.py para todas as interações numéricas

3- Outros
- Certificar por que a coluna "Data" está sendo removida junto com as `COLUMNS_REMOVED_FOR_PRODUCTION_COMPATIBILITY`.
- Normalização é realmente necessária para esse dataset? Por que? 
- Verificar a função column_type_classifier e ver o que ela realmente faz e porque está tão grande. E garantir que ele tenha uma lógica confiável, nem que seja pelo conhecimento de domínio, mas que seja faiclmente expansível.
- Fazer cada processamento de acordo com o classificador de colunas. 






### Futuras implementações:
- Chamar LLM (langchain?) para determinar o vocabulário de palavras a serem usadas para criar as features de texto.
    - Tornar a pipeline robusta à linguas diferentes
- Criar função que reconhece as colunas dos dados de produção e faz matching com as de treino para verificar a integridade. Preventing data skew / data drift.
- Criar função que chama LLM para decidir se o arquivo é UTM, pesquisa ou compradores.
- Reconhecer o tratamento de dados necessário ou recomendado para aquela coluna.
 
### Sobre o software: 
- Toda vez que um cliente criar uma conta, gera um bucket na Cloud Storage ou pasta no google drive para ele. 
- Toda vez que ele adiciona um arquivo, perguntar se ainda existem mais arquivos, ou se ele já deseja começar o treinamento do modelo.
- Verificar requisitos de privacidade no bucket da cloud storage.