#!/usr/bin/env python
"""
Script completo para coleta e integra√ß√£o de dados - VERS√ÉO FINAL CORRIGIDA
Inclui todas as corre√ß√µes:
1. Preserva√ß√£o de emails originais
2. Deduplica√ß√£o de UTMs no merge
3. Corre√ß√£o do warning de dtype
4. Valida√ß√£o de integridade do target
5. Valida√ß√£o de compatibilidade com produ√ß√£o
"""

import os
import sys
import pandas as pd
import time
import numpy as np
import json

# Adicionar o diret√≥rio raiz do projeto ao sys.path
project_root = "/Users/ramonmoreira/Desktop/smart_ads"
sys.path.insert(0, project_root)

# Importar m√≥dulos existentes
from src.utils.local_storage import (
    connect_to_gcs, list_files_by_extension, categorize_files,
    load_csv_or_excel, load_csv_with_auto_delimiter, extract_launch_id
)
from src.preprocessing.email_processing import normalize_emails_in_dataframe, normalize_email
from src.preprocessing.column_normalization import normalize_survey_columns, validate_normalized_columns
from sklearn.model_selection import train_test_split

# ============================================================================
# CONFIGURA√á√ÉO: COLUNAS PERMITIDAS NA INFER√äNCIA
# ============================================================================

INFERENCE_COLUMNS = [
    # Dados de UTM
    'DATA',
    'E-MAIL',
    'UTM_CAMPAING',
    'UTM_SOURCE',
    'UTM_MEDIUM',
    'UTM_CONTENT',
    'UTM_TERM',
    'GCLID',
    
    # Dados da pesquisa
    'Marca temporal',
    '¬øC√≥mo te llamas?',
    '¬øCu√°l es tu g√©nero?',
    '¬øCu√°l es tu edad?',
    '¬øCual es tu pa√≠s?',
    '¬øCu√°l es tu e-mail?',
    '¬øCual es tu telefono?',
    '¬øCu√°l es tu instagram?',
    '¬øHace qu√°nto tiempo me conoces?',
    '¬øCu√°l es tu disponibilidad de tiempo para estudiar ingl√©s?',
    'Cuando hables ingl√©s con fluidez, ¬øqu√© cambiar√° en tu vida? ¬øQu√© oportunidades se abrir√°n para ti?',
    '¬øCu√°l es tu profesi√≥n?',
    '¬øCu√°l es tu sueldo anual? (en d√≥lares)',
    '¬øCu√°nto te gustar√≠a ganar al a√±o?',
    '¬øCrees que aprender ingl√©s te acercar√≠a m√°s al salario que mencionaste anteriormente?',
    '¬øCrees que aprender ingl√©s puede ayudarte en el trabajo o en tu vida diaria?',
    '¬øQu√© esperas aprender en el evento Cero a Ingl√©s Fluido?',
    'D√©jame un mensaje',
    
    # Features novas do L22
    '¬øCu√°les son tus principales razones para aprender ingl√©s?',
    '¬øHas comprado alg√∫n curso para aprender ingl√©s antes?',
    
    # Qualidade
    'Qualidade (Nome)',
    'Qualidade (N√∫mero)',
    
    # Vari√°vel alvo (adicionada durante o treino)
    'target'
]

# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================

def find_email_column(df):
    """Encontra a coluna que cont√©m emails em um DataFrame."""
    email_patterns = ['email', 'e-mail', 'correo', '@', 'mail']
    for col in df.columns:
        if any(pattern in col.lower() for pattern in email_patterns):
            return col
    return None

def normalize_emails_preserving_originals(df, survey_email_col='¬øCu√°l es tu e-mail?', utm_email_col='E-MAIL'):
    """
    Normaliza emails criando email_norm para matching, mas preserva as colunas originais.
    
    Args:
        df: DataFrame para normalizar
        survey_email_col: Nome da coluna de email das pesquisas
        utm_email_col: Nome da coluna de email das UTMs
        
    Returns:
        DataFrame com email_norm adicionado e colunas originais preservadas
    """
    df_result = df.copy()
    
    # Criar lista de poss√≠veis fontes de email
    email_sources = []
    
    # Verificar coluna de email das pesquisas
    if survey_email_col in df_result.columns:
        email_sources.append(survey_email_col)
    
    # Verificar coluna de email das UTMs
    if utm_email_col in df_result.columns:
        email_sources.append(utm_email_col)
    
    # Verificar coluna gen√©rica 'email'
    if 'email' in df_result.columns:
        email_sources.append('email')
    
    # CORRE√á√ÉO: Inicializar email_norm com tipo string para evitar FutureWarning
    df_result['email_norm'] = pd.Series(dtype='object')
    
    for source_col in email_sources:
        # Preencher email_norm com valores n√£o-nulos da fonte
        mask = df_result['email_norm'].isna() & df_result[source_col].notna()
        if mask.any():
            df_result.loc[mask, 'email_norm'] = df_result.loc[mask, source_col].apply(normalize_email)
            print(f"   üìß Normalized {mask.sum()} emails from '{source_col}'")
    
    return df_result

def preserve_email_data(df):
    """
    Preserva dados de email movendo-os para as colunas corretas antes da filtragem.
    
    Args:
        df: DataFrame original
        
    Returns:
        DataFrame com emails preservados nas colunas corretas
    """
    df_result = df.copy()
    
    # Se existe coluna 'email' gen√©rica, distribuir para as colunas corretas
    if 'email' in df_result.columns:
        # Para registros de pesquisa (n√£o t√™m E-MAIL preenchido)
        if '¬øCu√°l es tu e-mail?' not in df_result.columns:
            df_result['¬øCu√°l es tu e-mail?'] = np.nan
        
        # Copiar emails para a coluna de pesquisa onde apropriado
        mask_survey = df_result['email'].notna() & df_result['¬øCu√°l es tu e-mail?'].isna()
        if 'E-MAIL' in df_result.columns:
            # Se tem E-MAIL, s√≥ copiar onde E-MAIL est√° vazio (indica que √© da pesquisa)
            mask_survey = mask_survey & df_result['E-MAIL'].isna()
        
        df_result.loc[mask_survey, '¬øCu√°l es tu e-mail?'] = df_result.loc[mask_survey, 'email']
        
        # Para registros de UTM (t√™m E-MAIL)
        if 'E-MAIL' not in df_result.columns:
            df_result['E-MAIL'] = np.nan
            
        mask_utm = df_result['email'].notna() & df_result['E-MAIL'].isna()
        if '¬øCu√°l es tu e-mail?' in df_result.columns:
            # Se tem coluna de pesquisa, s√≥ copiar onde est√° vazia
            mask_utm = mask_utm & df_result['¬øCu√°l es tu e-mail?'].isna()
            
        df_result.loc[mask_utm, 'E-MAIL'] = df_result.loc[mask_utm, 'email']
    
    return df_result

def filter_to_inference_columns_preserving_data(df, add_missing=True):
    """
    Filtra DataFrame para conter apenas colunas dispon√≠veis na infer√™ncia,
    mas preserva os dados existentes nas colunas de email corretas.
    
    Args:
        df: DataFrame a ser filtrado
        add_missing: Se deve adicionar colunas faltantes com NaN
        
    Returns:
        DataFrame filtrado com dados preservados
    """
    # Primeiro, preservar dados de email se existirem em colunas gen√©ricas
    df_with_preserved_emails = preserve_email_data(df)
    
    # Colunas que existem no DataFrame e est√£o na lista de infer√™ncia
    existing_inference_cols = [col for col in INFERENCE_COLUMNS if col in df_with_preserved_emails.columns]
    
    # Filtrar DataFrame
    filtered_df = df_with_preserved_emails[existing_inference_cols].copy()
    
    if add_missing:
        # Adicionar colunas faltantes com NaN (exceto target)
        missing_cols = [col for col in INFERENCE_COLUMNS 
                       if col not in filtered_df.columns and col != 'target']
        
        for col in missing_cols:
            filtered_df[col] = np.nan
        
        # Reordenar colunas conforme INFERENCE_COLUMNS (exceto target se n√£o existir)
        final_cols = [col for col in INFERENCE_COLUMNS 
                     if col in filtered_df.columns]
        filtered_df = filtered_df[final_cols]
    
    return filtered_df

def validate_production_compatibility(df, show_warnings=True):
    """
    Valida se o DataFrame √© compat√≠vel com produ√ß√£o.
    
    Args:
        df: DataFrame a ser validado
        show_warnings: Se deve mostrar avisos detalhados
        
    Returns:
        Tuple (is_compatible, validation_report)
    """
    print("\nüîç Production Compatibility Validation:")
    
    validation_report = {
        'is_compatible': True,
        'errors': [],
        'warnings': [],
        'info': []
    }
    
    # Colunas esperadas em produ√ß√£o (sem target)
    production_columns = [col for col in INFERENCE_COLUMNS if col != 'target']
    
    # 1. Verificar colunas obrigat√≥rias
    df_columns = set(df.columns)
    missing_columns = set(production_columns) - df_columns
    extra_columns = df_columns - set(production_columns) - {'target'}  # target √© permitido em treino
    
    if missing_columns:
        validation_report['is_compatible'] = False
        validation_report['errors'].append(f"Missing required columns: {missing_columns}")
        if show_warnings:
            print(f"   ‚ùå Missing columns: {missing_columns}")
    else:
        print(f"   ‚úÖ All required columns present")
    
    if extra_columns:
        validation_report['warnings'].append(f"Extra columns found: {extra_columns}")
        if show_warnings:
            print(f"   ‚ö†Ô∏è  Extra columns (will be ignored): {extra_columns}")
    
    # 2. Verificar ordem das colunas
    expected_order = [col for col in production_columns if col in df.columns]
    actual_order = [col for col in df.columns if col in production_columns]
    
    if expected_order != actual_order:
        validation_report['warnings'].append("Column order differs from expected")
        if show_warnings:
            print(f"   ‚ö†Ô∏è  Column order differs from production expectation")
    else:
        print(f"   ‚úÖ Column order matches production")
    
    # 3. Verificar dados nas colunas cr√≠ticas
    critical_data_columns = ['¬øCu√°l es tu e-mail?', 'E-MAIL', 'DATA']
    for col in critical_data_columns:
        if col in df.columns:
            non_null_count = df[col].notna().sum()
            null_percentage = (df[col].isna().sum() / len(df) * 100) if len(df) > 0 else 0
            
            validation_report['info'].append(f"{col}: {non_null_count} non-null values ({100-null_percentage:.1f}% coverage)")
            
            if null_percentage > 90:
                validation_report['warnings'].append(f"{col} has {null_percentage:.1f}% null values")
                if show_warnings:
                    print(f"   ‚ö†Ô∏è  {col} has very low data coverage: {100-null_percentage:.1f}%")
    
    # 4. Verificar target (se presente)
    if 'target' in df.columns:
        target_values = df['target'].unique()
        if not set(target_values).issubset({0, 1, np.nan}):
            validation_report['errors'].append(f"Invalid target values: {target_values}")
            validation_report['is_compatible'] = False
            if show_warnings:
                print(f"   ‚ùå Invalid target values found: {target_values}")
        else:
            positive_rate = (df['target'] == 1).sum() / len(df) * 100 if len(df) > 0 else 0
            validation_report['info'].append(f"Target positive rate: {positive_rate:.2f}%")
            print(f"   ‚úÖ Target variable valid (positive rate: {positive_rate:.2f}%)")
    
    # 5. Sum√°rio
    print(f"\n   üìä Validation Summary:")
    print(f"      Errors: {len(validation_report['errors'])}")
    print(f"      Warnings: {len(validation_report['warnings'])}")
    print(f"      Compatible: {'‚úÖ Yes' if validation_report['is_compatible'] else '‚ùå No'}")
    
    return validation_report['is_compatible'], validation_report

# ============================================================================
# FUN√á√ïES DE PROCESSAMENTO
# ============================================================================

def process_survey_file(bucket, file_path):
    """Processa um arquivo de pesquisa preservando a coluna de email original."""
    df = load_csv_or_excel(bucket, file_path)
    if df is None:
        return None
        
    # Identificar o lan√ßamento deste arquivo
    launch_id = extract_launch_id(file_path)
    
    # Normalizar colunas
    df = normalize_survey_columns(df, launch_id)
    validate_normalized_columns(df, launch_id)
    
    # N√ÉO renomear coluna de email para 'email' gen√©rico
    # Manter a coluna original (¬øCu√°l es tu e-mail?)
    
    # Adicionar identificador de lan√ßamento se dispon√≠vel
    if launch_id:
        df['lan√ßamento'] = launch_id
    
    return df

def process_buyer_file(bucket, file_path):
    """Processa um arquivo de compradores."""
    df = load_csv_or_excel(bucket, file_path)
    if df is None:
        return None
        
    # Identificar o lan√ßamento deste arquivo
    launch_id = extract_launch_id(file_path)
    
    # Para buyers, ainda precisamos da coluna 'email' para normaliza√ß√£o
    email_col = find_email_column(df)
    if email_col and email_col != 'email':
        df = df.rename(columns={email_col: 'email'})
    elif not email_col:
        print(f"  - Warning: No email column found in {file_path}. Available columns: {', '.join(df.columns[:5])}...")
    
    # Adicionar identificador de lan√ßamento se dispon√≠vel
    if launch_id:
        df['lan√ßamento'] = launch_id
    
    return df

def process_utm_file(bucket, file_path):
    """Processa um arquivo de UTM preservando a coluna E-MAIL."""
    df = load_csv_with_auto_delimiter(bucket, file_path)
    if df is None:
        return None
        
    # Identificar o lan√ßamento deste arquivo
    launch_id = extract_launch_id(file_path)
    
    # N√ÉO renomear E-MAIL para 'email' gen√©rico
    # Manter a coluna original (E-MAIL)
    
    # Adicionar identificador de lan√ßamento se dispon√≠vel
    if launch_id:
        df['lan√ßamento'] = launch_id
    
    return df

# ============================================================================
# FUN√á√ïES DE CARREGAMENTO
# ============================================================================

def load_survey_files(bucket, survey_files):
    """Carrega todos os arquivos de pesquisa preservando emails originais."""
    survey_dfs = []
    launch_data = {}
    
    print("\nLoading survey files...")
    for file_path in survey_files:
        try:
            df = process_survey_file(bucket, file_path)
            if df is not None:
                survey_dfs.append(df)
                
                launch_id = extract_launch_id(file_path)
                if launch_id:
                    if launch_id not in launch_data:
                        launch_data[launch_id] = {}
                    launch_data[launch_id]['survey'] = df
                    
                print(f"  - Loaded: {file_path} ({launch_id if launch_id else ''}), {df.shape[0]} rows, {df.shape[1]} columns")
        except Exception as e:
            print(f"  - Error processing {file_path}: {str(e)}")
    
    return survey_dfs, launch_data

def load_buyer_files(bucket, buyer_files):
    """Carrega todos os arquivos de compradores."""
    buyer_dfs = []
    launch_data = {}
    
    print("\nLoading buyer files...")
    for file_path in buyer_files:
        try:
            df = process_buyer_file(bucket, file_path)
            if df is not None:
                buyer_dfs.append(df)
                
                launch_id = extract_launch_id(file_path)
                if launch_id:
                    if launch_id not in launch_data:
                        launch_data[launch_id] = {}
                    launch_data[launch_id]['buyer'] = df
                    
                print(f"  - Loaded: {file_path} ({launch_id if launch_id else ''}), {df.shape[0]} rows, {df.shape[1]} columns")
        except Exception as e:
            print(f"  - Error processing {file_path}: {str(e)}")
    
    return buyer_dfs, launch_data

def load_utm_files(bucket, utm_files):
    """Carrega todos os arquivos de UTM preservando E-MAIL original."""
    utm_dfs = []
    launch_data = {}
    
    print("\nLoading UTM files...")
    for file_path in utm_files:
        try:
            df = process_utm_file(bucket, file_path)
            if df is not None:
                utm_dfs.append(df)
                
                launch_id = extract_launch_id(file_path)
                if launch_id:
                    if launch_id not in launch_data:
                        launch_data[launch_id] = {}
                    launch_data[launch_id]['utm'] = df
                    
                print(f"  - Loaded: {file_path} ({launch_id if launch_id else ''}), {df.shape[0]} rows, {df.shape[1]} columns")
        except Exception as e:
            print(f"  - Error processing {file_path}: {str(e)}")
    
    return utm_dfs, launch_data

# ============================================================================
# MATCHING E INTEGRA√á√ÉO
# ============================================================================

def match_surveys_with_buyers_improved(surveys, buyers, utms=None):
    """
    Realiza correspond√™ncia entre pesquisas e compradores usando email_norm.
    Adaptado para trabalhar com as colunas de email originais preservadas.
    """
    print("\nMatching surveys with buyers (preserving original email columns)...")
    start_time = time.time()
    
    # Verificar se podemos prosseguir com a correspond√™ncia
    if surveys.empty or buyers.empty or 'email_norm' not in buyers.columns or 'email_norm' not in surveys.columns:
        print("Warning: Cannot perform matching. Missing email_norm column.")
        return pd.DataFrame(columns=['buyer_id', 'survey_id', 'match_type', 'score'])
    
    # Preparar estruturas de dados para matching eficiente
    survey_emails_dict = dict(zip(surveys['email_norm'], surveys.index))
    survey_emails_set = set(surveys['email_norm'].dropna())
    
    matches = []
    match_count = 0
    
    # Processar cada comprador
    for idx, buyer in buyers.iterrows():
        buyer_email_norm = buyer.get('email_norm')
        if pd.isna(buyer_email_norm):
            continue
        
        # Matching direto com email_norm
        if buyer_email_norm in survey_emails_set:
            survey_idx = survey_emails_dict[buyer_email_norm]
            
            match_data = {
                'buyer_id': idx,
                'survey_id': survey_idx,
                'match_type': 'exact',
                'score': 1.0
            }
            
            # Adicionar informa√ß√£o de lan√ßamento se dispon√≠vel
            if 'lan√ßamento' in buyer and not pd.isna(buyer['lan√ßamento']):
                match_data['lan√ßamento'] = buyer['lan√ßamento']
            
            matches.append(match_data)
            match_count += 1
    
    print(f"   üéØ Found {match_count} matches out of {len(buyers)} buyers")
    print(f"   üìà Match rate: {(match_count/len(buyers)*100):.1f}%")
    
    matches_df = pd.DataFrame(matches)
    
    # Calcular tempo gasto
    end_time = time.time()
    print(f"   ‚è±Ô∏è  Matching completed in {end_time - start_time:.2f} seconds.")
    
    return matches_df

def create_target_variable(surveys_df, matches_df):
    """Cria a vari√°vel alvo com base nas correspond√™ncias de compras."""
    if surveys_df.empty:
        print("No surveys data - creating empty DataFrame with target variable")
        return pd.DataFrame(columns=['target'])
    
    # Copiar o DataFrame para n√£o modificar o original
    result_df = surveys_df.copy()
    
    # Adicionar coluna de target inicializada com 0
    result_df['target'] = 0
    
    # Se n√£o houver correspond√™ncias, retornar com todos zeros
    if matches_df.empty:
        print("No matches found - target variable will be all zeros")
        return result_df
    
    # Marcar os registros correspondentes como positivos
    matched_count = 0
    for _, match in matches_df.iterrows():
        survey_id = match['survey_id']
        if survey_id in result_df.index:
            result_df.loc[survey_id, 'target'] = 1
            matched_count += 1
    
    # VALIDA√á√ÉO: Verificar integridade do target
    positive_count = result_df['target'].sum()
    expected_positives = len(matches_df)
    
    print(f"   üìä Created target variable: {positive_count} positive examples out of {len(result_df)}")
    
    if positive_count != expected_positives:
        print(f"   ‚ö†Ô∏è  WARNING: Target integrity check failed!")
        print(f"      Expected {expected_positives} positives (from matches)")
        print(f"      Got {positive_count} positives in target")
        print(f"      Difference: {positive_count - expected_positives}")
        
        # Investigar a discrep√¢ncia
        if positive_count < expected_positives:
            print(f"      Some matches have survey_ids not in the survey DataFrame")
        else:
            print(f"      Target has more positives than matches - this should not happen!")
    else:
        print(f"   ‚úÖ Target integrity check passed")
    
    return result_df

def merge_datasets(surveys_df, utm_df, buyers_df):
    """
    Mescla as diferentes fontes de dados em um √∫nico dataset.
    CORRIGIDO: Remove duplicatas de UTM antes do merge para evitar explos√£o de registros.
    """
    print("Merging datasets (preserving original email columns)...")
    
    if surveys_df.empty:
        print("WARNING: Empty survey data")
        return pd.DataFrame()
    
    # Mesclar pesquisas com UTM usando email_norm
    if not utm_df.empty and 'email_norm' in utm_df.columns and 'email_norm' in surveys_df.columns:
        # CORRE√á√ÉO: Remover duplicatas de email_norm nas UTMs antes do merge
        print(f"   üìä UTM records before deduplication: {len(utm_df):,}")
        utm_df_dedup = utm_df.drop_duplicates(subset=['email_norm'])
        print(f"   ‚úÇÔ∏è  UTM records after deduplication: {len(utm_df_dedup):,}")
        print(f"   üîÑ Removed {len(utm_df) - len(utm_df_dedup):,} duplicate emails from UTMs")
        
        merged_df = pd.merge(
            surveys_df,
            utm_df_dedup,
            on='email_norm',
            how='left',
            suffixes=('', '_utm')
        )
        print(f"   ‚úÖ Merged surveys with UTM data: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns")
        
        # Verificar que n√£o houve explos√£o de registros
        if len(merged_df) > len(surveys_df):
            print(f"   ‚ö†Ô∏è  WARNING: Merge created {len(merged_df) - len(surveys_df)} extra records!")
            print("   üîç This should not happen with deduplicated UTMs. Investigating...")
        
        # Consolidar colunas de email das UTMs
        if 'E-MAIL_utm' in merged_df.columns and 'E-MAIL' not in merged_df.columns:
            merged_df['E-MAIL'] = merged_df['E-MAIL_utm']
        elif 'E-MAIL_utm' in merged_df.columns and 'E-MAIL' in merged_df.columns:
            # Preencher valores vazios de E-MAIL com E-MAIL_utm
            mask = merged_df['E-MAIL'].isna() & merged_df['E-MAIL_utm'].notna()
            merged_df.loc[mask, 'E-MAIL'] = merged_df.loc[mask, 'E-MAIL_utm']
    else:
        merged_df = surveys_df.copy()
        print("   ‚ö†Ô∏è  No UTM data available for merging")
    
    print(f"   üìä Final merged dataset: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns")
    return merged_df

def prepare_final_dataset(df):
    """
    Prepara o dataset final removendo apenas email_norm e preservando emails originais.
    """
    print("\nPreparing final dataset (preserving original emails)...")
    
    print(f"   üìä Original dataset: {df.shape[0]} rows, {df.shape[1]} columns")
    
    # Listar todas as colunas originais
    original_columns = set(df.columns)
    
    # Primeiro, preservar dados de email nas colunas corretas
    df_preserved = preserve_email_data(df)
    
    # Remover email_norm (usado apenas para matching)
    if 'email_norm' in df_preserved.columns:
        df_preserved = df_preserved.drop(columns=['email_norm'])
        print(f"   ‚úÇÔ∏è  Removed email_norm column (matching-only)")
    
    # Remover coluna 'email' gen√©rica se ainda existir
    if 'email' in df_preserved.columns:
        df_preserved = df_preserved.drop(columns=['email'])
        print(f"   ‚úÇÔ∏è  Removed generic 'email' column")
    
    # Filtrar para colunas de infer√™ncia (preservando dados)
    df_filtered = filter_to_inference_columns_preserving_data(df_preserved, add_missing=True)
    
    # An√°lise de colunas
    final_columns = set(df_filtered.columns)
    removed_columns = original_columns - final_columns
    added_columns = final_columns - original_columns
    
    print(f"\n   üìã AN√ÅLISE DETALHADA DE COLUNAS:")
    print(f"   üìä Colunas originais: {len(original_columns)}")
    print(f"   üìä Colunas finais: {len(final_columns)}")
    
    if removed_columns:
        print(f"   üóëÔ∏è  Colunas REMOVIDAS ({len(removed_columns)}):")
        for col in sorted(removed_columns):
            print(f"      - {col}")
    
    # Verificar se as colunas de email t√™m dados
    email_cols_status = []
    if '¬øCu√°l es tu e-mail?' in df_filtered.columns:
        non_null = df_filtered['¬øCu√°l es tu e-mail?'].notna().sum()
        email_cols_status.append(f"'¬øCu√°l es tu e-mail?': {non_null} valores")
    
    if 'E-MAIL' in df_filtered.columns:
        non_null = df_filtered['E-MAIL'].notna().sum()
        email_cols_status.append(f"'E-MAIL': {non_null} valores")
    
    if email_cols_status:
        print(f"   üìß Status das colunas de email:")
        for status in email_cols_status:
            print(f"      - {status}")
    
    print(f"   üìä Final dataset: {df_filtered.shape[0]} rows, {df_filtered.shape[1]} columns")
    
    return df_filtered

def split_data(df, output_dir, test_size=0.3, val_size=0.5, stratify=True, random_state=42):
    """Divide os dados em conjuntos de treino, valida√ß√£o e teste."""
    print("\nSplitting data into train/val/test sets...")
    
    if df.shape[0] == 0:
        print("WARNING: Empty dataset - cannot split.")
        empty_df = pd.DataFrame(columns=df.columns)
        return empty_df, empty_df, empty_df
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Verificar se temos target para estratificar
    if stratify and 'target' in df.columns and df['target'].nunique() > 1:
        print("   üìä Using stratified split based on target variable")
        strat_col = df['target']
    else:
        print("   üé≤ Using random split (no stratification)")
        strat_col = None
    
    # Primeira divis√£o: treino vs. (valida√ß√£o + teste)
    train_df, temp_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=strat_col
    )
    
    # Segunda divis√£o: valida√ß√£o vs. teste
    if stratify and 'target' in temp_df.columns and temp_df['target'].nunique() > 1:
        strat_col_temp = temp_df['target']
    else:
        strat_col_temp = None
    
    val_df, test_df = train_test_split(
        temp_df,
        test_size=val_size,
        random_state=random_state,
        stratify=strat_col_temp
    )
    
    # Salvar os conjuntos
    print(f"   üìÅ Train set: {train_df.shape[0]} rows, {train_df.shape[1]} columns")
    train_df.to_csv(os.path.join(output_dir, "train.csv"), index=False)
    
    print(f"   üìÅ Validation set: {val_df.shape[0]} rows, {val_df.shape[1]} columns")
    val_df.to_csv(os.path.join(output_dir, "validation.csv"), index=False)
    
    print(f"   üìÅ Test set: {test_df.shape[0]} rows, {test_df.shape[1]} columns")
    test_df.to_csv(os.path.join(output_dir, "test.csv"), index=False)
    
    return train_df, val_df, test_df

# ============================================================================
# PIPELINE PRINCIPAL
# ============================================================================

def main():
    """Pipeline principal corrigido para preservar dados de email."""
    print("üöÄ Starting CORRECTED data collection and integration pipeline...")
    print("   ‚úÖ FIX: Preserving original email data in correct columns")
    print("   ‚úÖ FIX: Using email_norm only for matching")
    print("   ‚úÖ FIX: Deduplicating UTMs before merge")
    print("   ‚úÖ FIX: Validating target integrity")
    print("   ‚úÖ FIX: Production compatibility validation")
    
    # 1. Conectar ao armazenamento local
    print("\n1Ô∏è‚É£ Setting up connection to local storage...")
    bucket = connect_to_gcs("local_bucket")
    
    # 2. Listar e categorizar arquivos
    print("\n2Ô∏è‚É£ Discovering and categorizing files...")
    file_paths = list_files_by_extension(bucket, prefix="")
    print(f"   üìÅ Found {len(file_paths)} files")
    
    # 3. Categorizar arquivos
    survey_files, buyer_files, utm_files, all_files_by_launch = categorize_files(file_paths)
    
    print(f"   üìä Survey files: {len(survey_files)}")
    print(f"   üí∞ Buyer files: {len(buyer_files)}")
    print(f"   üîó UTM files: {len(utm_files)}")
    
    # 4. Carregar dados (preservando colunas originais)
    print("\n3Ô∏è‚É£ Loading data files (preserving original columns)...")
    survey_dfs, _ = load_survey_files(bucket, survey_files)
    buyer_dfs, _ = load_buyer_files(bucket, buyer_files)
    utm_dfs, _ = load_utm_files(bucket, utm_files)
    
    # 5. Combinar datasets
    print("\n4Ô∏è‚É£ Combining datasets...")
    surveys = pd.concat(survey_dfs, ignore_index=True) if survey_dfs else pd.DataFrame()
    buyers = pd.concat(buyer_dfs, ignore_index=True) if buyer_dfs else pd.DataFrame()
    utms = pd.concat(utm_dfs, ignore_index=True) if utm_dfs else pd.DataFrame()
    
    print(f"   üìä Survey data: {surveys.shape[0]:,} rows, {surveys.shape[1]} columns")
    print(f"   üí∞ Buyer data: {buyers.shape[0]:,} rows, {buyers.shape[1]} columns")
    print(f"   üîó UTM data: {utms.shape[0]:,} rows, {utms.shape[1]} columns")
    
    # 6. Normalizar emails (criando email_norm para matching)
    print("\n5Ô∏è‚É£ Normalizing emails for matching...")
    
    # Para surveys - preservar coluna original
    if not surveys.empty:
        surveys = normalize_emails_preserving_originals(surveys)
        print(f"   ‚úÖ Survey emails normalized (preserving originals)")
    
    # Para buyers - ainda precisa da coluna 'email'
    if not buyers.empty and 'email' in buyers.columns:
        buyers = normalize_emails_in_dataframe(buyers)
        print(f"   ‚úÖ Buyer emails normalized")
    
    # Para UTMs - preservar coluna original
    if not utms.empty:
        utms = normalize_emails_preserving_originals(utms)
        print(f"   ‚úÖ UTM emails normalized (preserving originals)")
    
    # 7. Matching
    print("\n6Ô∏è‚É£ Performing matching...")
    matches_df = match_surveys_with_buyers_improved(surveys, buyers, utms)
    
    # 8. Criar vari√°vel alvo
    print("\n7Ô∏è‚É£ Creating target variable...")
    surveys_with_target = create_target_variable(surveys, matches_df)
    
    # 9. Mesclar datasets
    print("\n8Ô∏è‚É£ Merging datasets...")
    merged_data = merge_datasets(surveys_with_target, utms, pd.DataFrame())
    
    # 10. Preparar dataset final (preservando emails)
    print("\n9Ô∏è‚É£ Preparing final dataset...")
    final_data = prepare_final_dataset(merged_data)
    
    # NOVA VALIDA√á√ÉO: Verificar compatibilidade com produ√ß√£o
    print("\nüîü Validating production compatibility...")
    is_compatible, validation_report = validate_production_compatibility(final_data)
    
    # 11. Estat√≠sticas finais
    print("\n1Ô∏è‚É£1Ô∏è‚É£ Final statistics...")
    if 'target' in final_data.columns:
        target_counts = final_data['target'].value_counts()
        total_records = len(final_data)
        positive_rate = (target_counts.get(1, 0) / total_records * 100) if total_records > 0 else 0
        print(f"   üéØ Target variable distribution:")
        print(f"      - Negative (0): {target_counts.get(0, 0):,} ({100-positive_rate:.2f}%)")
        print(f"      - Positive (1): {target_counts.get(1, 0):,} ({positive_rate:.2f}%)")
        
        # VALIDA√á√ÉO FINAL: Verificar se n√∫mero de positivos est√° correto
        if len(matches_df) > 0:
            expected_positives = len(matches_df)
            actual_positives = target_counts.get(1, 0)
            if actual_positives != expected_positives:
                print(f"\n   ‚ö†Ô∏è  ALERT: Final target validation")
                print(f"      Matches found: {expected_positives}")
                print(f"      Final positives: {actual_positives}")
                print(f"      Discrepancy: {actual_positives - expected_positives}")
    
    # 12. Split dos dados
    print("\n1Ô∏è‚É£2Ô∏è‚É£ Splitting data for ML pipeline...")
    output_dir = os.path.join(project_root, "data", "V5", "01_split")
    
    if final_data.shape[0] > 0:
        train_df, val_df, test_df = split_data(final_data, output_dir, stratify=True)
        
        print(f"\n‚úÖ Pipeline completed successfully!")
        print(f"   üìÅ Data saved to: {output_dir}")
        print(f"   üìä Final dataset: {final_data.shape[0]:,} rows, {final_data.shape[1]} columns")
        print(f"   üè≠ Production compatible: {'Yes' if is_compatible else 'No'}")
        
        # Verificar status final dos emails
        print(f"\nüìß Email data preservation check:")
        if '¬øCu√°l es tu e-mail?' in final_data.columns:
            survey_emails = final_data['¬øCu√°l es tu e-mail?'].notna().sum()
            print(f"   ‚úÖ Survey emails preserved: {survey_emails:,}")
        
        if 'E-MAIL' in final_data.columns:
            utm_emails = final_data['E-MAIL'].notna().sum()
            print(f"   ‚úÖ UTM emails preserved: {utm_emails:,}")
        
        # Salvar relat√≥rio de valida√ß√£o
        validation_report_path = os.path.join(output_dir, "validation_report.json")
        with open(validation_report_path, 'w') as f:
            json.dump(validation_report, f, indent=2)
        print(f"   üìÑ Validation report saved to: {validation_report_path}")
    else:
        print(f"\n‚ùå No data was processed successfully!")

if __name__ == "__main__":
    main()