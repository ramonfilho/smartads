Se # -*- coding: utf-8 -*-
"""smart_ads_devclub_eda_V3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f253LusZuLtjSQBM18dZHzaQHOwdomg9

## 1- Importar os arquivos do ambiente local e imprimir as abas e colunas de cada um.
"""

from google.colab import files
import pandas as pd
import os
# Upload dos arquivos
print("üì§ FA√áA UPLOAD DOS ARQUIVOS .XLSX")
uploaded = files.upload()

"""## 2- Remover dados duplicados e abas desnecess√°rias:
1. Abas em branco
2. Abas que n√£o contenham "Pesquisa", "Vendas", "Alunos", "tmb", "Guru", "Sheet"
3. As abas que contenham "Leads", "Pontua√ß√£o", "Lead Score", "DEBUG_LOG", "Tabela Din√¢mica 1", "Detalhe1" ou que tenham menos de 230 linhas.
4. Abas de alunos dos arquivos LF.
"""

# FILTRAR ABAS + REMOVER DUPLICATAS - OPERA√á√ÉO COMBINADA
print("üîÑ FILTRAGEM DE ABAS + REMO√á√ÉO DE DUPLICATAS")
print("=" * 60)

# Definir crit√©rios de filtragem
termos_manter = ["Pesquisa", "Vendas", "tmb", "Guru", "Sheet"]
termos_remover = ["Pontua√ß√£o", "Lead Score", "DEBUG_LOG", "Tabela Din√¢mica 1", "Detalhe1", "Alunos", "Guru", "TMB", "LEADS"]
min_linhas = 230

arquivos_processados = {}
relatorio_completo = []

for filename in uploaded.keys():
   try:
       xl_file = pd.ExcelFile(filename)
       abas_originais = len(xl_file.sheet_names)
       abas_processadas = {}

       for sheet_name in xl_file.sheet_names:
           df = pd.read_excel(xl_file, sheet_name=sheet_name)
           linhas_antes_filtro = len(df)

           # APLICAR CRIT√âRIOS DE FILTRAGEM
           deve_remover_por_termo = any(termo.lower() in sheet_name.lower() for termo in termos_remover)
           tem_termo_permitido = any(termo.lower() in sheet_name.lower() for termo in termos_manter)
           tem_linhas_suficientes = len(df) >= min_linhas
           nao_esta_vazia = len(df) > 0 and not df.empty

           # Crit√©rio espec√≠fico: remover abas TMB e Guru dos arquivos LF (exceto LF06 que n√£o tem dados de vendas principais)
           eh_lf_com_vendas = 'LF' in filename and any(vendas_termo.lower() in sheet_name.lower() for vendas_termo in ['tmb', 'guru']) and 'LF06' not in filename

           # Decidir se mant√©m a aba
           if (nao_esta_vazia and not deve_remover_por_termo and not eh_lf_com_vendas and
               (tem_termo_permitido or tem_linhas_suficientes)):

               # REMOVER DUPLICATAS
               df_sem_duplicatas = df.drop_duplicates(keep='first')
               linhas_finais = len(df_sem_duplicatas)
               duplicatas_removidas = linhas_antes_filtro - linhas_finais

               # Salvar aba processada
               abas_processadas[sheet_name] = df_sem_duplicatas

               # Adicionar ao relat√≥rio
               relatorio_completo.append({
                   'arquivo': filename,
                   'aba': sheet_name,
                   'linhas_original': linhas_antes_filtro,
                   'linhas_final': linhas_finais,
                   'duplicatas_removidas': duplicatas_removidas,
                   'status': 'MANTIDA + LIMPA'
               })
           else:
               # Aba removida - adicionar ao relat√≥rio para controle
               relatorio_completo.append({
                   'arquivo': filename,
                   'aba': sheet_name,
                   'linhas_original': linhas_antes_filtro,
                   'linhas_final': 0,
                   'duplicatas_removidas': 0,
                   'status': 'REMOVIDA'
               })

       # Salvar arquivo processado se tiver abas v√°lidas
       if abas_processadas:
           arquivos_processados[filename] = abas_processadas

   except Exception as e:
       print(f"‚ùå ERRO ao processar {filename}: {e}")

# RELAT√ìRIO DE ABAS MANTIDAS
print(f"\nüìä ABAS MANTIDAS E PROCESSADAS")
print("=" * 80)
print(f"{'ARQUIVO':<35} {'ABA':<20} {'ORIGINAL':>10} {'FINAL':>10} {'REMOVIDAS':>10}")
print("-" * 80)

total_original = 0
total_final = 0
total_duplicatas = 0
abas_mantidas = 0

for item in relatorio_completo:
   if item['status'] == 'MANTIDA + LIMPA':
       print(f"{item['arquivo'][:34]:<35} {item['aba'][:19]:<20} "
             f"{item['linhas_original']:>10,} {item['linhas_final']:>10,} "
             f"{item['duplicatas_removidas']:>10,}")

       total_original += item['linhas_original']
       total_final += item['linhas_final']
       total_duplicatas += item['duplicatas_removidas']
       abas_mantidas += 1

print("-" * 80)
print(f"{'TOTAL':<35} {'':<20} {total_original:>10,} {total_final:>10,} {total_duplicatas:>10,}")

print(f"\nüìà RESUMO FINAL:")
print(f"Arquivos processados: {len(arquivos_processados)}")
print(f"Abas mantidas: {abas_mantidas}")
print(f"Abas removidas: {len(relatorio_completo) - abas_mantidas}")
print(f"Linhas totais ap√≥s processamento: {total_final:,}")
print(f"Duplicatas removidas: {total_duplicatas:,}")
print(f"Redu√ß√£o por duplicatas: {(total_duplicatas/total_original*100):.2f}%")

# Disponibilizar dados limpos
arquivos_filtrados = arquivos_processados
print(f"\n‚úÖ Dados processados dispon√≠veis na vari√°vel 'arquivos_filtrados'")

"""## 3- Remo√ß√£o das colunas desnecess√°rias mapeadas manualmente"""

# LIMPEZA DE COLUNAS DESNECESS√ÅRIAS - ARQUIVOS FILTRADOS
print("LIMPEZA DE COLUNAS DESNECESS√ÅRIAS - ARQUIVOS FILTRADOS")
print("=" * 60)

def obter_colunas_remover_exatas():
   """Lista exata das colunas para remover"""
   return [
       'Faixa 3.0', 'Faixa A 3.0', 'Faixa B 3.0', 'Faixa C 3.0', 'Faixa D 3.0',
       'Pontua√ß√£o 3.0', 'Score 3.0', 'adquirente nome', 'adquirente tid',
       'assinatura ciclo', 'assinatura c√≥digo', 'Bairro', 'bairro contato',
       'CEP', 'cep', 'cep contato', 'Cidade', 'cidade contato', 'Cliente CPF',
       'c√≥digo de rastreamento', 'codigo telefone contato', 'Complemento',
       'complemento contato', 'cupom c√≥digo', 'cupom valor',
       'Data Cancelado', 'data 1¬™ captura', 'data cancelamento', 'Data Efetivado',
       'data garantia', 'data indispon√≠vel', 'Data de Renova√ß√£o Prevista',
       'data √∫ltima captura', 'doc contato', 'Endere√ßo completo', 'Estado',
       'estado contato', 'externalid', 'Faixa', 'Faixa A', 'Faixa B', 'Faixa C',
       'Faixa D', 'Faixa E', 'fbc', 'fbp', 'id marketplace',
       'id marketplace ultima venda', 'id produto', 'id transa√ß√£o',
       'linha digit√°vel do boleto', 'Logradouro', 'logradouro contato',
       'Modalidade de Contrato', 'moeda', 'motivo reembolso',
       'nome da transportadora', 'nome empresa contato', 'nome marketplace',
       'nome martketplace ultima venda', 'nome oferta', 'Oferta', 'origem 1',
       'origem 2', 'origem 3', 'origem rppc venda', 'Page URL', 'pagamento',
       'pais', 'Pa√≠s', 'pa√≠s contato', 'parcelas', 'Pedido', 'pix',
       'Pontua√ß√£o', 'primeira captura', 'primeira origem', 'Produtor',
       'quantidade produto', 'Remote IP', 'Renova√ß√£o automatica',
       'Renova√ß√£o enviada', 'resposta auto atribui√ß√£o', 'retorno marketplace',
       'rppc checkout', 'rppc utm campaign', 'rppc utm content',
       'rppc utm medium', 'rppc utm term', 'rppc venda', 'Score',
       'servi√ßo da transportadora', 'Status', 'status', 'Status Cancelamento',
       'Status Financeiro', 'tempo de entrega', 'tipo', '√∫ltima captura',
       '√∫ltima origem', 'url do boleto', 'url oferta', 'User Agent',
       'valor afiliado', 'valor da transportadora', 'valor desconto', 'valor frete',
       'valor imposto', 'valor l√≠quido', 'valor marketplace', 'valor parcelas',
       'valor venda', 'vencimento do boleto', "Qual estado voc√™ mora?", "data pedido",
       "n√∫mero contato", "N√∫mero"
   ]

def aplicar_limpeza_colunas():
   """Remove colunas desnecess√°rias de todos os arquivos filtrados"""
   colunas_remover = obter_colunas_remover_exatas()
   colunas_remover_lower = [col.lower() for col in colunas_remover]

   arquivos_limpos = {}
   relatorio_limpeza = []

   for arquivo, abas_dict in arquivos_filtrados.items():
       abas_limpas = {}

       for aba_nome, df in abas_dict.items():
           colunas_antes = len(df.columns)

           # Identificar colunas para remover
           colunas_para_remover = []
           for col in df.columns:
               # Remover se est√° na lista exata
               if col.lower() in colunas_remover_lower:
                   colunas_para_remover.append(col)
               # Remover colunas Unnamed
               elif col.startswith('Unnamed:'):
                   colunas_para_remover.append(col)

           # Aplicar remo√ß√£o
           df_limpo = df.drop(columns=colunas_para_remover) if colunas_para_remover else df.copy()
           abas_limpas[aba_nome] = df_limpo

           # Relat√≥rio
           colunas_depois = len(df_limpo.columns)
           relatorio_limpeza.append({
               'arquivo': arquivo,
               'aba': aba_nome,
               'colunas_antes': colunas_antes,
               'colunas_depois': colunas_depois,
               'removidas': len(colunas_para_remover)
           })

       arquivos_limpos[arquivo] = abas_limpas

   return arquivos_limpos, relatorio_limpeza

# Executar limpeza
arquivos_limpos, relatorio = aplicar_limpeza_colunas()

# Exibir relat√≥rio
print(f"{'ARQUIVO':<35} {'ABA':<20} {'ANTES':>8} {'DEPOIS':>8} {'REMOVIDAS':>10}")
print("-" * 90)

total_removidas = 0
for item in relatorio:
   print(f"{item['arquivo'][:34]:<35} {item['aba'][:19]:<20} "
         f"{item['colunas_antes']:>8} {item['colunas_depois']:>8} {item['removidas']:>10}")
   total_removidas += item['removidas']

print("-" * 90)
print(f"Total de colunas removidas: {total_removidas}")

# Disponibilizar dados limpos
arquivos_filtrados_limpos = arquivos_limpos
print(f"\nDados limpos dispon√≠veis na vari√°vel 'arquivos_filtrados_limpos'")

"""##4- Unifica os datasets de pesquisa em um, e os de alunos em um, mostrando as colunas resultantes e os stats de cada coluna"""

# CONSOLIDA√á√ÉO DE DATASETS - PESQUISA E VENDAS
print("CONSOLIDA√á√ÉO DE DATASETS - PESQUISA E VENDAS")
print("=" * 45)

def consolidar_datasets():
   """Consolida arquivos de pesquisa e vendas em datasets √∫nicos"""

   # Identificar e consolidar datasets de pesquisa
   dados_pesquisa = []
   dados_vendas = []

   for arquivo, abas_dict in arquivos_filtrados_limpos.items():
       for aba_nome, df in abas_dict.items():
           df_copia = df.copy()
           df_copia['arquivo_origem'] = arquivo
           df_copia['aba_origem'] = aba_nome

           # Classificar por tipo
           if any(termo in aba_nome.lower() for termo in ['pesquisa']):
               dados_pesquisa.append(df_copia)
           elif any(termo in aba_nome.lower() for termo in ['vendas', 'sheet1']):
               dados_vendas.append(df_copia)

   # Consolidar em DataFrames √∫nicos
   df_pesquisa_consolidado = pd.concat(dados_pesquisa, ignore_index=True) if dados_pesquisa else pd.DataFrame()
   df_vendas_consolidado = pd.concat(dados_vendas, ignore_index=True) if dados_vendas else pd.DataFrame()

   return df_pesquisa_consolidado, df_vendas_consolidado

def gerar_relatorio_colunas(df, nome_dataset):
   """Gera relat√≥rio detalhado das colunas de um dataset"""

   print(f"\n{nome_dataset.upper()} - {len(df)} registros")
   print("=" * 70)
   print(f"{'COLUNA':<35} {'√öNICOS':>10} {'% AUSENTES':>12} {'TOTAL':>10}")
   print("-" * 70)

   for col in df.columns:
       valores_unicos = df[col].nunique()
       valores_ausentes = df[col].isnull().sum()
       pct_ausentes = (valores_ausentes / len(df)) * 100 if len(df) > 0 else 0
       total_registros = len(df)

       print(f"{col[:34]:<35} {valores_unicos:>10,} {pct_ausentes:>11.1f}% {total_registros:>10,}")

# Executar consolida√ß√£o
df_pesquisa, df_vendas = consolidar_datasets()

# Gerar relat√≥rios
gerar_relatorio_colunas(df_pesquisa, "DATASET PESQUISA")
gerar_relatorio_colunas(df_vendas, "DATASET VENDAS")

print(f"\nRESUMO:")
print(f"Dataset Pesquisa: {len(df_pesquisa):,} registros, {len(df_pesquisa.columns)} colunas")
print(f"Dataset Vendas: {len(df_vendas):,} registros, {len(df_vendas.columns)} colunas")

# Disponibilizar datasets consolidados
dataset_pesquisa_final = df_pesquisa
dataset_vendas_final = df_vendas
print(f"\nDatasets consolidados dispon√≠veis nas vari√°veis:")
print(f"- dataset_pesquisa_final")
print(f"- dataset_vendas_final")

"""## 5- Unificar colunas duplicadas:
Pesquisas:
1. J√° investiu em algum curso online para aprender uma nova forma de ganhar dinheiro?
2. O que mais te chama aten√ß√£o na profiss√£o de Programador?

Vendas:
1. Ticket (R$) + valor produtos ‚Üí valor
2. Produto + nome produto ‚Üí produto
3. Cliente Nome + nome contato ‚Üí nome
4. Cliente Email + email contato ‚Üí email
5. Criado Em + data aprovacao ‚Üí data
6. utm_last_source + utm_source ‚Üí source
7. utm_last_medium + utm_medium ‚Üí medium
8. utm_last_campaign + utm_campaign ‚Üí campaign
9. utm_last_content + utm_content ‚Üí content
10. Telefone + telefone contato ‚Üí telefone


"""

# UNIFICA√á√ÉO DE COLUNAS DUPLICADAS
print("UNIFICA√á√ÉO DE COLUNAS DUPLICADAS")
print("=" * 32)

def identificar_colunas_duplicadas_pesquisa(df):
   """Identifica todas as colunas duplicadas no dataset de pesquisa"""

   colunas = df.columns.tolist()
   duplicadas = []

   # Verificar padr√µes de duplica√ß√£o
   for i, col1 in enumerate(colunas):
       for j, col2 in enumerate(colunas[i+1:], i+1):
           # Comparar in√≠cio das strings (truncadas podem ser iguais)
           if col1[:30] == col2[:30] and col1 != col2:
               duplicadas.append((col1, col2))

   return duplicadas

def unificar_colunas_datasets():
   """Unifica colunas duplicadas nos datasets"""

   # DATASET PESQUISA
   df_pesquisa_unificado = dataset_pesquisa_final.copy()

   print("PESQUISA - Colunas duplicadas identificadas:")
   duplicadas_pesquisa = identificar_colunas_duplicadas_pesquisa(df_pesquisa_unificado)

   for col1, col2 in duplicadas_pesquisa:
       print(f"  {col1}")
       print(f"  {col2}")
       print()

   # Unificar colunas duplicadas de pesquisa
   colunas_investiu = [
       'J√° investiu em algum curso online para aprender uma nova forma de ganhar dinheiro?',
       'J√° investiu em algum curso online para aprender uma nova forma de ganhar dinheiro? '
   ]

   if all(col in df_pesquisa_unificado.columns for col in colunas_investiu):
       for i, row_idx in enumerate(df_pesquisa_unificado.index):
           valor_final = None
           for col in colunas_investiu:
               valor = df_pesquisa_unificado.loc[row_idx, col]
               if pd.notna(valor) and valor_final is None:
                   valor_final = valor
           df_pesquisa_unificado.loc[row_idx, 'investiu_curso_online'] = valor_final

       df_pesquisa_unificado = df_pesquisa_unificado.drop(columns=colunas_investiu)

   colunas_atencao = [
       'O que mais te chama aten√ß√£o na profiss√£o de Programador?',
       'O que mais te chama aten√ß√£o na profiss√£o de Programador? '
   ]

   if all(col in df_pesquisa_unificado.columns for col in colunas_atencao):
       for i, row_idx in enumerate(df_pesquisa_unificado.index):
           valor_final = None
           for col in colunas_atencao:
               valor = df_pesquisa_unificado.loc[row_idx, col]
               if pd.notna(valor) and valor_final is None:
                   valor_final = valor
           df_pesquisa_unificado.loc[row_idx, 'interesse_programacao'] = valor_final

       df_pesquisa_unificado = df_pesquisa_unificado.drop(columns=colunas_atencao)

   # DATASET VENDAS
   df_vendas_unificado = dataset_vendas_final.copy()

   print("VENDAS - Unificando colunas:")

   # Unificar valor
   if 'Ticket (R$)' in df_vendas_unificado.columns and 'valor produtos' in df_vendas_unificado.columns:
       df_vendas_unificado['valor'] = df_vendas_unificado['Ticket (R$)'].fillna(df_vendas_unificado['valor produtos'])
       df_vendas_unificado = df_vendas_unificado.drop(columns=['Ticket (R$)', 'valor produtos'])
       print("  Ticket (R$) + valor produtos ‚Üí valor")

   # Unificar produto
   if 'Produto' in df_vendas_unificado.columns and 'nome produto' in df_vendas_unificado.columns:
       df_vendas_unificado['produto'] = df_vendas_unificado['Produto'].fillna(df_vendas_unificado['nome produto'])
       df_vendas_unificado = df_vendas_unificado.drop(columns=['Produto', 'nome produto'])
       print("  Produto + nome produto ‚Üí produto")

   # Unificar nome
   if 'Cliente Nome' in df_vendas_unificado.columns and 'nome contato' in df_vendas_unificado.columns:
       df_vendas_unificado['nome'] = df_vendas_unificado['Cliente Nome'].fillna(df_vendas_unificado['nome contato'])
       df_vendas_unificado = df_vendas_unificado.drop(columns=['Cliente Nome', 'nome contato'])
       print("  Cliente Nome + nome contato ‚Üí nome")

   # Unificar email
   if 'Cliente Email' in df_vendas_unificado.columns and 'email contato' in df_vendas_unificado.columns:
       df_vendas_unificado['email'] = df_vendas_unificado['Cliente Email'].fillna(df_vendas_unificado['email contato'])
       df_vendas_unificado = df_vendas_unificado.drop(columns=['Cliente Email', 'email contato'])
       print("  Cliente Email + email contato ‚Üí email")

   # Unificar data
   if 'Criado Em' in df_vendas_unificado.columns and 'data aprovacao' in df_vendas_unificado.columns:
       df_vendas_unificado['data'] = df_vendas_unificado['Criado Em'].fillna(df_vendas_unificado['data aprovacao'])
       df_vendas_unificado = df_vendas_unificado.drop(columns=['Criado Em', 'data aprovacao'])
       print("  Criado Em + data aprovacao ‚Üí data")

   # Unificar telefone
   if 'Telefone' in df_vendas_unificado.columns and 'telefone contato' in df_vendas_unificado.columns:
       df_vendas_unificado['telefone'] = df_vendas_unificado['Telefone'].fillna(df_vendas_unificado['telefone contato'])
       df_vendas_unificado = df_vendas_unificado.drop(columns=['Telefone', 'telefone contato'])
       print("  Telefone + telefone contato ‚Üí telefone")

   # Unificar UTMs (manter as vers√µes 'last' quando dispon√≠veis)
   utms_map = [
       ('utm_last_source', 'utm_source', 'source'),
       ('utm_last_medium', 'utm_medium', 'medium'),
       ('utm_last_campaign', 'utm_campaign', 'campaign'),
       ('utm_last_content', 'utm_content', 'content')
   ]

   for utm_last, utm_regular, utm_final in utms_map:
       if utm_last in df_vendas_unificado.columns and utm_regular in df_vendas_unificado.columns:
           df_vendas_unificado[utm_final] = df_vendas_unificado[utm_last].fillna(df_vendas_unificado[utm_regular])
           df_vendas_unificado = df_vendas_unificado.drop(columns=[utm_last, utm_regular])
           print(f"  {utm_last} + {utm_regular} ‚Üí {utm_final}")

   # Remover colunas UTM unificadas com alta porcentagem de ausentes
   print("\nVENDAS - Removendo colunas UTM com alta porcentagem de ausentes:")
   colunas_utm_remover = ['source', 'medium', 'campaign', 'content']
   colunas_existentes_utm = [col for col in colunas_utm_remover if col in df_vendas_unificado.columns]

   if colunas_existentes_utm:
       df_vendas_unificado = df_vendas_unificado.drop(columns=colunas_existentes_utm)
       for col in colunas_existentes_utm:
           print(f"  Removida: {col}")

   return df_pesquisa_unificado, df_vendas_unificado

# Executar unifica√ß√£o
df_pesquisa_final, df_vendas_final = unificar_colunas_datasets()

print(f"\nRESULTADO:")
print(f"Pesquisa: {len(df_pesquisa_final)} registros, {len(df_pesquisa_final.columns)} colunas")
print(f"Vendas: {len(df_vendas_final)} registros, {len(df_vendas_final.columns)} colunas")

# Disponibilizar datasets finais
pesquisa_unificado = df_pesquisa_final
vendas_unificado = df_vendas_final

gerar_relatorio_colunas(df_pesquisa_final, "DATASET PESQUISA")
gerar_relatorio_colunas(df_vendas_final, "DATASET VENDAS")

"""## 6-  Investiga√ß√£o
As colunas ‚Äúqual √© o seu n√≠vel de programa√ß√£o‚Äù e ‚Äúvoc√™ j√° estudou programa√ß√£o‚Äù s√£o unific√°veis.

Resultado: Refutada.
"""

# AN√ÅLISE TEMPORAL DAS PERGUNTAS SOBRE PROGRAMA√á√ÉO
print("AN√ÅLISE TEMPORAL DAS PERGUNTAS SOBRE PROGRAMA√á√ÉO")
print("=" * 55)

# Identificar colunas relacionadas a programa√ß√£o
colunas_programacao = []
for col in pesquisa_unificado.columns:
   if any(termo in col.lower() for termo in ['n√≠vel em programa√ß√£o', 'estudou programa√ß√£o']):
       colunas_programacao.append(col)

# Verificar se existe coluna de data
coluna_data = None
for col in pesquisa_unificado.columns:
   if 'data' in col.lower():
       coluna_data = col
       break

if colunas_programacao and coluna_data:
   for coluna in colunas_programacao:
       print(f"\nCOLUNA: {coluna}")
       print("-" * 60)

       # Filtrar apenas registros com valores v√°lidos (n√£o nulos)
       dados_validos = pesquisa_unificado[pesquisa_unificado[coluna].notna()]

       if len(dados_validos) > 0:
           # Converter coluna de data para datetime
           dados_validos_copy = dados_validos.copy()
           dados_validos_copy[coluna_data] = pd.to_datetime(dados_validos_copy[coluna_data], errors='coerce')
           dados_validos_copy = dados_validos_copy[dados_validos_copy[coluna_data].notna()]

           if len(dados_validos_copy) > 0:
               data_inicio = dados_validos_copy[coluna_data].min()
               data_fim = dados_validos_copy[coluna_data].max()

               print(f"Registros com valores v√°lidos: {len(dados_validos):,}")
               print(f"Data de in√≠cio: {data_inicio.strftime('%Y-%m-%d')}")
               print(f"Data de fim: {data_fim.strftime('%Y-%m-%d')}")
               print(f"Per√≠odo: {(data_fim - data_inicio).days} dias")

               # Mostrar distribui√ß√£o por m√™s
               dados_validos_copy['ano_mes'] = dados_validos_copy[coluna_data].dt.to_period('M')
               distribuicao_mensal = dados_validos_copy['ano_mes'].value_counts().sort_index()

               print(f"\nDistribui√ß√£o mensal (primeiros 5 meses):")
               for periodo, count in distribuicao_mensal.head().items():
                   print(f"  {periodo}: {count:,} registros")

               print(f"\nDistribui√ß√£o mensal (√∫ltimos 5 meses):")
               for periodo, count in distribuicao_mensal.tail().items():
                   print(f"  {periodo}: {count:,} registros")
           else:
               print("Nenhuma data v√°lida encontrada")
       else:
           print("Nenhum valor v√°lido encontrado")

       print("=" * 60)

   # An√°lise de sobreposi√ß√£o temporal
   if len(colunas_programacao) == 2:
       print(f"\nAN√ÅLISE DE SOBREPOSI√á√ÉO TEMPORAL")
       print("-" * 40)

       col1, col2 = colunas_programacao

       # Registros com ambas as colunas preenchidas
       ambas_preenchidas = pesquisa_unificado[
           pesquisa_unificado[col1].notna() & pesquisa_unificado[col2].notna()
       ]

       # Registros com apenas uma coluna preenchida
       apenas_col1 = pesquisa_unificado[
           pesquisa_unificado[col1].notna() & pesquisa_unificado[col2].isna()
       ]

       apenas_col2 = pesquisa_unificado[
           pesquisa_unificado[col1].isna() & pesquisa_unificado[col2].notna()
       ]

       print(f"Registros com ambas preenchidas: {len(ambas_preenchidas):,}")
       print(f"Registros apenas com '{col1}': {len(apenas_col1):,}")
       print(f"Registros apenas com '{col2}': {len(apenas_col2):,}")

       total_preenchidos = len(ambas_preenchidas) + len(apenas_col1) + len(apenas_col2)
       cobertura = (total_preenchidos / len(pesquisa_unificado)) * 100
       print(f"Cobertura combinada: {cobertura:.1f}%")

else:
   print("Colunas necess√°rias n√£o encontradas")
   print(f"Colunas de programa√ß√£o: {colunas_programacao}")
   print(f"Coluna de data: {coluna_data}")

# VALORES √öNICOS DAS PERGUNTAS SOBRE PROGRAMA√á√ÉO
print("VALORES √öNICOS DAS PERGUNTAS SOBRE PROGRAMA√á√ÉO")
print("=" * 50)

# Identificar colunas relacionadas a programa√ß√£o
colunas_programacao = []
for col in pesquisa_unificado.columns:
   if any(termo in col.lower() for termo in ['n√≠vel em programa√ß√£o', 'estudou programa√ß√£o']):
       colunas_programacao.append(col)

if colunas_programacao:
   for coluna in colunas_programacao:
       print(f"\nCOLUNA: {coluna}")
       print("-" * 60)
       print(f"Total de registros: {len(pesquisa_unificado):,}")

       valores_unicos = pesquisa_unificado[coluna].value_counts(dropna=False)

       print(f"\nVALORES √öNICOS:")
       for valor, count in valores_unicos.items():
           porcentagem = (count / len(pesquisa_unificado)) * 100
           print(f"{str(valor):<35} {count:>6,} ({porcentagem:>5.1f}%)")

       print(f"\nRESUMO:")
       print(f"Valores √∫nicos: {pesquisa_unificado[coluna].nunique()}")
       print(f"Valores ausentes: {pesquisa_unificado[coluna].isnull().sum():,}")
       print(f"% de ausentes: {(pesquisa_unificado[coluna].isnull().sum() / len(pesquisa_unificado)) * 100:.1f}%")
       print("=" * 60)

else:
   print("Nenhuma coluna relacionada a programa√ß√£o encontrada.")
   print("Colunas dispon√≠veis:")
   for col in pesquisa_unificado.columns:
       if 'program' in col.lower():
           print(f"  - {col}")

# SIMULA√á√ÉO DA UNIFICA√á√ÉO DAS PERGUNTAS SOBRE PROGRAMA√á√ÉO
print("SIMULA√á√ÉO DA UNIFICA√á√ÉO DAS PERGUNTAS SOBRE PROGRAMA√á√ÉO")
print("=" * 60)

# Valores originais da pergunta detalhada
valores_detalhados = {
   'Nunca estudei programa√ß√£o.': 16339,
   'J√° ouvi falar, mas nunca pratiquei.': 6204,
   'At√© fiz alguns cursos, mas ainda me sinto perdido.': 3613,
   'J√° estudo programa√ß√£o e busco uma oportunidade no mercado.': 716,
   'J√° trabalho na √°rea de programa√ß√£o.': 82
}

# Valores da pergunta bin√°ria
valores_binarios = {
   'N√£o': 32952,
   'Sim': 15188
}

total_registros = 75110

print("MAPEAMENTO PROPOSTO:")
print("-" * 30)
print("'Nunca estudei programa√ß√£o.' ‚Üí 'N√£o'")
print("'J√° ouvi falar, mas nunca pratiquei.' ‚Üí 'N√£o'")
print("'At√© fiz alguns cursos...' ‚Üí 'Sim'")
print("'J√° estudo programa√ß√£o...' ‚Üí 'Sim'")
print("'J√° trabalho na √°rea...' ‚Üí 'Sim'")

print(f"\nRESULTADO DA UNIFICA√á√ÉO:")
print("-" * 30)

# Calcular unifica√ß√£o
nao_unificado = valores_detalhados['Nunca estudei programa√ß√£o.'] + valores_detalhados['J√° ouvi falar, mas nunca pratiquei.']
sim_unificado = (valores_detalhados['At√© fiz alguns cursos, mas ainda me sinto perdido.'] +
               valores_detalhados['J√° estudo programa√ß√£o e busco uma oportunidade no mercado.'] +
               valores_detalhados['J√° trabalho na √°rea de programa√ß√£o.'])

print(f"'N√£o' unificado: {nao_unificado:,} registros ({(nao_unificado/total_registros)*100:.1f}%)")
print(f"'Sim' unificado: {sim_unificado:,} registros ({(sim_unificado/total_registros)*100:.1f}%)")

print(f"\nCOMPARA√á√ÉO COM PERGUNTA BIN√ÅRIA ORIGINAL:")
print("-" * 45)
print(f"Pergunta bin√°ria 'N√£o': {valores_binarios['N√£o']:,} ({(valores_binarios['N√£o']/total_registros)*100:.1f}%)")
print(f"Unifica√ß√£o 'N√£o': {nao_unificado:,} ({(nao_unificado/total_registros)*100:.1f}%)")
print(f"Diferen√ßa: {abs(valores_binarios['N√£o'] - nao_unificado):,} registros")

print(f"\nPergunta bin√°ria 'Sim': {valores_binarios['Sim']:,} ({(valores_binarios['Sim']/total_registros)*100:.1f}%)")
print(f"Unifica√ß√£o 'Sim': {sim_unificado:,} ({(sim_unificado/total_registros)*100:.1f}%)")
print(f"Diferen√ßa: {abs(valores_binarios['Sim'] - sim_unificado):,} registros")

print(f"\nCONCLUS√ÉO:")
print("-" * 15)
cobertura_total = nao_unificado + sim_unificado + valores_binarios['N√£o'] + valores_binarios['Sim']
print(f"Cobertura total ap√≥s unifica√ß√£o: {cobertura_total:,} registros ({(cobertura_total/total_registros)*100:.1f}%)")

# Verificar compatibilidade das distribui√ß√µes
diff_nao = abs(valores_binarios['N√£o'] - nao_unificado)
diff_sim = abs(valores_binarios['Sim'] - sim_unificado)
compatibilidade = (1 - (diff_nao + diff_sim) / (valores_binarios['N√£o'] + valores_binarios['Sim'])) * 100

print(f"Compatibilidade entre distribui√ß√µes: {compatibilidade:.1f}%")

if compatibilidade > 80:
   print("‚úÖ Unifica√ß√£o vi√°vel - distribui√ß√µes compat√≠veis")
else:
   print("‚ö†Ô∏è Unifica√ß√£o pode introduzir vi√©s - distribui√ß√µes divergentes")

"""## 6.1- Investiga√ß√£o:

Para colunas nome, telefone e e-mail:
1. M√≠nimo de caracteres
2. M√°ximo de caracteres
3. M√©dia de caracteres
4. Outliers (preenchimentos com pouqu√≠ssimos ou muit√≠ssimos caracteres)
"""

# AN√ÅLISE DE QUALIDADE DOS DADOS - IDENTIFICADORES
print("AN√ÅLISE DE QUALIDADE DOS DADOS - IDENTIFICADORES")
print("=" * 50)

def analisar_identificadores():
   """Analisa qualidade dos campos nome, telefone e email"""

   # Datasets para analisar
   datasets = {
       'Pesquisa': pesquisa_unificado,
       'Vendas': vendas_unificado
   }

   for dataset_nome, df in datasets.items():
       print(f"\nDATASET: {dataset_nome}")
       print("-" * 30)

       # Identificar colunas de identificadores
       colunas_identificadores = []
       for col in df.columns:
           if any(termo in col.lower() for termo in ['nome', 'telefone', 'email', 'e-mail']):
               colunas_identificadores.append(col)

       for coluna in colunas_identificadores:
           print(f"\nColuna: {coluna}")

           # Filtrar apenas valores n√£o nulos
           dados_validos = df[df[coluna].notna()][coluna].astype(str)

           if len(dados_validos) == 0:
               print("  Nenhum dado v√°lido encontrado")
               continue

           # Calcular comprimentos
           comprimentos = dados_validos.str.len()

           min_chars = comprimentos.min()
           max_chars = comprimentos.max()
           media_chars = comprimentos.mean()

           print(f"  Registros v√°lidos: {len(dados_validos):,}")
           print(f"  M√≠nimo de caracteres: {min_chars}")
           print(f"  M√°ximo de caracteres: {max_chars}")
           print(f"  M√©dia de caracteres: {media_chars:.1f}")

           # Identificar outliers usando quartis
           q1 = comprimentos.quantile(0.25)
           q3 = comprimentos.quantile(0.75)
           iqr = q3 - q1
           limite_inferior = q1 - 1.5 * iqr
           limite_superior = q3 + 1.5 * iqr

           outliers_baixos = dados_validos[comprimentos < limite_inferior]
           outliers_altos = dados_validos[comprimentos > limite_superior]

           print(f"  Outliers baixos (< {limite_inferior:.1f} chars): {len(outliers_baixos)}")
           if len(outliers_baixos) > 0:
               print(f"    Exemplos: {list(outliers_baixos.head(3))}")

           print(f"  Outliers altos (> {limite_superior:.1f} chars): {len(outliers_altos)}")
           if len(outliers_altos) > 0:
               print(f"    Exemplos: {list(outliers_altos.head(3))}")

           # Identificar registros extremamente curtos ou longos
           muito_curtos = dados_validos[comprimentos <= 2]
           muito_longos = dados_validos[comprimentos >= 100]

           if len(muito_curtos) > 0:
               print(f"  Registros muito curtos (‚â§2 chars): {len(muito_curtos)}")
               print(f"    Exemplos: {list(muito_curtos.head(5))}")

           if len(muito_longos) > 0:
               print(f"  Registros muito longos (‚â•100 chars): {len(muito_longos)}")
               print(f"    Exemplos: {list(muito_longos.head(3))}")

# Executar an√°lise
analisar_identificadores()

"""## 6.2 Investiga√ß√£o:
Investigar para cada categ√≥rica que n√£o seja identificadores (excluindo identificadores, data, aba origem e arquivo origem):
1. Valores √∫nicos
2. Nulos
3. Distribui√ß√£o de categorias imprimindo somente as top 10 caso haja mais de 10 categorias, e um gr√°fico de barras com a frequ√™ncia.
"""

# AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS
print("AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS")
print("=" * 35)

import matplotlib.pyplot as plt

def analisar_categoricas():
   """Analisa vari√°veis categ√≥ricas dos datasets"""

   datasets = {
       'Pesquisa': pesquisa_unificado,
       'Vendas': vendas_unificado
   }

   # Colunas a excluir da an√°lise
   excluir = ['nome', 'telefone', 'email', 'e-mail', 'data', 'aba_origem', 'arquivo_origem']

   for dataset_nome, df in datasets.items():
       print(f"\nDATASET: {dataset_nome}")
       print("=" * 40)

       # Identificar colunas categ√≥ricas
       colunas_categoricas = []
       for col in df.columns:
           # Excluir identificadores, datas e metadados
           if not any(termo in col.lower() for termo in excluir):
               # Incluir se for object/string ou se tiver poucos valores √∫nicos
               if df[col].dtype == 'object' or df[col].nunique() <= 20:
                   colunas_categoricas.append(col)

       print(f"Colunas categ√≥ricas encontradas: {len(colunas_categoricas)}")

       for coluna in colunas_categoricas:
           print(f"\nCOLUNA: {coluna}")
           print("-" * 50)

           # 1. Valores √∫nicos
           valores_unicos = df[coluna].nunique()
           print(f"Valores √∫nicos: {valores_unicos}")

           # 2. Nulos
           nulos = df[coluna].isnull().sum()
           pct_nulos = (nulos / len(df)) * 100
           print(f"Valores nulos: {nulos:,} ({pct_nulos:.1f}%)")

           # 3. Distribui√ß√£o de categorias
           distribuicao = df[coluna].value_counts(dropna=False)

           if valores_unicos > 10:
               print(f"Distribui√ß√£o (Top 10):")
               top_10 = distribuicao.head(10)
               for valor, count in top_10.items():
                   pct = (count / len(df)) * 100
                   print(f"  {str(valor)[:30]:<32} {count:>6,} ({pct:>5.1f}%)")

               # Gr√°fico apenas com Top 10
               plt.figure(figsize=(10, 6))
               plt.bar(range(len(top_10)), top_10.values)
               plt.title(f'{dataset_nome} - {coluna} (Top 10)')
               plt.xlabel('Categorias')
               plt.ylabel('Frequ√™ncia')
               plt.xticks(range(len(top_10)), [str(x)[:15] for x in top_10.index], rotation=45)
               plt.tight_layout()
               plt.show()

           else:
               print(f"Distribui√ß√£o (Todas as categorias):")
               for valor, count in distribuicao.items():
                   pct = (count / len(df)) * 100
                   print(f"  {str(valor)[:30]:<32} {count:>6,} ({pct:>5.1f}%)")

               # Gr√°fico com todas as categorias
               plt.figure(figsize=(10, 6))
               plt.bar(range(len(distribuicao)), distribuicao.values)
               plt.title(f'{dataset_nome} - {coluna}')
               plt.xlabel('Categorias')
               plt.ylabel('Frequ√™ncia')
               plt.xticks(range(len(distribuicao)), [str(x)[:15] for x in distribuicao.index], rotation=45)
               plt.tight_layout()
               plt.show()

# Executar an√°lise
analisar_categoricas()

"""## 6.3- Investiga√ß√£o
Para coluna de data:
1. Data de in√≠cio e do fim
2. Formato da data
"""

# AN√ÅLISE DE QUALIDADE DOS DADOS - DATAS
print("AN√ÅLISE DE QUALIDADE DOS DADOS - DATAS")
print("=" * 42)

def analisar_datas():
    """Analisa qualidade dos campos de data"""

    # Datasets para analisar
    datasets = {
        'Pesquisa': pesquisa_unificado,
        'Vendas': vendas_unificado
    }

    for dataset_nome, df in datasets.items():
        print(f"\nDATASET: {dataset_nome}")
        print("-" * 30)

        # Identificar colunas de data
        colunas_data = []
        for col in df.columns:
            if any(termo in col.lower() for termo in ['data', 'date', 'criado', 'created']):
                colunas_data.append(col)

        if len(colunas_data) == 0:
            print(" Nenhuma coluna de data encontrada")
            continue

        for coluna in colunas_data:
            print(f"\nColuna: {coluna}")

            # Filtrar apenas valores n√£o nulos
            dados_validos = df[df[coluna].notna()][coluna]

            if len(dados_validos) == 0:
                print(" Nenhum dado v√°lido encontrado")
                continue

            print(f" Registros v√°lidos: {len(dados_validos):,}")

            # 1. Data de in√≠cio e fim
            try:
                # Tentar converter para datetime se ainda n√£o for
                if dados_validos.dtype == 'object':
                    datas_convertidas = pd.to_datetime(dados_validos, errors='coerce')
                else:
                    datas_convertidas = dados_validos

                # Remover valores que n√£o puderam ser convertidos
                datas_validas = datas_convertidas.dropna()

                if len(datas_validas) > 0:
                    data_inicio = datas_validas.min()
                    data_fim = datas_validas.max()

                    print(f" Data de in√≠cio: {data_inicio}")
                    print(f" Data de fim: {data_fim}")
                    print(f" Per√≠odo: {(data_fim - data_inicio).days} dias")
                else:
                    print(" N√£o foi poss√≠vel converter nenhuma data")

            except Exception as e:
                print(f" Erro ao processar datas: {e}")

            # 2. Formato da data
            print(f" Tipo de dados: {dados_validos.dtype}")

            # Analisar formatos se for string
            if dados_validos.dtype == 'object':
                # Amostrar alguns valores para an√°lise de formato
                amostras = dados_validos.head(10).tolist()
                print(f" Amostras de formato:")
                for i, amostra in enumerate(amostras):
                    print(f"  {i+1}. {amostra}")

                # Analisar padr√µes de formato
                formatos_detectados = set()
                for valor in dados_validos.head(100):  # Analisar primeiros 100
                    valor_str = str(valor)

                    # Detectar padr√µes comuns
                    if '/' in valor_str:
                        partes = valor_str.split('/')
                        if len(partes) == 3:
                            formatos_detectados.add('DD/MM/YYYY ou MM/DD/YYYY')
                    elif '-' in valor_str:
                        partes = valor_str.split('-')
                        if len(partes) == 3:
                            formatos_detectados.add('YYYY-MM-DD ou DD-MM-YYYY')
                    elif ' ' in valor_str:
                        formatos_detectados.add('Data com hor√°rio')

                if formatos_detectados:
                    print(f" Formatos detectados: {', '.join(formatos_detectados)}")
                else:
                    print(" Formato n√£o identificado automaticamente")

# Executar an√°lise
analisar_datas()

"""## 6.4 Investiga√ß√£o
Quantidade de cada produto vendido com o valor, e quantidade de cada produto vendido por m√™s desde o in√≠cio do dataset de leads.
"""

# AN√ÅLISE DE PRODUTOS E VENDAS
print("AN√ÅLISE DE PRODUTOS E VENDAS")
print("=" * 32)

import matplotlib.pyplot as plt
import pandas as pd

def analisar_produtos_vendas():
    """Analisa produtos vendidos com valores e evolu√ß√£o temporal"""

    df = vendas_unificado.copy()

    # Verificar se existem as colunas necess√°rias
    if 'produto' not in df.columns or 'valor' not in df.columns:
        print("Colunas 'produto' ou 'valor' n√£o encontradas no dataset")
        return

    print("\n1. TABELA DE PRODUTOS COM VALORES")
    print("=" * 50)

    # An√°lise por produto
    analise_produto = df.groupby('produto').agg({
        'valor': ['count', 'sum', 'mean'],
        'produto': 'size'
    }).round(2)

    # Simplificar colunas
    analise_produto.columns = ['quantidade', 'valor_total', 'valor_medio', 'registros']
    analise_produto = analise_produto[['quantidade', 'valor_total', 'valor_medio']]

    # Ordenar por valor total decrescente
    analise_produto = analise_produto.sort_values('valor_total', ascending=False)

    # Calcular percentuais
    total_vendas = analise_produto['quantidade'].sum()
    total_receita = analise_produto['valor_total'].sum()

    analise_produto['pct_vendas'] = (analise_produto['quantidade'] / total_vendas * 100).round(1)
    analise_produto['pct_receita'] = (analise_produto['valor_total'] / total_receita * 100).round(1)

    # Exibir tabela formatada
    print(f"{'PRODUTO':<35} {'QTD':<6} {'%VND':<5} {'VALOR TOTAL':<12} {'%REC':<5} {'VALOR M√âDIO':<11}")
    print("-" * 80)

    for produto, row in analise_produto.iterrows():
        produto_truncado = produto[:33] if len(produto) > 33 else produto
        print(f"{produto_truncado:<35} {row['quantidade']:<6,.0f} {row['pct_vendas']:<5.1f} R$ {row['valor_total']:<9,.0f} {row['pct_receita']:<5.1f} R$ {row['valor_medio']:<8,.0f}")

    print("-" * 80)
    print(f"{'TOTAL':<35} {total_vendas:<6,.0f} {'100.0':<5} R$ {total_receita:<9,.0f} {'100.0':<5}")

    # Verificar se existe coluna de data
    colunas_data = [col for col in df.columns if any(termo in col.lower() for termo in ['data', 'date', 'criado', 'created'])]

    if len(colunas_data) == 0:
        print("\nNenhuma coluna de data encontrada para an√°lise temporal")
        return

    coluna_data = colunas_data[0]
    print(f"\nUsando coluna de data: {coluna_data}")

    # Converter data para datetime
    df[coluna_data] = pd.to_datetime(df[coluna_data], errors='coerce')
    df_com_data = df[df[coluna_data].notna()].copy()

    if len(df_com_data) == 0:
        print("Nenhuma data v√°lida encontrada")
        return

    # Criar coluna m√™s-ano
    df_com_data['mes_ano'] = df_com_data[coluna_data].dt.to_period('M')

    print(f"\n2. EVOLU√á√ÉO MENSAL DE VENDAS POR PRODUTO")
    print("=" * 50)

    # An√°lise temporal por produto
    vendas_mensais = df_com_data.groupby(['mes_ano', 'produto']).agg({
        'valor': ['count', 'sum']
    }).reset_index()

    # Simplificar colunas
    vendas_mensais.columns = ['mes_ano', 'produto', 'quantidade', 'valor_total']

    # Criar tabela pivotada para visualiza√ß√£o
    pivot_quantidade = vendas_mensais.pivot(index='mes_ano', columns='produto', values='quantidade').fillna(0)
    pivot_valor = vendas_mensais.pivot(index='mes_ano', columns='produto', values='valor_total').fillna(0)

    # Ordenar produtos por volume total
    ordem_produtos = analise_produto.head(10).index.tolist()  # Top 10 produtos

    # Filtrar apenas os produtos principais
    produtos_principais = [p for p in ordem_produtos if p in pivot_quantidade.columns]

    if len(produtos_principais) > 0:
        pivot_quantidade_top = pivot_quantidade[produtos_principais]
        pivot_valor_top = pivot_valor[produtos_principais]

        print("\nQUANTIDADE VENDIDA POR M√äS (Top 10 produtos):")
        print("-" * 80)

        # Exibir tabela de quantidade de forma leg√≠vel
        meses = pivot_quantidade_top.index[-12:]  # √öltimos 12 meses
        dados_recentes = pivot_quantidade_top.loc[meses]

        # Cabe√ßalho
        produtos_curtos = [p[:12] for p in produtos_principais[:6]]  # M√°ximo 6 produtos para legibilidade
        print(f"{'M√äS':<8} " + " ".join(f"{p:<12}" for p in produtos_curtos))
        print("-" * 80)

        for mes in dados_recentes.index:
            linha = f"{str(mes):<8} "
            for produto in produtos_principais[:6]:
                qtd = int(dados_recentes.loc[mes, produto])
                linha += f"{qtd:<12}"
            print(linha)

    # Gr√°fico de evolu√ß√£o temporal
    plt.figure(figsize=(15, 8))

    # Subplot 1: Quantidade
    plt.subplot(2, 1, 1)
    for produto in produtos_principais[:5]:  # Top 5 para visualiza√ß√£o
        plt.plot(pivot_quantidade_top.index.astype(str), pivot_quantidade_top[produto],
                marker='o', label=produto[:20], linewidth=2)

    plt.title('Evolu√ß√£o Mensal - Quantidade de Vendas (Top 5 Produtos)')
    plt.xlabel('M√™s')
    plt.ylabel('Quantidade')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    # Subplot 2: Valor
    plt.subplot(2, 1, 2)
    for produto in produtos_principais[:5]:  # Top 5 para visualiza√ß√£o
        plt.plot(pivot_valor_top.index.astype(str), pivot_valor_top[produto],
                marker='s', label=produto[:20], linewidth=2)

    plt.title('Evolu√ß√£o Mensal - Valor de Vendas (Top 5 Produtos)')
    plt.xlabel('M√™s')
    plt.ylabel('Valor (R$)')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Executar an√°lise
analisar_produtos_vendas()

"""## 6.5 - Investiga√ß√£o
Data a partir de qual cada feature ausente come√ßa a ter dados.
Objetivo: identificar se, a partir de determinada data, o dataset tem todas as features dispon√≠veis.
"""

# AN√ÅLISE DE EVOLU√á√ÉO DE FEATURES NO TEMPO
print("AN√ÅLISE DE EVOLU√á√ÉO DE FEATURES NO TEMPO")
print("=" * 44)

import pandas as pd

def analisar_evolucao_features():
    """Analisa quando cada feature come√ßou e parou de ter respostas"""

    df = pesquisa_unificado.copy()

    # Features de interesse
    features_interesse = [
        'J√° estudou programa√ß√£o?',
        'Voc√™ j√° fez/faz/pretende fazer faculdade?',
        'Tem computador/notebook?',
        'Qual o seu n√≠vel em programa√ß√£o?',
        "Voc√™ possui cart√£o de cr√©dito?",
        "O que mais voc√™ quer ver no evento?"
    ]

    # DEBUG: Listar todas as colunas para encontrar a feature faltante
    print(f"\nDEBUG - TODAS AS COLUNAS DO DATASET:")
    print("-" * 50)
    for i, coluna in enumerate(df.columns, 1):
        print(f"{i:2d}. {coluna}")

    # DEBUG: Buscar colunas que contenham palavras relacionadas √† feature faltante
    print(f"\nDEBUG - COLUNAS COM 'FAC', 'FACUL' OU 'FAZER':")
    print("-" * 50)
    colunas_relacionadas = []
    for coluna in df.columns:
        if any(termo in coluna.lower() for termo in ['fac', 'facul', 'fazer', 'pretende']):
            colunas_relacionadas.append(coluna)
            print(f"  {coluna}")

    if len(colunas_relacionadas) == 0:
        print("  Nenhuma coluna encontrada com esses termos")

    print()

    # Identificar coluna de data
    colunas_data = [col for col in df.columns if any(termo in col.lower() for termo in ['data', 'date'])]

    if len(colunas_data) == 0:
        print("Nenhuma coluna de data encontrada")
        return

    coluna_data = colunas_data[0]
    print(f"Usando coluna de data: {coluna_data}")

    # Converter data para datetime
    df[coluna_data] = pd.to_datetime(df[coluna_data], errors='coerce')
    df_com_data = df[df[coluna_data].notna()].copy()

    if len(df_com_data) == 0:
        print("Nenhuma data v√°lida encontrada")
        return

    print(f"\nPER√çODO TOTAL DO DATASET:")
    print(f"Data mais antiga: {df_com_data[coluna_data].min()}")
    print(f"Data mais recente: {df_com_data[coluna_data].max()}")
    print(f"Total de registros com data: {len(df_com_data):,}")

    print(f"\nAN√ÅLISE POR FEATURE:")
    print("=" * 80)
    print(f"{'FEATURE':<35} {'PRIMEIRA':<12} {'√öLTIMA':<12} {'TOTAL':<8} {'%AUSENTES':<10}")
    print("-" * 80)

    for feature in features_interesse:
        if feature in df_com_data.columns:
            # Filtrar apenas registros com respostas v√°lidas para esta feature
            dados_validos = df_com_data[df_com_data[feature].notna()]

            if len(dados_validos) > 0:
                primeira_data = dados_validos[coluna_data].min()
                ultima_data = dados_validos[coluna_data].max()
                total_respostas = len(dados_validos)

                # Calcular % ausentes
                total_registros = len(df_com_data)
                pct_ausentes = ((total_registros - total_respostas) / total_registros * 100)

                # Formata√ß√£o das datas
                primeira_str = primeira_data.strftime('%Y-%m-%d')
                ultima_str = ultima_data.strftime('%Y-%m-%d')

                feature_truncada = feature[:33] if len(feature) > 33 else feature

                print(f"{feature_truncada:<35} {primeira_str:<12} {ultima_str:<12} {total_respostas:<8,} {pct_ausentes:<10.1f}%")

                # An√°lise detalhada por feature
                print(f"\nDETALHES - {feature}:")
                print("-" * 50)

                # Distribui√ß√£o mensal de respostas
                dados_validos_copia = dados_validos.copy()
                dados_validos_copia['mes_ano'] = dados_validos_copia[coluna_data].dt.to_period('M')

                respostas_mensais = dados_validos_copia.groupby('mes_ano').size()

                print(f"Meses com respostas: {len(respostas_mensais)}")
                print(f"M√©dia de respostas por m√™s: {respostas_mensais.mean():.1f}")

                # Primeiros e √∫ltimos meses com mais respostas
                top_meses = respostas_mensais.head(5)
                print(f"Primeiros 5 meses:")
                for mes, qtd in top_meses.items():
                    print(f"  {mes}: {qtd:,} respostas")

                # Verificar se h√° gaps (per√≠odos sem respostas)
                periodo_completo = pd.period_range(
                    start=primeira_data.to_period('M'),
                    end=ultima_data.to_period('M'),
                    freq='M'
                )

                meses_sem_resposta = []
                for mes in periodo_completo:
                    if mes not in respostas_mensais.index:
                        meses_sem_resposta.append(mes)

                if len(meses_sem_resposta) > 0:
                    print(f"Meses sem respostas no per√≠odo: {len(meses_sem_resposta)}")
                    if len(meses_sem_resposta) <= 5:
                        for mes in meses_sem_resposta:
                            print(f"  {mes}")
                    else:
                        print(f"  Primeiros 5: {', '.join(str(m) for m in meses_sem_resposta[:5])}")

                print()

            else:
                feature_truncada = feature[:33] if len(feature) > 33 else feature
                print(f"{feature_truncada:<35} {'SEM DADOS':<12} {'SEM DADOS':<12} {'0':<8} {'100.0%':<10}")
        else:
            feature_truncada = feature[:33] if len(feature) > 33 else feature
            print(f"{feature_truncada:<35} {'N√ÉO ENCONTRADA':<25}")

    print("-" * 80)

    # An√°lise comparativa
    print(f"\nAN√ÅLISE COMPARATIVA:")
    print("=" * 30)

    features_encontradas = [f for f in features_interesse if f in df_com_data.columns]

    if len(features_encontradas) > 1:
        print("Ordem cronol√≥gica de introdu√ß√£o das features:")

        datas_inicio = {}
        for feature in features_encontradas:
            dados_validos = df_com_data[df_com_data[feature].notna()]
            if len(dados_validos) > 0:
                datas_inicio[feature] = dados_validos[coluna_data].min()

        # Ordenar por data de in√≠cio
        features_ordenadas = sorted(datas_inicio.items(), key=lambda x: x[1])

        for i, (feature, data_inicio) in enumerate(features_ordenadas, 1):
            print(f"{i}. {feature[:40]} - {data_inicio.strftime('%Y-%m-%d')}")

# Executar an√°lise
analisar_evolucao_features()

"""## 7- Unificar categorias duplicadas"""

# UNIFICA√á√ÉO COMPLETA DE CATEGORIAS - NOVO C√ìDIGO
print("UNIFICA√á√ÉO COMPLETA DE CATEGORIAS - NOVO C√ìDIGO")
print("=" * 52)

def limpar_texto(texto):
    """Limpa caracteres invis√≠veis e normaliza texto"""
    if pd.isna(texto):
        return texto

    # Converter para string e limpar caracteres invis√≠veis
    texto_limpo = str(texto)
    texto_limpo = texto_limpo.replace('\u2060', '')  # Word joiner
    texto_limpo = texto_limpo.replace('\xa0', ' ')   # Non-breaking space
    texto_limpo = texto_limpo.replace('\u200b', '')  # Zero width space
    texto_limpo = texto_limpo.strip()

    return texto_limpo

def unificar_categorias_completo():
    """Unifica categorias com limpeza e mappings robustos"""

    df = pesquisa_unificado.copy()

    print("Aplicando limpeza e unifica√ß√£o completa...")

    # 1. INTERESSE PROGRAMA√á√ÉO
    print("\n1. Unificando interesse_programacao...")
    if 'interesse_programacao' in df.columns:
        # Limpar textos primeiro
        df['interesse_programacao'] = df['interesse_programacao'].apply(limpar_texto)

        # Mapping ap√≥s limpeza
        df.loc[df['interesse_programacao'] == 'Todas as alternativas.', 'interesse_programacao'] = 'Todas as alternativas'
        df.loc[df['interesse_programacao'] == 'Poder trabalhar de qualquer lugar do mundo.', 'interesse_programacao'] = 'Poder trabalhar de qualquer lugar do mundo'
        df.loc[df['interesse_programacao'] == 'A possibilidade de ganhar altos sal√°rios.', 'interesse_programacao'] = 'A possibilidade de ganhar altos sal√°rios'
        df.loc[df['interesse_programacao'] == 'Trabalhar para outros pa√≠ses e ganhar em outra moeda.', 'interesse_programacao'] = 'Trabalhar para outros pa√≠ses e ganhar em outra moeda'
        df.loc[df['interesse_programacao'] == 'A ideia de nunca faltar emprego na √°rea.', 'interesse_programacao'] = 'A ideia de nunca faltar emprego na √°rea'

        valores_unicos = df['interesse_programacao'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    # 2. TEM COMPUTADOR/NOTEBOOK
    print("\n2. Unificando Tem computador/notebook?...")
    if 'Tem computador/notebook?' in df.columns:
        df['Tem computador/notebook?'] = df['Tem computador/notebook?'].apply(limpar_texto)

        df.loc[df['Tem computador/notebook?'] == 'SIM', 'Tem computador/notebook?'] = 'Sim'
        df.loc[df['Tem computador/notebook?'] == 'n√£o', 'Tem computador/notebook?'] = 'N√£o'

        valores_unicos = df['Tem computador/notebook?'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    # 3. O QUE MAIS VOC√ä QUER VER NO EVENTO
    print("\n3. Unificando O que mais voc√™ quer ver no evento?...")
    if 'O que mais voc√™ quer ver no evento?' in df.columns:
        df['O que mais voc√™ quer ver no evento?'] = df['O que mais voc√™ quer ver no evento?'].apply(limpar_texto)

        # Unificar "conseguir" vs "consegui"
        df.loc[df['O que mais voc√™ quer ver no evento?'] == 'Fazer transi√ß√£o de carreira e consegui meu primeiro emprego na √°rea', 'O que mais voc√™ quer ver no evento?'] = 'Fazer transi√ß√£o de carreira e conseguir meu primeiro emprego na √°rea'

        # Unificar "Quero saber se √© para mim" (com espa√ßos especiais)
        df.loc[df['O que mais voc√™ quer ver no evento?'].str.contains('Quero saber.*√©.*para.*mim', na=False, regex=True), 'O que mais voc√™ quer ver no evento?'] = 'Quero saber se √© para mim'

        # Unificar recrutadora
        df.loc[df['O que mais voc√™ quer ver no evento?'] == 'A aula com a recrutadora;', 'O que mais voc√™ quer ver no evento?'] = 'A aula com a recrutadora'

        valores_unicos = df['O que mais voc√™ quer ver no evento?'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    # 4. VOC√ä POSSUI CART√ÉO DE CR√âDITO
    print("\n4. Unificando Voc√™ possui cart√£o de cr√©dito?...")
    if 'Voc√™ possui cart√£o de cr√©dito?' in df.columns:
        df['Voc√™ possui cart√£o de cr√©dito?'] = df['Voc√™ possui cart√£o de cr√©dito?'].apply(limpar_texto)

        # Todos os "Sim" (com ou sem caracteres especiais)
        df.loc[df['Voc√™ possui cart√£o de cr√©dito?'].str.contains('Sim', na=False), 'Voc√™ possui cart√£o de cr√©dito?'] = 'Sim'

        valores_unicos = df['Voc√™ possui cart√£o de cr√©dito?'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    # 5. ATUALMENTE, QUAL A SUA FAIXA SALARIAL
    print("\n5. Unificando Atualmente, qual a sua faixa salarial?...")
    if 'Atualmente, qual a sua faixa salarial?' in df.columns:
        df['Atualmente, qual a sua faixa salarial?'] = df['Atualmente, qual a sua faixa salarial?'].apply(limpar_texto)

        # Remover pontos finais
        df.loc[df['Atualmente, qual a sua faixa salarial?'] == 'N√£o tenho renda.', 'Atualmente, qual a sua faixa salarial?'] = 'N√£o tenho renda'
        df.loc[df['Atualmente, qual a sua faixa salarial?'] == 'Entre R$1.000 a R$2.000 reais ao m√™s.', 'Atualmente, qual a sua faixa salarial?'] = 'Entre R$1.000 a R$2.000 reais ao m√™s'
        df.loc[df['Atualmente, qual a sua faixa salarial?'] == 'Entre R$2.001 a R$3.000 reais ao m√™s.', 'Atualmente, qual a sua faixa salarial?'] = 'Entre R$2.001 a R$3.000 reais ao m√™s'
        df.loc[df['Atualmente, qual a sua faixa salarial?'] == 'Entre R$3.001 a R$5.000 reais ao m√™s.', 'Atualmente, qual a sua faixa salarial?'] = 'Entre R$3.001 a R$5.000 reais ao m√™s'
        df.loc[df['Atualmente, qual a sua faixa salarial?'] == 'Mais de R$5.001 reais ao m√™s.', 'Atualmente, qual a sua faixa salarial?'] = 'Mais de R$5.001 reais ao m√™s'

        valores_unicos = df['Atualmente, qual a sua faixa salarial?'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    # 6. O QUE VOC√ä FAZ ATUALMENTE
    print("\n6. Unificando O que voc√™ faz atualmente?...")
    if 'O que voc√™ faz atualmente?' in df.columns:
        df['O que voc√™ faz atualmente?'] = df['O que voc√™ faz atualmente?'].apply(limpar_texto)

        # Corrigir "autonomo" para "aut√¥nomo"
        df.loc[df['O que voc√™ faz atualmente?'] == 'Sou autonomo', 'O que voc√™ faz atualmente?'] = 'Sou aut√¥nomo'

        # Unificar "aut√¥nomo" com descri√ß√£o
        df.loc[df['O que voc√™ faz atualmente?'] == 'Sou aut√¥nomo (Uber, freela, vendedor, etc).', 'O que voc√™ faz atualmente?'] = 'Sou aut√¥nomo'

        # Unificar "n√£o trabalho"
        df.loc[df['O que voc√™ faz atualmente?'] == 'Atualmente n√£o trabalho e nem estudo.', 'O que voc√™ faz atualmente?'] = 'N√£o trabalho e nem estudo'

        # Remover ponto final de "Trabalho em outra √°rea"
        df.loc[df['O que voc√™ faz atualmente?'] == 'Trabalho em outra √°rea e quero fazer transi√ß√£o para tecnologia.', 'O que voc√™ faz atualmente?'] = 'Trabalho em outra √°rea e quero fazer transi√ß√£o para tecnologia'

        # Remover ponto final de outras categorias
        df.loc[df['O que voc√™ faz atualmente?'] == 'Estou no ensino m√©dio ou acabei de sair e quero entrar na programa√ß√£o.', 'O que voc√™ faz atualmente?'] = 'Estou no ensino m√©dio ou acabei de sair e quero entrar na programa√ß√£o'
        df.loc[df['O que voc√™ faz atualmente?'] == 'Estudo T.I. na faculdade mas quero aprender mais por fora.', 'O que voc√™ faz atualmente?'] = 'Estudo T.I. na faculdade mas quero aprender mais por fora'
        df.loc[df['O que voc√™ faz atualmente?'] == 'Fa√ßo outro curso na faculdade e quero mudar para T.I.', 'O que voc√™ faz atualmente?'] = 'Fa√ßo outro curso na faculdade e quero mudar para T.I'

        valores_unicos = df['O que voc√™ faz atualmente?'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    # 7. QUAL A SUA IDADE
    print("\n7. Unificando Qual a sua idade?...")
    if 'Qual a sua idade?' in df.columns:
        df['Qual a sua idade?'] = df['Qual a sua idade?'].apply(limpar_texto)

        # Remover pontos finais
        df.loc[df['Qual a sua idade?'] == 'Menos de 18 anos.', 'Qual a sua idade?'] = 'Menos de 18 anos'
        df.loc[df['Qual a sua idade?'] == 'Mais de 55 anos.', 'Qual a sua idade?'] = 'Mais de 55 anos'

        valores_unicos = df['Qual a sua idade?'].nunique()
        print(f"   Resultado: {valores_unicos} valores √∫nicos")

    print(f"\nRESULTADO FINAL:")
    print(f"Dataset unificado: {len(df)} registros, {len(df.columns)} colunas")

    return df

def gerar_relatorio_final_novo(df):
    """Gera relat√≥rio final ap√≥s unifica√ß√£o"""

    print(f"\nRELAT√ìRIO FINAL - UNIFICA√á√ÉO COMPLETA")
    print("=" * 50)

    colunas_analisadas = [
        'interesse_programacao',
        'Tem computador/notebook?',
        'O que mais voc√™ quer ver no evento?',
        'Voc√™ possui cart√£o de cr√©dito?',
        'Atualmente, qual a sua faixa salarial?',
        'O que voc√™ faz atualmente?',
        'Qual a sua idade?'
    ]

    for coluna in colunas_analisadas:
        if coluna in df.columns:
            print(f"\nCOLUNA: {coluna}")
            print("-" * 50)

            valores_unicos = df[coluna].nunique()
            nulos = df[coluna].isnull().sum()
            pct_nulos = (nulos / len(df)) * 100

            print(f"Valores √∫nicos: {valores_unicos}")
            print(f"Valores nulos: {nulos:,} ({pct_nulos:.1f}%)")

            distribuicao = df[coluna].value_counts(dropna=False)
            print(f"Distribui√ß√£o (Todas as categorias):")

            for valor, count in distribuicao.items():
                pct = (count / len(df)) * 100
                valor_str = str(valor) if valor is not None else 'nan'
                print(f"  {valor_str:<40} {count:>6,} ({pct:>5.1f}%)")

# Executar unifica√ß√£o completa
df_final_unificado = unificar_categorias_completo()

# Gerar relat√≥rio final
gerar_relatorio_final_novo(df_final_unificado)

# Disponibilizar dataset final
pesquisa_final_unificado = df_final_unificado

"""## 8- Remo√ß√£o de colunas desnecess√°rias:
1. Campaign: remover. Se trata do lan√ßamento espec√≠fico.
2. Content: remover. S√£o an√∫ncios que carregam caracter√≠sticas individuais de cada lan√ßamento.
3. Coluna com nome em branco.

"""

# REMO√á√ÉO DE FEATURES DESNECESS√ÅRIAS
print("REMO√á√ÉO DE FEATURES DESNECESS√ÅRIAS")
print("=" * 38)

def remover_features_desnecessarias():
    """Remove features que n√£o ser√£o utilizadas no modelo"""

    df = pesquisa_final_unificado.copy()

    print(f"Dataset inicial: {len(df)} registros, {len(df.columns)} colunas")

    # DEBUG: Identificar colunas vazias ou com nomes problem√°ticos
    print(f"\nDEBUG - An√°lise de nomes de colunas:")
    print("-" * 50)

    colunas_problematicas = []
    for i, coluna in enumerate(df.columns):
        coluna_repr = repr(coluna)  # Mostra representa√ß√£o exata
        comprimento = len(str(coluna)) if coluna is not None else 0

        # Identificar poss√≠veis problemas
        problemas = []
        if coluna == '':
            problemas.append('VAZIA')
        if coluna is None:
            problemas.append('NONE')
        if pd.isna(coluna):
            problemas.append('NAN')
        if isinstance(coluna, str) and coluna.strip() == '':
            problemas.append('APENAS_ESPACOS')
        if comprimento == 0:
            problemas.append('COMPRIMENTO_ZERO')

        if problemas:
            print(f"  {i+1:2d}. {coluna_repr:<30} - PROBLEMA: {', '.join(problemas)}")
            colunas_problematicas.append(coluna)
        elif comprimento < 3 or not isinstance(coluna, str):
            print(f"  {i+1:2d}. {coluna_repr:<30} - SUSPEITA (len={comprimento})")

    if not colunas_problematicas:
        print("  Nenhuma coluna problem√°tica encontrada atrav√©s de an√°lise autom√°tica")

        # Verificar manualmente se alguma coluna parece vazia
        print("\n  Verificando colunas que podem parecer vazias:")
        for i, coluna in enumerate(df.columns):
            if len(str(coluna).strip()) <= 2:  # Muito curta
                print(f"    {i+1:2d}. '{coluna}' (comprimento: {len(str(coluna))})")
                colunas_problematicas.append(coluna)

    # Features a serem removidas (incluindo as encontradas no debug)
    features_remover = [
        'Campaign',  # Lan√ßamento espec√≠fico
        'Content',   # An√∫ncios individuais
    ]

    # Adicionar colunas problem√°ticas encontradas
    features_remover.extend(colunas_problematicas)

    print(f"\nFeatures marcadas para remo√ß√£o:")
    for feature in features_remover:
        if feature == '' or pd.isna(feature) or feature is None:
            print(f"  - Coluna problem√°tica: {repr(feature)}")
        else:
            print(f"  - {feature}")

    # Verificar quais colunas existem no dataset
    colunas_existentes = []
    colunas_nao_encontradas = []

    for feature in features_remover:
        if feature in df.columns:
            colunas_existentes.append(feature)
        else:
            colunas_nao_encontradas.append(feature)

    # Remover colunas existentes
    if len(colunas_existentes) > 0:
        print(f"\nColunas encontradas e removidas:")
        for coluna in colunas_existentes:
            if coluna == '' or pd.isna(coluna) or coluna is None:
                print(f"  ‚úì Coluna problem√°tica removida: {repr(coluna)}")
            else:
                print(f"  ‚úì {coluna} removida")

        df = df.drop(columns=colunas_existentes)

    # Reportar colunas n√£o encontradas
    if len(colunas_nao_encontradas) > 0:
        print(f"\nColunas n√£o encontradas no dataset:")
        for coluna in colunas_nao_encontradas:
            if coluna == '' or pd.isna(coluna) or coluna is None:
                print(f"  ! Coluna problem√°tica n√£o encontrada: {repr(coluna)}")
            else:
                print(f"  ! {coluna} n√£o encontrada")

    print(f"\nDataset final: {len(df)} registros, {len(df.columns)} colunas")
    print(f"Colunas removidas: {len(colunas_existentes)}")

    return df

def listar_colunas_restantes(df):
    """Lista as colunas que restaram no dataset"""

    print(f"\nCOLUNAS RESTANTES NO DATASET:")
    print("-" * 40)

    for i, coluna in enumerate(df.columns, 1):
        print(f"{i:2d}. {coluna}")

    print(f"\nTotal de colunas: {len(df.columns)}")

# Executar remo√ß√£o
df_features_removidas = remover_features_desnecessarias()

# Listar colunas restantes
listar_colunas_restantes(df_features_removidas)

# Disponibilizar dataset final
pesquisa_features_limpas = df_features_removidas

"""## 9- Investiga√ß√£o
Term: entender se as categorias num√©ricas ainda chegam.
Medium: entender os padr√µes de duplica√ß√£o para uma poss√≠vel unifica√ß√£o.
"""

# AN√ÅLISE DE UTM TERM E MEDIUM
print("AN√ÅLISE DE UTM TERM E MEDIUM")
print("=" * 32)

def analisar_utm_term():
    """Analisa a coluna Term para entender padr√µes e temporalidade"""

    df = pesquisa_features_limpas.copy()

    if 'Term' not in df.columns:
        print("Coluna 'Term' n√£o encontrada")
        return

    print("\n1. AN√ÅLISE GERAL - UTM TERM")
    print("=" * 40)

    # Estat√≠sticas b√°sicas
    total_registros = len(df)
    term_validos = df['Term'].notna().sum()
    term_nulos = df['Term'].isna().sum()
    valores_unicos = df['Term'].nunique()

    print(f"Total de registros: {total_registros:,}")
    print(f"Term v√°lidos: {term_validos:,} ({term_validos/total_registros*100:.1f}%)")
    print(f"Term nulos: {term_nulos:,} ({term_nulos/total_registros*100:.1f}%)")
    print(f"Valores √∫nicos: {valores_unicos}")

    # Categorizar tipos de Term
    df_term = df[df['Term'].notna()].copy()

    # Identificar padr√µes
    df_term['term_tipo'] = 'outro'
    df_term.loc[df_term['Term'] == 'ig', 'term_tipo'] = 'instagram'
    df_term.loc[df_term['Term'] == 'fb', 'term_tipo'] = 'facebook'
    df_term.loc[df_term['Term'].str.contains('--', na=False), 'term_tipo'] = 'id_numerico'
    df_term.loc[df_term['Term'].str.contains('{', na=False), 'term_tipo'] = 'parametro_dinamico'

    print(f"\n2. TIPOS DE TERM IDENTIFICADOS:")
    print("-" * 35)

    tipos_term = df_term['term_tipo'].value_counts()
    for tipo, count in tipos_term.items():
        pct = count / len(df_term) * 100
        print(f"{tipo:<20} {count:>8,} ({pct:>5.1f}%)")

    # An√°lise temporal se houver coluna de data
    if 'Data' in df.columns:
        print(f"\n3. AN√ÅLISE TEMPORAL - UTM TERM")
        print("-" * 35)

        df_term['Data'] = pd.to_datetime(df_term['Data'], errors='coerce')
        df_term_data = df_term[df_term['Data'].notna()].copy()

        if len(df_term_data) > 0:
            data_inicio = df_term_data['Data'].min()
            data_fim = df_term_data['Data'].max()

            print(f"Per√≠odo dos dados: {data_inicio.date()} a {data_fim.date()}")

            # An√°lise dos √∫ltimos 3 meses para ver se IDs num√©ricos ainda chegam
            data_corte = data_fim - pd.Timedelta(days=90)
            recentes = df_term_data[df_term_data['Data'] >= data_corte]

            print(f"\n√öltimos 3 meses (desde {data_corte.date()}):")
            tipos_recentes = recentes['term_tipo'].value_counts()
            for tipo, count in tipos_recentes.items():
                pct = count / len(recentes) * 100
                print(f"  {tipo:<20} {count:>6,} ({pct:>5.1f}%)")

    # Top valores por tipo
    print(f"\n4. TOP VALORES POR TIPO:")
    print("-" * 30)

    for tipo in tipos_term.index:
        subset = df_term[df_term['term_tipo'] == tipo]['Term'].value_counts().head(5)
        print(f"\n{tipo.upper()}:")
        for valor, count in subset.items():
            print(f"  {str(valor)[:40]:<42} {count:,}")

def analisar_utm_medium():
    """Analisa a coluna Medium para identificar duplicatas"""

    df = pesquisa_features_limpas.copy()

    if 'Medium' not in df.columns:
        print("Coluna 'Medium' n√£o encontrada")
        return

    print(f"\n\n5. AN√ÅLISE COMPLETA - UTM MEDIUM")
    print("=" * 42)

    # Estat√≠sticas b√°sicas
    total_registros = len(df)
    medium_validos = df['Medium'].notna().sum()
    medium_nulos = df['Medium'].isna().sum()
    valores_unicos = df['Medium'].nunique()

    print(f"Total de registros: {total_registros:,}")
    print(f"Medium v√°lidos: {medium_validos:,} ({medium_validos/total_registros*100:.1f}%)")
    print(f"Medium nulos: {medium_nulos:,} ({medium_nulos/total_registros*100:.1f}%)")
    print(f"Valores √∫nicos: {valores_unicos}")

    # Listar TODOS os valores para identificar duplicatas
    print(f"\n6. TODOS OS VALORES DE MEDIUM:")
    print("-" * 40)

    medium_counts = df['Medium'].value_counts(dropna=False)

    print(f"{'VALOR':<50} {'COUNT':<8} {'%':<6}")
    print("-" * 65)

    for i, (valor, count) in enumerate(medium_counts.items(), 1):
        pct = count / total_registros * 100
        valor_str = str(valor) if pd.notna(valor) else 'nan'

        # Truncar valores muito longos mas mostrar in√≠cio e fim
        if len(valor_str) > 47:
            valor_display = valor_str[:22] + '...' + valor_str[-22:]
        else:
            valor_display = valor_str

        print(f"{i:3d}. {valor_display:<47} {count:<8,} {pct:<6.2f}%")

    # Identificar poss√≠veis duplicatas por similaridade
    print(f"\n7. AN√ÅLISE DE POSS√çVEIS DUPLICATAS:")
    print("-" * 45)

    valores_medium = [str(v) for v in medium_counts.index if pd.notna(v)]
    duplicatas_detectadas = []

    # Buscar padr√µes similares
    for i, valor1 in enumerate(valores_medium):
        for valor2 in valores_medium[i+1:]:
            # Verificar similaridades
            if valor1.lower().strip() == valor2.lower().strip():
                duplicatas_detectadas.append((valor1, valor2, "case_different"))
            elif valor1.replace(' ', '') == valor2.replace(' ', ''):
                duplicatas_detectadas.append((valor1, valor2, "spacing_different"))
            elif valor1.rstrip('.') == valor2.rstrip('.'):
                duplicatas_detectadas.append((valor1, valor2, "punctuation_different"))
            elif valor1 in valor2 or valor2 in valor1:
                if abs(len(valor1) - len(valor2)) <= 3:  # Pequena diferen√ßa
                    duplicatas_detectadas.append((valor1, valor2, "substring_similar"))

    if duplicatas_detectadas:
        for valor1, valor2, tipo in duplicatas_detectadas[:20]:  # Primeiros 20
            count1 = medium_counts[valor1] if valor1 in medium_counts.index else 0
            count2 = medium_counts[valor2] if valor2 in medium_counts.index else 0
            print(f"\n{tipo.upper()}:")
            print(f"  '{valor1}' ({count1:,})")
            print(f"  '{valor2}' ({count2:,})")
    else:
        print("Nenhuma duplicata √≥bvia detectada automaticamente")

    print(f"\nTotal de poss√≠veis pares duplicados encontrados: {len(duplicatas_detectadas)}")

# Executar an√°lises
analisar_utm_term()
analisar_utm_medium()

# AN√ÅLISE TEMPORAL DOS IDs DE TERM
print("AN√ÅLISE TEMPORAL DOS IDs DE TERM")
print("=" * 36)

def analisar_ciclo_vida_ids():
    """Analisa datas de in√≠cio e fim dos IDs num√©ricos espec√≠ficos"""

    df = pesquisa_features_limpas.copy()

    if 'Term' not in df.columns or 'Data' not in df.columns:
        print("Colunas 'Term' ou 'Data' n√£o encontradas")
        return

    # Converter data
    df['Data'] = pd.to_datetime(df['Data'], errors='coerce')
    df_valido = df[(df['Term'].notna()) & (df['Data'].notna())].copy()

    # IDs espec√≠ficos para an√°lise
    ids_interesse = [
        '22527413714--180108372678--750940275538',
        '22527413657--179264823996--750940275529',
        '22451610660--179723795642--747241373657',
        '22521023178--174450244690--750940275532',
        '22451610660--179723795642--747241373663'
    ]

    print("AN√ÅLISE INDIVIDUAL DOS IDs PRINCIPAIS:")
    print("=" * 50)

    for i, term_id in enumerate(ids_interesse, 1):
        dados_id = df_valido[df_valido['Term'] == term_id]

        if len(dados_id) > 0:
            data_inicio = dados_id['Data'].min()
            data_fim = dados_id['Data'].max()
            total_leads = len(dados_id)
            duracao = (data_fim - data_inicio).days

            print(f"\n{i}. ID: {term_id}")
            print(f"   Primeira apari√ß√£o: {data_inicio.strftime('%Y-%m-%d')}")
            print(f"   √öltima apari√ß√£o:   {data_fim.strftime('%Y-%m-%d')}")
            print(f"   Dura√ß√£o:           {duracao} dias")
            print(f"   Total de leads:    {total_leads:,}")

            # Distribui√ß√£o temporal b√°sica
            if duracao > 0:
                leads_por_dia = total_leads / (duracao + 1)
                print(f"   Leads por dia:     {leads_por_dia:.1f}")

                # Verificar se houve gaps
                dados_id_sorted = dados_id.sort_values('Data')
                datas_unicas = dados_id_sorted['Data'].dt.date.unique()

                if len(datas_unicas) > 1:
                    maior_gap = 0
                    for j in range(1, len(datas_unicas)):
                        gap = (datas_unicas[j] - datas_unicas[j-1]).days
                        if gap > maior_gap:
                            maior_gap = gap

                    print(f"   Maior gap:         {maior_gap} dias")

                    # Analisar se teve per√≠odo ativo cont√≠nuo
                    if maior_gap <= 7:
                        print(f"   Status:            ATIVO CONT√çNUO (gaps ‚â§ 7 dias)")
                    elif maior_gap <= 30:
                        print(f"   Status:            ATIVO COM PAUSAS (gaps ‚â§ 30 dias)")
                    else:
                        print(f"   Status:            REATIVADO (gap > 30 dias)")
        else:
            print(f"\n{i}. ID: {term_id}")
            print(f"   Status: N√ÉO ENCONTRADO")

    # An√°lise geral de todos os IDs num√©ricos
    print(f"\n\nAN√ÅLISE GERAL - TODOS OS IDs NUM√âRICOS:")
    print("=" * 45)

    # Filtrar apenas IDs num√©ricos (padr√£o com --)
    ids_numericos = df_valido[df_valido['Term'].str.contains('--', na=False)]

    if len(ids_numericos) > 0:
        # Estat√≠sticas gerais
        total_ids_unicos = ids_numericos['Term'].nunique()
        data_inicio_geral = ids_numericos['Data'].min()
        data_fim_geral = ids_numericos['Data'].max()

        print(f"Total de IDs √∫nicos: {total_ids_unicos}")
        print(f"Per√≠odo geral: {data_inicio_geral.strftime('%Y-%m-%d')} a {data_fim_geral.strftime('%Y-%m-%d')}")
        print(f"Total de leads com IDs: {len(ids_numericos):,}")

        # Analisar ciclo de vida de cada ID
        ciclos_vida = []

        for term_id in ids_numericos['Term'].unique():
            dados_id = ids_numericos[ids_numericos['Term'] == term_id]

            inicio = dados_id['Data'].min()
            fim = dados_id['Data'].max()
            duracao = (fim - inicio).days
            total = len(dados_id)

            ciclos_vida.append({
                'id': term_id,
                'inicio': inicio,
                'fim': fim,
                'duracao': duracao,
                'total_leads': total
            })

        # Converter para DataFrame para an√°lise
        df_ciclos = pd.DataFrame(ciclos_vida)

        print(f"\nESTAT√çSTICAS DOS CICLOS DE VIDA:")
        print("-" * 40)
        print(f"Dura√ß√£o m√©dia:     {df_ciclos['duracao'].mean():.1f} dias")
        print(f"Dura√ß√£o mediana:   {df_ciclos['duracao'].median():.1f} dias")
        print(f"Dura√ß√£o m√≠nima:    {df_ciclos['duracao'].min()} dias")
        print(f"Dura√ß√£o m√°xima:    {df_ciclos['duracao'].max()} dias")

        # IDs de curta dura√ß√£o (‚â§ 7 dias)
        curta_duracao = df_ciclos[df_ciclos['duracao'] <= 7]
        print(f"\nIDs com dura√ß√£o ‚â§ 7 dias: {len(curta_duracao)} ({len(curta_duracao)/len(df_ciclos)*100:.1f}%)")

        # IDs de longa dura√ß√£o (> 30 dias)
        longa_duracao = df_ciclos[df_ciclos['duracao'] > 30]
        print(f"IDs com dura√ß√£o > 30 dias: {len(longa_duracao)} ({len(longa_duracao)/len(df_ciclos)*100:.1f}%)")

        # Top 10 IDs por dura√ß√£o
        print(f"\nTOP 10 IDs POR DURA√á√ÉO:")
        print("-" * 30)
        top_duracao = df_ciclos.nlargest(10, 'duracao')

        for idx, row in top_duracao.iterrows():
            print(f"{row['id'][:30]:<32} {row['duracao']:>3} dias ({row['total_leads']:>4} leads)")

        # Verificar sobreposi√ß√£o temporal
        print(f"\nAN√ÅLISE DE SOBREPOSI√á√ÉO:")
        print("-" * 30)

        ids_ativos_simultaneos = {}

        # Para cada data, contar quantos IDs estavam ativos
        for data in pd.date_range(data_inicio_geral, data_fim_geral, freq='D'):
            ids_ativos = 0
            for _, row in df_ciclos.iterrows():
                if row['inicio'] <= data <= row['fim']:
                    ids_ativos += 1
            ids_ativos_simultaneos[data] = ids_ativos

        max_simultaneos = max(ids_ativos_simultaneos.values())
        media_simultaneos = sum(ids_ativos_simultaneos.values()) / len(ids_ativos_simultaneos)

        print(f"M√°ximo de IDs simult√¢neos: {max_simultaneos}")
        print(f"M√©dia de IDs simult√¢neos: {media_simultaneos:.1f}")

# Executar an√°lise
analisar_ciclo_vida_ids()

"""## 10- Unifica√ß√£o das categorias da coluna Term e Source"""

# UNIFICA√á√ÉO DE UTM SOURCE E TERM
print("UNIFICA√á√ÉO DE UTM SOURCE E TERM")
print("=" * 35)

def unificar_utm_source_term():
    """Unifica categorias das colunas Source e Term"""

    df = pesquisa_features_limpas.copy()

    print(f"Dataset inicial: {len(df)} registros")

    # 1. UNIFICAR COLUNA SOURCE
    print(f"\n1. UNIFICANDO COLUNA SOURCE:")
    print("-" * 35)

    if 'Source' in df.columns:
        # Mostrar distribui√ß√£o antes
        source_antes = df['Source'].value_counts(dropna=False)
        print(f"Valores √∫nicos antes: {df['Source'].nunique()}")
        print("Distribui√ß√£o antes:")
        for valor, count in source_antes.head(10).items():
            pct = count / len(df) * 100
            valor_str = str(valor) if pd.notna(valor) else 'nan'
            print(f"  {valor_str:<25} {count:>6,} ({pct:>5.1f}%)")

        # Aplicar unifica√ß√£o
        df['Source'] = df['Source'].astype('object')  # Garantir tipo object

        # Manter as duas principais
        # facebook-ads: 67,311 (89.6%) - manter
        # google-ads: 6,242 (8.3%) - manter

        # Agrupar outras categorias em "outros"
        outras_sources = ['fb', 'teste', '[field id="utm_source"]', 'facebook-ads-SiteLink']

        for source in outras_sources:
            if source in df['Source'].values:
                df.loc[df['Source'] == source, 'Source'] = 'outros'

        print(f"\nAp√≥s unifica√ß√£o:")
        source_depois = df['Source'].value_counts(dropna=False)
        print(f"Valores √∫nicos depois: {df['Source'].nunique()}")
        for valor, count in source_depois.items():
            pct = count / len(df) * 100
            valor_str = str(valor) if pd.notna(valor) else 'nan'
            print(f"  {valor_str:<25} {count:>6,} ({pct:>5.1f}%)")

    # 2. UNIFICAR COLUNA TERM
    print(f"\n2. UNIFICANDO COLUNA TERM:")
    print("-" * 35)

    if 'Term' in df.columns:
        # Mostrar distribui√ß√£o antes
        term_antes = df['Term'].value_counts(dropna=False)
        print(f"Valores √∫nicos antes: {df['Term'].nunique()}")
        print("Distribui√ß√£o antes (top 10):")
        for valor, count in term_antes.head(10).items():
            pct = count / len(df) * 100
            valor_str = str(valor) if pd.notna(valor) else 'nan'
            print(f"  {valor_str:<35} {count:>6,} ({pct:>5.1f}%)")

        # Aplicar unifica√ß√£o
        df['Term'] = df['Term'].astype('object')  # Garantir tipo object

        # Criar coluna auxiliar para categoriza√ß√£o
        df_term_valid = df[df['Term'].notna()].copy()

        # Mapear categorias
        # 1. Instagram: 'ig' -> 'instagram'
        df.loc[df['Term'] == 'ig', 'Term'] = 'instagram'

        # 2. Facebook: 'fb' -> 'facebook'
        df.loc[df['Term'] == 'fb', 'Term'] = 'facebook'

        # 3. IDs num√©ricos (padr√£o com --) -> 'outros'
        mask_ids_numericos = df['Term'].str.contains('--', na=False)
        df.loc[mask_ids_numericos, 'Term'] = 'outros'

        # 4. Par√¢metros din√¢micos -> 'outros'
        mask_parametros = df['Term'].str.contains('{', na=False)
        df.loc[mask_parametros, 'Term'] = 'outros'

        # 5. Outros valores espec√≠ficos -> 'outros'
        outros_terms = df['Term'].notna() & (~df['Term'].isin(['instagram', 'facebook']))
        # Pegar valores que n√£o s√£o instagram, facebook ou NaN
        valores_outros = df.loc[outros_terms, 'Term'].unique()

        # Converter valores restantes para 'outros' (exceto os j√° processados acima)
        for valor in valores_outros:
            if isinstance(valor, str) and valor not in ['instagram', 'facebook']:
                # Verificar se √© um valor num√©rico ou outro tipo que deve virar 'outros'
                if not valor.isdigit() or len(valor) > 10:  # IDs longos ou textos especiais
                    df.loc[df['Term'] == valor, 'Term'] = 'outros'

        print(f"\nAp√≥s unifica√ß√£o:")
        term_depois = df['Term'].value_counts(dropna=False)
        print(f"Valores √∫nicos depois: {df['Term'].nunique()}")
        for valor, count in term_depois.items():
            pct = count / len(df) * 100
            valor_str = str(valor) if pd.notna(valor) else 'nan'
            print(f"  {valor_str:<25} {count:>6,} ({pct:>5.1f}%)")

    print(f"\nRESULTADO FINAL:")
    print(f"Dataset: {len(df)} registros, {len(df.columns)} colunas")

    return df

def verificar_consistencia_utm(df):
    """Verifica a consist√™ncia entre Source e Term ap√≥s unifica√ß√£o"""

    print(f"\n3. VERIFICA√á√ÉO DE CONSIST√äNCIA:")
    print("-" * 40)

    if 'Source' in df.columns and 'Term' in df.columns:
        # Tabela cruzada
        tabela_cruzada = pd.crosstab(df['Source'], df['Term'], margins=True, dropna=False)

        print("Tabela cruzada Source x Term:")
        print(tabela_cruzada)

        # Verificar l√≥gica: Term s√≥ deveria ser instagram/facebook quando Source = facebook-ads
        inconsistencias = []

        # Term = instagram/facebook mas Source != facebook-ads
        mask_term_fb = df['Term'].isin(['instagram', 'facebook'])
        mask_source_nao_fb = df['Source'] != 'facebook-ads'

        inconsistentes = df[mask_term_fb & mask_source_nao_fb]

        if len(inconsistentes) > 0:
            print(f"\nInconsist√™ncias encontradas: {len(inconsistentes)} registros")
            print("Term = instagram/facebook mas Source != facebook-ads")

            for idx, row in inconsistentes.head(5).iterrows():
                print(f"  Source: {row['Source']}, Term: {row['Term']}")
        else:
            print(f"\nNenhuma inconsist√™ncia detectada - dados coerentes!")

# Executar unifica√ß√£o
df_utm_unificado = unificar_utm_source_term()

# Verificar consist√™ncia
verificar_consistencia_utm(df_utm_unificado)

# Disponibilizar dataset final
pesquisa_utm_unificado = df_utm_unificado

"""##11- Unifica√ß√£o de categorias Medium"""

# UNIFICA√á√ÉO DE UTM MEDIUM - EXTRA√á√ÉO DE P√öBLICOS
print("UNIFICA√á√ÉO DE UTM MEDIUM - EXTRA√á√ÉO DE P√öBLICOS")
print("=" * 52)

import re

def extrair_publico_medium():
    """Extrai e unifica tipos de p√∫blico da coluna Medium"""

    df = pesquisa_utm_unificado.copy()

    if 'Medium' not in df.columns:
        print("Coluna 'Medium' n√£o encontrada")
        return df

    print(f"Dataset inicial: {len(df)} registros")
    print(f"Medium - valores √∫nicos antes: {df['Medium'].nunique()}")

    # Mostrar alguns exemplos antes
    print(f"\nExemplos antes da extra√ß√£o:")
    exemplos_antes = df['Medium'].value_counts().head(10)
    for valor, count in exemplos_antes.items():
        if pd.notna(valor):
            print(f"  {str(valor)[:70]:<72} ({count:,})")

    # Fun√ß√£o para extrair p√∫blico (parte antes do |)
    def extrair_publico(medium_value):
        if pd.isna(medium_value):
            return medium_value

        medium_str = str(medium_value).strip()

        # Se tem |, pegar parte depois do √∫ltimo |, n√£o antes
        if '|' in medium_str:
            partes = medium_str.split('|')
            if len(partes) >= 2:
                # Se primeira parte √© s√≥ "ADV", pegar a segunda parte
                if partes[0].strip().upper() in ['ADV', 'ADV ']:
                    publico = partes[1].strip()
                else:
                    publico = partes[0].strip()
            else:
                publico = medium_str
        else:
            publico = medium_str

        # Se ainda sobrou s√≥ "ADV", tentar extrair de outra forma
        if publico.upper().strip() == 'ADV':
            # Voltar ao valor original e tentar alternativa
            if '|' in medium_str:
                # Pegar tudo depois do primeiro |
                publico = medium_str.split('|', 1)[1].strip()

        return publico

    # Aplicar extra√ß√£o
    print(f"\nExtraindo p√∫blicos...")
    df['Medium'] = df['Medium'].apply(extrair_publico)

    print(f"Medium - valores √∫nicos ap√≥s extra√ß√£o: {df['Medium'].nunique()}")

    # Mostrar distribui√ß√£o ap√≥s extra√ß√£o inicial
    print(f"\nDistribui√ß√£o ap√≥s extra√ß√£o inicial (top 15):")
    medium_apos_extracao = df['Medium'].value_counts(dropna=False)
    for i, (valor, count) in enumerate(medium_apos_extracao.head(15).items(), 1):
        pct = count / len(df) * 100
        valor_str = str(valor) if pd.notna(valor) else 'nan'
        print(f"{i:2d}. {valor_str[:60]:<62} {count:>6,} ({pct:>5.1f}%)")

    # Identificar e unificar duplicatas
    print(f"\nIdentificando p√∫blicos similares para unifica√ß√£o...")

    valores_medium = df['Medium'].dropna().unique()
    grupos_similares = {}
    processados = set()

    # Fun√ß√£o para normalizar texto para compara√ß√£o
    def normalizar_para_comparacao(texto):
        if pd.isna(texto):
            return ""

        texto_norm = str(texto).lower().strip()

        # Remover espa√ßos extras
        texto_norm = re.sub(r'\s+', ' ', texto_norm)

        # Remover pontua√ß√£o final
        texto_norm = texto_norm.rstrip('.')

        return texto_norm

    # Agrupar p√∫blicos id√™nticos (ap√≥s normaliza√ß√£o)
    for valor in valores_medium:
        if valor in processados:
            continue

        valor_norm = normalizar_para_comparacao(valor)
        grupo = [valor]

        # Buscar valores similares
        for outro_valor in valores_medium:
            if outro_valor != valor and outro_valor not in processados:
                outro_norm = normalizar_para_comparacao(outro_valor)

                # Crit√©rios de similaridade
                if valor_norm == outro_norm:
                    grupo.append(outro_valor)
                    processados.add(outro_valor)

        if len(grupo) > 1:
            # Escolher representante (o mais comum ou mais limpo)
            contagens = [(v, (df['Medium'] == v).sum()) for v in grupo]
            representante = max(contagens, key=lambda x: x[1])[0]
            grupos_similares[representante] = grupo

        processados.add(valor)

    # Aplicar unifica√ß√µes
    if grupos_similares:
        print(f"\nGrupos similares encontrados para unifica√ß√£o:")
        for representante, grupo in grupos_similares.items():
            if len(grupo) > 1:
                count_total = sum((df['Medium'] == v).sum() for v in grupo)
                print(f"\nUnificando em: '{representante}' ({count_total:,} registros)")
                for valor in grupo:
                    if valor != representante:
                        count_individual = (df['Medium'] == valor).sum()
                        print(f"  '{valor}' ({count_individual:,})")
                        df.loc[df['Medium'] == valor, 'Medium'] = representante
    else:
        print("Nenhum grupo similar detectado automaticamente")

    # Unifica√ß√µes manuais espec√≠ficas baseadas nos dados mostrados
    print(f"\nAplicando unifica√ß√µes manuais espec√≠ficas...")

    unificacoes_manuais = {
        # Case sensitivity
        'ABERTO': 'Aberto',

        # Lookalikes similares mas diferentes percentuais (manter separados como solicitado)
        # Interesses similares mas diferentes especificidades (manter separados como solicitado)

        # Apenas unificar exatamente iguais ap√≥s limpeza
    }

    for original, unificado in unificacoes_manuais.items():
        if original in df['Medium'].values:
            count = (df['Medium'] == original).sum()
            df.loc[df['Medium'] == original, 'Medium'] = unificado
            print(f"  '{original}' ‚Üí '{unificado}' ({count:,} registros)")

    print(f"\nResultado final:")
    print(f"Medium - valores √∫nicos ap√≥s unifica√ß√£o: {df['Medium'].nunique()}")

    return df

def relatorio_final_medium(df):
    """Gera relat√≥rio final da coluna Medium ap√≥s unifica√ß√£o"""

    print(f"\n" + "="*60)
    print(f"RELAT√ìRIO FINAL - MEDIUM (P√öBLICOS)")
    print(f"="*60)

    if 'Medium' not in df.columns:
        print("Coluna Medium n√£o encontrada")
        return

    total_registros = len(df)
    medium_validos = df['Medium'].notna().sum()
    medium_nulos = df['Medium'].isna().sum()
    valores_unicos = df['Medium'].nunique()

    print(f"Total de registros: {total_registros:,}")
    print(f"Medium v√°lidos: {medium_validos:,} ({medium_validos/total_registros*100:.1f}%)")
    print(f"Medium nulos: {medium_nulos:,} ({medium_nulos/total_registros*100:.1f}%)")
    print(f"P√∫blicos √∫nicos: {valores_unicos}")

    print(f"\nDistribui√ß√£o final dos p√∫blicos (top 20):")
    print("-" * 80)
    print(f"{'#':<3} {'P√öBLICO':<55} {'COUNT':<8} {'%':<6}")
    print("-" * 80)

    medium_final = df['Medium'].value_counts(dropna=False)

    for i, (valor, count) in enumerate(medium_final.head(20).items(), 1):
        pct = count / total_registros * 100
        valor_str = str(valor) if pd.notna(valor) else 'nan'

        # Truncar se muito longo
        if len(valor_str) > 52:
            valor_display = valor_str[:49] + '...'
        else:
            valor_display = valor_str

        print(f"{i:<3} {valor_display:<55} {count:<8,} {pct:<6.1f}%")

    if len(medium_final) > 20:
        print(f"... e mais {len(medium_final) - 20} p√∫blicos")

# Executar extra√ß√£o e unifica√ß√£o
df_medium_unificado = extrair_publico_medium()

# Gerar relat√≥rio final
relatorio_final_medium(df_medium_unificado)

# Exportar categorias para CSV
print(f"\n" + "="*60)
print(f"EXPORTA√á√ÉO DAS CATEGORIAS MEDIUM")
print(f"="*60)

# Criar DataFrame com todas as categorias e suas estat√≠sticas
medium_stats = df_medium_unificado['Medium'].value_counts(dropna=False)
total_registros = len(df_medium_unificado)

categorias_data = []
for i, (categoria, count) in enumerate(medium_stats.items(), 1):
    pct = (count / total_registros) * 100
    categoria_str = str(categoria) if pd.notna(categoria) else 'NaN'

    categorias_data.append({
        'rank': i,
        'categoria_medium': categoria_str,
        'quantidade': count,
        'percentual': round(pct, 2)
    })

# Converter para DataFrame
df_categorias = pd.DataFrame(categorias_data)

# Nome do arquivo
arquivo_csv = 'categorias_medium_publicos.csv'

try:
    # Exportar para CSV
    df_categorias.to_csv(arquivo_csv, index=False, encoding='utf-8')

    print(f"Arquivo exportado com sucesso:")
    print(f"  Nome: {arquivo_csv}")
    print(f"  Total de categorias: {len(df_categorias)}")
    print(f"  Colunas: rank, categoria_medium, quantidade, percentual")

    # Mostrar pr√©via do arquivo
    print(f"\nPr√©via do arquivo CSV (primeiras 10 linhas):")
    print("-" * 70)
    print(f"{'RANK':<5} {'CATEGORIA':<35} {'QTD':<8} {'%':<6}")
    print("-" * 70)

    for _, row in df_categorias.head(10).iterrows():
        categoria_display = row['categoria_medium'][:32] + '...' if len(str(row['categoria_medium'])) > 32 else row['categoria_medium']
        print(f"{row['rank']:<5} {categoria_display:<35} {row['quantidade']:<8} {row['percentual']:<6}%")

    if len(df_categorias) > 10:
        print(f"... e mais {len(df_categorias) - 10} categorias no arquivo")

except Exception as e:
    print(f"Erro ao exportar arquivo CSV: {e}")
    print("Verifique permiss√µes de escrita no diret√≥rio atual")

# Disponibilizar dataset final
pesquisa_medium_unificado = df_medium_unificado

# UNIFICA√á√ÉO DE UTM MEDIUM BASEADA EM ACTIONS
print("UNIFICA√á√ÉO DE UTM MEDIUM BASEADA EM ACTIONS")
print("=" * 45)

import pandas as pd

def unificar_medium_por_actions():
    """Unifica categorias Medium baseado no mapeamento de actions"""

    df = pesquisa_medium_unificado.copy()

    if 'Medium' not in df.columns:
        print("Coluna 'Medium' n√£o encontrada")
        return df

    print(f"Dataset inicial: {len(df)} registros")
    print(f"Medium - valores √∫nicos antes: {df['Medium'].nunique()}")

    # Criar mapeamento baseado na an√°lise da planilha
    mapping_dict = {
        # MANTER (12 categorias)
        'Lookalike 2% Cadastrados - DEV 2.0 + Interesses': 'manter',
        'Aberto': 'manter',
        'Linguagem de programa√ß√£o': 'manter',
        'Lookalike 2% Alunos + Interesse Linguagem de Programa√ß√£o': 'manter',
        'dgen': 'manter',
        'Lookalike 1% Cadastrados - DEV 2.0 + Interesse Ci√™ncia da Computa√ß√£o': 'manter',
        'Lookalike 2% Alunos + Interesse Ci√™ncia da Computa√ß√£o': 'manter',
        'Lookalike 1% Cadastrados - DEV 2.0 + Interesse Linguagem de Programa√ß√£o': 'manter',
        'Interesse Python (linguagem de programa√ß√£o)': 'manter',
        'Interesse Programa√ß√£o': 'manter',
        'nan': 'manter',
        'Interesse Ci√™ncia da computa√ß√£o': 'manter',

        # OUTROS (25 categorias)
        '{{adset.name}}': 'outros',
        'paid': 'outros',
        'Interesses': 'outros',
        'search': 'outros',
        'pmax': 'outros',
        'Desenvolvimento profissional': 'outros',
        'Funcion√°rios de m√©dias empresas B2B (200 a 500 funcion√°rios)': 'outros',
        'Funcion√°rios de pequenas empresas B2B (10 a 200 funcion√°rios)': 'outros',
        'Funcion√°rios de grandes empresas B2B (mais de 500 funcion√°rios) ‚Äî C√≥pia': 'outros',
        'Lookalike 2% Alunos   Interesse Linguagem de Programa√ß√£o': 'outros',
        'Lookalike 1% Cadastrados - DEV 2.0   Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Aberto++AD08-1002': 'outros',
        'Lookalike 1% Cadastrados - DEV 2.0 Interesse Linguagem de Programa√ß√£o': 'outros',
        'Lookalike 2% Alunos Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'ADV+%7C+Lookalike+2%25+Cadastrados+-+DEV+2.0+%2B+Interesses': 'outros',
        'Lookalike Envolvimento 30D Salvou 180D Direct 180D Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Lookalike% Cadastrados - DEV 2.0 + Interesse Linguagem de Programa√ß√£o': 'outros',
        'teste': 'outros',
        '[field id="utm_medium"]': 'outros',
        'ADV %7C Linguagem de programa√ß√£o': 'outros',
        'gdn': 'outros',
        'Lookalike 3% Alunos Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Lookalike Envolvimento 30D   Salvou 180D   Direct 180D   Interesse Linguagem de Programa√ß√£o': 'outros',
        'Lookalike Envolvimento 30D + Salvou80D + Direct80D + Interesse Linguagem de Programa√ß√£o': 'outros',
        'Lookalike Envolvimento 60D Salvou 365D Direct 365D Interesse Ci√™ncia da Computa√ß√£o': 'outros',

        # A√á√ïES ESPEC√çFICAS
        'Lookalike 2% Cadastrados - DEV 2.0   Interesses': 'Lookalike 2% Cadastrados - DEV 2.0 + Interesses',

        'Lookalike 3% Alunos + Interesses': 'outros',
        'Lookalike 3% Alunos + Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Lookalike 3% Alunos + Interesse Linguagem de Programa√ß√£o': 'outros',

        'Interesse Python': 'outros',

        'Lookalike 3% Cadastrados - DEV 2.0 + Interesses': 'outros',
        'Lookalike 3% Cadastrados - DEV 2.0 + Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Lookalike 3% Cadastrados - DEV 2.0 + Interesse Linguagem de Programa√ß√£o': 'outros',

        'Lookalike Envolvimento 30D + Salvou 180D + Direct 180D + Interesse Linguagem de Programa√ß√£o': 'outros',
        'Lookalike Envolvimento 30D + Salvou 180D + Direct 180D + Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Lookalike Envolvimento 60D + Salvou 365D + Direct 365D + Interesse Ci√™ncia da Computa√ß√£o': 'outros',
        'Lookalike Envolvimento 60D + Salvou 365D + Direct 365D + Interesse Linguagem de Programa√ß√£o': 'outros',

        'Interesse Linguagem de programa√ß√£o': 'Linguagem de programa√ß√£o'
    }

    print(f"Mapeamento criado para {len(mapping_dict)} categorias")

    # Mostrar estat√≠sticas antes da unifica√ß√£o
    print(f"\nDistribui√ß√£o antes da unifica√ß√£o (top 10):")
    medium_antes = df['Medium'].value_counts(dropna=False)
    for i, (valor, count) in enumerate(medium_antes.head(10).items(), 1):
        pct = count / len(df) * 100
        valor_str = str(valor) if pd.notna(valor) else 'nan'
        print(f"{i:2d}. {valor_str[:50]:<52} {count:>6,} ({pct:>5.1f}%)")

    # Aplicar unifica√ß√£o
    print(f"\nAplicando unifica√ß√£o baseada em actions...")

    def aplicar_unificacao(medium_value):
        if pd.isna(medium_value):
            return medium_value

        medium_str = str(medium_value)

        # Verificar se existe mapeamento para esta categoria
        if medium_str in mapping_dict:
            action = mapping_dict[medium_str]

            if action == 'manter':
                return medium_str
            elif action == 'outros':
                return 'Outros'
            else:
                # Para a√ß√µes espec√≠ficas, usar o nome da action como nova categoria
                return action
        else:
            # Se n√£o encontrou mapeamento, manter original
            return medium_str

    # Aplicar a fun√ß√£o de unifica√ß√£o
    df['Medium'] = df['Medium'].apply(aplicar_unificacao)

    print(f"Medium - valores √∫nicos ap√≥s unifica√ß√£o: {df['Medium'].nunique()}")

    return df

def relatorio_unificacao_actions(df_original, df_unificado):
    """Gera relat√≥rio detalhado da unifica√ß√£o por actions"""

    print(f"\n" + "="*70)
    print(f"RELAT√ìRIO DE UNIFICA√á√ÉO POR ACTIONS")
    print(f"="*70)

    # Definir contadores por action
    actions_stats = {
        'manter': {'categorias': 12, 'total_leads': 67343, 'percentual': 89.65},
        'outros': {'categorias': 25, 'total_leads': 894, 'percentual': 1.17},
        'Lookalike 2% Cadastrados - DEV 2.0 + Interesses': {'categorias': 1, 'total_leads': 29, 'percentual': 0.04},
        'Lookalike 3% Alunos + Interesses': {'categorias': 3, 'total_leads': 693, 'percentual': 0.93},
        'Interesse Python (linguagem de programa√ß√£o)': {'categorias': 1, 'total_leads': 208, 'percentual': 0.28},
        'Lookalike 3% Cadastrados - DEV 2.0': {'categorias': 3, 'total_leads': 1016, 'percentual': 1.34},
        'Lookalike Envolvimento + Salvou': {'categorias': 4, 'total_leads': 4221, 'percentual': 5.61},
        'Linguagem de programa√ß√£o': {'categorias': 1, 'total_leads': 706, 'percentual': 0.94}
    }

    print(f"Actions processadas:")
    for action, stats in actions_stats.items():
        print(f"\n{action}:")
        print(f"  Categorias originais: {stats['categorias']}")
        print(f"  Total de leads: {stats['total_leads']:,} ({stats['percentual']:.2f}%)")

        if action == 'manter':
            print(f"  ‚Üí Mantidas como categorias individuais")
        elif action == 'outros':
            print(f"  ‚Üí Agrupadas em 'Outros'")
        else:
            print(f"  ‚Üí Unificadas em '{action}'")

    # Compara√ß√£o antes/depois
    print(f"\n" + "="*70)
    print(f"COMPARA√á√ÉO ANTES/DEPOIS")
    print(f"="*70)

    antes_count = df_original['Medium'].nunique()
    depois_count = df_unificado['Medium'].nunique()
    reducao = antes_count - depois_count
    reducao_pct = (reducao / antes_count) * 100

    print(f"Categorias antes: {antes_count}")
    print(f"Categorias depois: {depois_count}")
    print(f"Redu√ß√£o: {reducao} categorias ({reducao_pct:.1f}%)")

    # Distribui√ß√£o final
    print(f"\nDistribui√ß√£o final (top 15):")
    print("-" * 70)
    print(f"{'#':<3} {'CATEGORIA':<45} {'COUNT':<8} {'%':<6}")
    print("-" * 70)

    medium_final = df_unificado['Medium'].value_counts(dropna=False)
    total_registros = len(df_unificado)

    for i, (valor, count) in enumerate(medium_final.head(16).items(), 1):
        pct = count / total_registros * 100
        valor_str = str(valor) if pd.notna(valor) else 'nan'

        if len(valor_str) > 42:
            valor_display = valor_str[:39] + '...'
        else:
            valor_display = valor_str

        print(f"{i:<3} {valor_display:<45} {count:<8,} {pct:<6.1f}%")

    if len(medium_final) > 16:
        print(f"... e mais {len(medium_final) - 16} categorias")

# Executar unifica√ß√£o
print("Iniciando processo de unifica√ß√£o...")
df_original = pesquisa_medium_unificado.copy()
df_medium_actions = unificar_medium_por_actions()

# Gerar relat√≥rio
relatorio_unificacao_actions(df_original, df_medium_actions)

# Disponibilizar dataset final
pesquisa_medium_actions_unificado = df_medium_actions

print(f"\nProcesso conclu√≠do!")
print(f"Dataset final dispon√≠vel em: pesquisa_medium_actions_unificado")

"""## 12- Investiga√ß√£o
Final para entender como ficaram as categorias das colunas categ√≥ricas ap√≥s todas as unifica√ß√µes
"""

# AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS - DATASET FINAL
print("AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS - DATASET FINAL")
print("=" * 50)

import matplotlib.pyplot as plt
import pandas as pd

def analisar_categoricas_final():
    """Analisa vari√°veis categ√≥ricas do dataset final unificado"""

    # Dataset final ap√≥s todas as unifica√ß√µes
    datasets = {
        'Dataset Final': pesquisa_medium_actions_unificado
    }

    # Colunas a excluir da an√°lise
    excluir = ['nome', 'telefone', 'email', 'e-mail', 'data', 'aba_origem', 'arquivo_origem']

    for dataset_nome, df in datasets.items():
        print(f"\nDATASET: {dataset_nome}")
        print("=" * 60)
        print(f"Total de registros: {len(df):,}")
        print(f"Total de colunas: {len(df.columns)}")

        # Identificar colunas categ√≥ricas
        colunas_categoricas = []

        for col in df.columns:
            # Excluir identificadores, datas e metadados
            if not any(termo in col.lower() for termo in excluir):
                # Incluir se for object/string ou se tiver poucos valores √∫nicos
                if df[col].dtype == 'object' or df[col].nunique() <= 50:
                    colunas_categoricas.append(col)

        print(f"Colunas categ√≥ricas encontradas: {len(colunas_categoricas)}")
        print(f"Colunas analisadas: {colunas_categoricas}")

        for coluna in colunas_categoricas:
            print(f"\n" + "="*80)
            print(f"COLUNA: {coluna}")
            print("="*80)

            # 1. Informa√ß√µes b√°sicas
            valores_unicos = df[coluna].nunique()
            total_registros = len(df)

            print(f"Valores √∫nicos: {valores_unicos:,}")
            print(f"Total de registros: {total_registros:,}")

            # 2. An√°lise de valores nulos
            nulos = df[coluna].isnull().sum()
            pct_nulos = (nulos / len(df)) * 100
            print(f"Valores nulos: {nulos:,} ({pct_nulos:.1f}%)")

            # 3. An√°lise de valores n√£o-nulos
            nao_nulos = df[coluna].notna().sum()
            pct_nao_nulos = (nao_nulos / len(df)) * 100
            print(f"Valores v√°lidos: {nao_nulos:,} ({pct_nao_nulos:.1f}%)")

            # 4. Distribui√ß√£o de categorias
            distribuicao = df[coluna].value_counts(dropna=False)

            print(f"\nDISTRIBUI√á√ÉO DE VALORES:")
            print("-" * 80)
            print(f"{'#':<3} {'VALOR':<40} {'COUNT':<10} {'%':<8}")
            print("-" * 80)

            # Mostrar todas se <= 20, sen√£o mostrar top 20
            limite = min(20, len(distribuicao))

            for i, (valor, count) in enumerate(distribuicao.head(limite).items(), 1):
                pct = (count / len(df)) * 100
                valor_str = str(valor) if pd.notna(valor) else 'NaN'

                # Truncar valor se muito longo
                if len(valor_str) > 37:
                    valor_display = valor_str[:34] + '...'
                else:
                    valor_display = valor_str

                print(f"{i:<3} {valor_display:<40} {count:<10,} {pct:<8.1f}%")

            if len(distribuicao) > 20:
                print(f"... e mais {len(distribuicao) - 20} valores")

            # 5. Estat√≠sticas resumo
            print(f"\nESTAT√çSTICAS RESUMO:")
            print("-" * 40)
            print(f"Categoria mais comum: '{distribuicao.index[0]}' ({distribuicao.iloc[0]:,} registros)")
            if len(distribuicao) > 1:
                print(f"Categoria menos comum: '{distribuicao.index[-1]}' ({distribuicao.iloc[-1]:,} registros)")

            # Categorias raras (< 1%)
            categorias_raras = distribuicao[distribuicao / len(df) < 0.01]
            if len(categorias_raras) > 0:
                print(f"Categorias raras (< 1%): {len(categorias_raras)}")
                total_raras = categorias_raras.sum()
                pct_raras = (total_raras / len(df)) * 100
                print(f"Total de registros em categorias raras: {total_raras:,} ({pct_raras:.1f}%)")

            # 6. Gr√°fico
            print(f"\nGerando gr√°fico...")

            plt.figure(figsize=(12, 8))

            if valores_unicos > 15:
                # Para muitas categorias, mostrar apenas top 15
                top_15 = distribuicao.head(15)
                bars = plt.bar(range(len(top_15)), top_15.values, color='skyblue', edgecolor='navy', alpha=0.7)
                plt.title(f'{dataset_nome} - {coluna} (Top 15 Categorias)', fontsize=14, fontweight='bold')
                plt.xticks(range(len(top_15)), [str(x)[:20] for x in top_15.index], rotation=45, ha='right')

                # Adicionar valores nas barras
                for bar in bars:
                    height = bar.get_height()
                    plt.text(bar.get_x() + bar.get_width()/2., height + len(df)*0.005,
                            f'{int(height):,}', ha='center', va='bottom', fontsize=9)
            else:
                # Para poucas categorias, mostrar todas
                bars = plt.bar(range(len(distribuicao)), distribuicao.values, color='lightcoral', edgecolor='darkred', alpha=0.7)
                plt.title(f'{dataset_nome} - {coluna}', fontsize=14, fontweight='bold')
                plt.xticks(range(len(distribuicao)), [str(x)[:20] for x in distribuicao.index], rotation=45, ha='right')

                # Adicionar valores nas barras
                for bar in bars:
                    height = bar.get_height()
                    plt.text(bar.get_x() + bar.get_width()/2., height + len(df)*0.005,
                            f'{int(height):,}', ha='center', va='bottom', fontsize=9)

            plt.xlabel('Categorias', fontsize=12)
            plt.ylabel('Frequ√™ncia', fontsize=12)
            plt.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            plt.show()

            print(f"An√°lise da coluna '{coluna}' conclu√≠da.")

        print(f"\n" + "="*80)
        print(f"AN√ÅLISE COMPLETA DO DATASET '{dataset_nome}' FINALIZADA")
        print("="*80)

# Executar an√°lise
analisar_categoricas_final()

"""## 13- Cria√ß√£o dos dois datasets de pesquisa
1. Vers√£o apenas com as features atuais, representando menor valor de missing
2. Vers√£o com as features de maior missing removidas, para teste
"""

# CRIA√á√ÉO DE VERS√ïES DO DATASET POR MISSING RATE
print("CRIA√á√ÉO DE VERS√ïES DO DATASET POR MISSING RATE")
print("=" * 50)

import pandas as pd

def criar_versoes_dataset():
    """Cria duas vers√µes do dataset baseadas na taxa de missing"""

    df = pesquisa_medium_actions_unificado.copy()

    print(f"Dataset original: {len(df)} registros, {len(df.columns)} colunas")

    # Converter coluna de data para datetime se n√£o estiver
    if 'Data' in df.columns:
        df['Data'] = pd.to_datetime(df['Data'], errors='coerce')

    # Definir cutoff de data (quando as features cr√≠ticas come√ßaram a ser preenchidas)
    cutoff_date = pd.to_datetime('2025-03-01')

    # VERS√ÉO 1: Dataset p√≥s-cutoff (per√≠odo com menor missing das features cr√≠ticas)
    versao_1 = df[df['Data'] >= cutoff_date].copy()

    # Remover manualmente a coluna "Qual o seu n√≠vel em programa√ß√£o?" da vers√£o 1
    coluna_remover_v1 = 'Qual o seu n√≠vel em programa√ß√£o?'
    if coluna_remover_v1 in versao_1.columns:
        versao_1 = versao_1.drop(columns=[coluna_remover_v1])
        print(f"Coluna removida da Vers√£o 1: '{coluna_remover_v1}'")

    print(f"Registros p√≥s {cutoff_date.strftime('%Y-%m-%d')}: {len(versao_1)}")

    # Definir features com missing cr√≠tico para remo√ß√£o APENAS da vers√£o 2
    features_missing_critico = [
        'J√° estudou programa√ß√£o?',
        'Voc√™ j√° fez/faz/pretende fazer',
        'Voc√™ j√° fez/faz/pretende fazer faculdade?',
        'Tem computador/notebook?',
        'Qual o seu n√≠vel em programa√ß√£o?'
    ]

    # Verificar quais features existem no dataset
    features_existentes = [col for col in features_missing_critico if col in df.columns]
    features_nao_existentes = [col for col in features_missing_critico if col not in df.columns]

    print(f"\nFeatures de missing cr√≠tico encontradas: {len(features_existentes)}")
    for feature in features_existentes:
        print(f"  ‚úì {feature}")

    if features_nao_existentes:
        print(f"\nFeatures de missing cr√≠tico N√ÉO encontradas: {len(features_nao_existentes)}")
        for feature in features_nao_existentes:
            print(f"  ‚úó {feature}")

    print(f"\n" + "="*60)
    print("VERS√ÉO 1: MENOR MISSING RATE (p√≥s 2025-03-01)")
    print("="*60)
    print(f"Registros: {len(versao_1):,}")
    print(f"Features cr√≠ticas MANTIDAS (per√≠odo com menor missing)")

    # An√°lise de missing rate na vers√£o 1
    missing_stats_v1 = {}
    for col in versao_1.columns:
        if col != 'Data':
            missing_count = versao_1[col].isnull().sum()
            missing_rate = (missing_count / len(versao_1)) * 100
            missing_stats_v1[col] = {
                'missing_count': missing_count,
                'missing_rate': missing_rate,
                'valid_count': len(versao_1) - missing_count
            }

    # Ordenar por taxa de missing
    missing_sorted_v1 = sorted(missing_stats_v1.items(), key=lambda x: x[1]['missing_rate'])

    print(f"\nTaxa de missing por coluna (ordenado):")
    print(f"{'COLUNA':<45} {'V√ÅLIDOS':<8} {'MISSING':<8} {'% MISS':<7}")
    print("-" * 70)

    for col, stats in missing_sorted_v1:
        print(f"{col[:42]:<45} {stats['valid_count']:<8,} {stats['missing_count']:<8,} {stats['missing_rate']:<7.1f}%")

    # VERS√ÉO 2: Dataset completo SEM features de missing cr√≠tico
    colunas_manter_v2 = [col for col in df.columns if col not in features_existentes]
    versao_2 = df[colunas_manter_v2].copy()

    print(f"\n" + "="*60)
    print("VERS√ÉO 2: SEM FEATURES DE MISSING CR√çTICO")
    print("="*60)
    print(f"Registros: {len(versao_2):,}")
    print(f"Colunas removidas: {len(features_existentes)}")
    for feature in features_existentes:
        print(f"  - {feature}")

    # An√°lise de missing rate na vers√£o 2
    missing_stats_v2 = {}
    for col in versao_2.columns:
        if col != 'Data':
            missing_count = versao_2[col].isnull().sum()
            missing_rate = (missing_count / len(versao_2)) * 100
            missing_stats_v2[col] = {
                'missing_count': missing_count,
                'missing_rate': missing_rate,
                'valid_count': len(versao_2) - missing_count
            }

    # Ordenar por taxa de missing
    missing_sorted_v2 = sorted(missing_stats_v2.items(), key=lambda x: x[1]['missing_rate'])

    print(f"\nTaxa de missing por coluna (ordenado):")
    print(f"{'COLUNA':<45} {'V√ÅLIDOS':<8} {'MISSING':<8} {'% MISS':<7}")
    print("-" * 70)

    for col, stats in missing_sorted_v2:
        print(f"{col[:42]:<45} {stats['valid_count']:<8,} {stats['missing_count']:<8,} {stats['missing_rate']:<7.1f}%")

    # Compara√ß√£o entre vers√µes
    print(f"\n" + "="*60)
    print("COMPARA√á√ÉO ENTRE VERS√ïES")
    print("="*60)

    # Missing rate m√©dio
    avg_missing_v1 = sum(stats['missing_rate'] for stats in missing_stats_v1.values()) / len(missing_stats_v1) if missing_stats_v1 else 0
    avg_missing_v2 = sum(stats['missing_rate'] for stats in missing_stats_v2.values()) / len(missing_stats_v2) if missing_stats_v2 else 0

    print(f"Vers√£o 1 (p√≥s-cutoff com features cr√≠ticas):")
    print(f"  Registros: {len(versao_1):,}")
    print(f"  Colunas: {len(versao_1.columns)}")
    print(f"  Missing rate m√©dio: {avg_missing_v1:.1f}%")

    print(f"\nVers√£o 2 (todos registros sem features cr√≠ticas):")
    print(f"  Registros: {len(versao_2):,}")
    print(f"  Colunas: {len(versao_2.columns)}")
    print(f"  Missing rate m√©dio: {avg_missing_v2:.1f}%")

    # An√°lise espec√≠fica das features cr√≠ticas na vers√£o 1
    if features_existentes:
        print(f"\nAn√°lise das features cr√≠ticas na Vers√£o 1:")
        for feature in features_existentes:
            if feature in versao_1.columns:
                missing_count = versao_1[feature].isnull().sum()
                missing_rate = (missing_count / len(versao_1)) * 100
                print(f"  {feature}: {missing_rate:.1f}% missing")

    return versao_1, versao_2

def salvar_versoes(versao_1, versao_2):
    """Salva as duas vers√µes do dataset"""

    print(f"\n" + "="*60)
    print("DISPONIBILIZA√á√ÉO DAS VERS√ïES")
    print("="*60)

    # Disponibilizar as vers√µes como vari√°veis globais
    globals()['pesquisa_v1_menor_missing'] = versao_1
    globals()['pesquisa_v2_sem_features_criticas'] = versao_2

    print(f"Vers√£o 1 dispon√≠vel em: pesquisa_v1_menor_missing")
    print(f"  Per√≠odo: 2025-02-11 em diante")
    print(f"  Todas as features mantidas")
    print(f"  Registros: {len(versao_1):,}")
    print(f"  Colunas: {len(versao_1.columns)}")

    print(f"\nVers√£o 2 dispon√≠vel em: pesquisa_v2_sem_features_criticas")
    print(f"  Per√≠odo: todos os registros")
    print(f"  Features cr√≠ticas removidas")
    print(f"  Registros: {len(versao_2):,}")
    print(f"  Colunas: {len(versao_2.columns)}")

# Executar cria√ß√£o das vers√µes
print("Iniciando cria√ß√£o das vers√µes...")
versao_1, versao_2 = criar_versoes_dataset()

# Salvar vers√µes
salvar_versoes(versao_1, versao_2)

print(f"\nProcesso conclu√≠do!")
print(f"Duas vers√µes do dataset criadas com sucesso.")

"""## 14- Investiga√ß√£o
Hip√≥tese: Os 5% de missing da pergunta "Tem computador/notebook?: 4.8% missing" existem porque a feature foi descontinuada.
Resultado: Refutada.
"""

import pandas as pd
import matplotlib.pyplot as plt
from difflib import get_close_matches

def analisar_computador_temporal():
    """Analisa a quantidade de registros da pergunta sobre computador/notebook semana a semana"""

    # Usar o dataset original completo
    df = pesquisa_medium_actions_unificado.copy()

    print("AN√ÅLISE TEMPORAL: Tem computador/notebook?")
    print("=" * 50)

    # Busca robusta da coluna
    target_keywords = ['computador', 'notebook', 'tem']
    coluna_encontrada = None

    # Buscar por palavras-chave nas colunas
    for col in df.columns:
        col_lower = col.lower()
        if all(keyword in col_lower for keyword in target_keywords):
            coluna_encontrada = col
            break

    # Se n√£o encontrou, usar busca por similaridade
    if not coluna_encontrada:
        target_phrase = "tem computador/notebook"
        matches = get_close_matches(target_phrase,
                                  [col.lower() for col in df.columns],
                                  n=1, cutoff=0.6)
        if matches:
            # Encontrar a coluna original correspondente
            for col in df.columns:
                if col.lower() == matches[0]:
                    coluna_encontrada = col
                    break

    if not coluna_encontrada:
        print("‚ùå Coluna sobre computador/notebook n√£o encontrada!")
        return

    print(f"‚úÖ Coluna encontrada: '{coluna_encontrada}'")

    # Converter Data para datetime
    df['Data'] = pd.to_datetime(df['Data'], errors='coerce')

    # Remover registros sem data
    df = df.dropna(subset=['Data'])

    print(f"üìä Dataset: {len(df):,} registros com data v√°lida")
    print(f"üìÖ Per√≠odo: {df['Data'].min().strftime('%Y-%m-%d')} a {df['Data'].max().strftime('%Y-%m-%d')}")

    # Criar coluna de semana
    df['Semana'] = df['Data'].dt.to_period('W-MON')  # Semana come√ßando na segunda

    # Agrupar por semana e contar registros v√°lidos (n√£o-nulos) da coluna alvo
    analise_semanal = df.groupby('Semana').agg({
        coluna_encontrada: ['count', lambda x: x.notna().sum()],
        'Data': 'count'
    }).round(0)

    # Simplificar nomes das colunas
    analise_semanal.columns = ['total_respostas', 'respostas_validas', 'total_registros']

    # Resetar index para facilitar visualiza√ß√£o
    analise_semanal = analise_semanal.reset_index()
    analise_semanal['Semana_str'] = analise_semanal['Semana'].astype(str)

    # Estat√≠sticas resumo
    print(f"\nüìà ESTAT√çSTICAS RESUMO:")
    print(f"Total de semanas analisadas: {len(analise_semanal)}")
    print(f"Semanas com respostas v√°lidas: {(analise_semanal['respostas_validas'] > 0).sum()}")
    print(f"Semanas SEM respostas v√°lidas: {(analise_semanal['respostas_validas'] == 0).sum()}")

    # Identificar per√≠odos de descontinuidade
    semanas_sem_dados = analise_semanal[analise_semanal['respostas_validas'] == 0]
    if len(semanas_sem_dados) > 0:
        print(f"\n‚ö†Ô∏è  PER√çODOS SEM DADOS:")
        for _, row in semanas_sem_dados.iterrows():
            print(f"  Semana {row['Semana_str']}: {row['total_registros']:.0f} registros totais, 0 respostas v√°lidas")

    # Criar gr√°fico
    plt.figure(figsize=(15, 8))

    # Converter semanas para posi√ß√µes num√©ricas para o gr√°fico
    x_pos = range(len(analise_semanal))

    plt.plot(x_pos, analise_semanal['respostas_validas'],
             marker='o', linewidth=2, markersize=4, color='#2E8B57')

    plt.title(f'Evolu√ß√£o Semanal: {coluna_encontrada}', fontsize=16, fontweight='bold')
    plt.xlabel('Semanas (cronol√≥gico)', fontsize=12)
    plt.ylabel('Quantidade de Respostas V√°lidas', fontsize=12)

    # Configurar eixo X com algumas datas de refer√™ncia
    step = max(1, len(analise_semanal) // 10)  # Mostrar ~10 labels
    x_labels = []
    x_ticks = []
    for i in range(0, len(analise_semanal), step):
        x_ticks.append(i)
        x_labels.append(analise_semanal.iloc[i]['Semana_str'])

    plt.xticks(x_ticks, x_labels, rotation=45, ha='right')

    # Adicionar grid
    plt.grid(True, alpha=0.3)

    # Destacar per√≠odos sem dados
    for i, (_, row) in enumerate(analise_semanal.iterrows()):
        if row['respostas_validas'] == 0:
            plt.axvline(x=i, color='red', alpha=0.3, linestyle='--')

    plt.tight_layout()
    plt.show()

    # AN√ÅLISE DE DESCONTINUIDADE
    print(f"\nüîç AN√ÅLISE DE DESCONTINUIDADE:")
    print("=" * 40)

    # Analisar as √∫ltimas 4 semanas com dados
    ultimas_semanas = analise_semanal.tail(4)

    # Verificar se h√° respostas v√°lidas nas √∫ltimas semanas
    semanas_com_dados = ultimas_semanas[ultimas_semanas['respostas_validas'] > 0]
    semanas_sem_dados = ultimas_semanas[ultimas_semanas['respostas_validas'] == 0]

    print(f"üìä √öLTIMAS 4 SEMANAS ANALISADAS:")
    for _, row in ultimas_semanas.iterrows():
        status = "‚úÖ ATIVA" if row['respostas_validas'] > 0 else "‚ùå INATIVA"
        taxa_resposta = (row['respostas_validas'] / row['total_registros'] * 100) if row['total_registros'] > 0 else 0
        print(f"  {row['Semana_str']}: {row['respostas_validas']:.0f}/{row['total_registros']:.0f} ({taxa_resposta:.1f}%) {status}")

    # An√°lise das √∫ltimas 2 semanas especificamente
    ultimas_2_semanas = analise_semanal.tail(2)
    respostas_ultimas_2 = ultimas_2_semanas['respostas_validas'].sum()
    total_ultimas_2 = ultimas_2_semanas['total_registros'].sum()

    print(f"\nüìà AN√ÅLISE DAS √öLTIMAS 2 SEMANAS:")
    print(f"Total de registros: {total_ultimas_2:.0f}")
    print(f"Respostas v√°lidas: {respostas_ultimas_2:.0f}")
    if total_ultimas_2 > 0:
        taxa_resposta_recente = (respostas_ultimas_2 / total_ultimas_2) * 100
        print(f"Taxa de resposta: {taxa_resposta_recente:.1f}%")

    # VEREDICTO FINAL
    print(f"\nüéØ VEREDICTO SOBRE DESCONTINUA√á√ÉO:")
    print("=" * 35)

    if respostas_ultimas_2 > 0 and total_ultimas_2 > 0:
        taxa_final = (respostas_ultimas_2 / total_ultimas_2) * 100
        if taxa_final > 80:
            print("‚úÖ PERGUNTA PROVAVELMENTE ATIVA")
            print(f"   Evid√™ncia: {taxa_final:.1f}% de taxa de resposta nas √∫ltimas 2 semanas")
            print(f"   Conclus√£o: O missing de 4.8% √© prov√°vel de ser por outros motivos")
        elif taxa_final > 50:
            print("üü° PERGUNTA PARCIALMENTE ATIVA")
            print(f"   Evid√™ncia: {taxa_final:.1f}% de taxa de resposta nas √∫ltimas 2 semanas")
            print(f"   Conclus√£o: Pode ter sido reintroduzida recentemente")
        else:
            print("üü† PERGUNTA COM BAIXA COLETA")
            print(f"   Evid√™ncia: Apenas {taxa_final:.1f}% de taxa de resposta")
    else:
        print("‚ùå PERGUNTA PROVAVELMENTE DESCONTINUADA")
        print("   Evid√™ncia: 0 respostas v√°lidas nas √∫ltimas 2 semanas")
        print("   Conclus√£o: O missing de 4.8% √© por descontinua√ß√£o")

    # An√°lise do missing no per√≠odo p√≥s-cutoff
    df_pos_cutoff = df[df['Data'] >= pd.to_datetime('2025-03-01')]
    total_pos_cutoff = len(df_pos_cutoff)
    validas_pos_cutoff = df_pos_cutoff[coluna_encontrada].notna().sum()
    missing_pos_cutoff = total_pos_cutoff - validas_pos_cutoff
    taxa_missing_pos_cutoff = (missing_pos_cutoff / total_pos_cutoff) * 100

    print(f"\nüìä CONTEXTO DO MISSING (p√≥s 2025-03-01):")
    print(f"Total de registros: {total_pos_cutoff:,}")
    print(f"Respostas v√°lidas: {validas_pos_cutoff:,}")
    print(f"Missing: {missing_pos_cutoff:,} ({taxa_missing_pos_cutoff:.1f}%)")

    return analise_semanal

# Executar an√°lise
resultado = analisar_computador_temporal()

"""## 15- Matching por e-mail"""

import pandas as pd

def normalizar_telefone_completo(telefone):
    """Normaliza telefone brasileiro e cria todas as variantes poss√≠veis"""
    if pd.isna(telefone):
        return set()

    # Extrair apenas d√≠gitos
    digitos = ''.join(filter(str.isdigit, str(telefone)))

    if len(digitos) < 8:  # Muito curto para ser v√°lido
        return set()

    variantes = set()

    # Remover c√≥digo do pa√≠s (55) se presente
    if digitos.startswith('55') and len(digitos) > 10:
        digitos_sem_pais = digitos[2:]
    else:
        digitos_sem_pais = digitos

    # Casos baseados no comprimento ap√≥s remover c√≥digo do pa√≠s
    if len(digitos_sem_pais) == 11:  # Formato: DDD + 9 + 8 d√≠gitos
        variantes.add(digitos_sem_pais)  # 37999610179
        variantes.add(digitos_sem_pais[3:])  # 99610179 (sem DDD)
        # Formato antigo (sem o 9)
        if digitos_sem_pais[2] == '9':
            formato_antigo = digitos_sem_pais[:2] + digitos_sem_pais[3:]  # 3799610179
            variantes.add(formato_antigo)
            variantes.add(formato_antigo[2:])  # 99610179 (sem DDD, sem 9)

    elif len(digitos_sem_pais) == 10:  # Formato: DDD + 8 d√≠gitos (antigo)
        variantes.add(digitos_sem_pais)  # 3799610179
        variantes.add(digitos_sem_pais[2:])  # 99610179 (sem DDD)
        # Formato novo (com o 9)
        if digitos_sem_pais[2] in ['8', '9']:
            formato_novo = digitos_sem_pais[:2] + '9' + digitos_sem_pais[2:]  # 37999610179
            variantes.add(formato_novo)

    elif len(digitos_sem_pais) == 9:  # Formato: 9 + 8 d√≠gitos (sem DDD)
        variantes.add(digitos_sem_pais)  # 999610179
        # Formato antigo (sem o 9)
        if digitos_sem_pais[0] == '9':
            formato_antigo = digitos_sem_pais[1:]  # 99610179
            variantes.add(formato_antigo)

    elif len(digitos_sem_pais) == 8:  # Formato: 8 d√≠gitos (sem DDD, sem 9)
        variantes.add(digitos_sem_pais)  # 99610179
        # Formato novo (com o 9)
        if digitos_sem_pais[0] in ['8', '9']:
            formato_novo = '9' + digitos_sem_pais  # 999610179
            variantes.add(formato_novo)

    # Remover variantes muito curtas ou inv√°lidas
    variantes_validas = {v for v in variantes if len(v) >= 8}

    return variantes_validas

def normalizar_email(email):
    """Normaliza email para matching"""
    if pd.isna(email):
        return None

    email_str = str(email).strip().lower()

    # Verificar se √© um email v√°lido b√°sico
    if '@' in email_str and email_str != 'nan' and len(email_str) > 5:
        return email_str

    return None

def fazer_matching_robusto():
    """Faz matching robusto por email E telefone"""

    print("MATCHING ROBUSTO POR EMAIL E TELEFONE")
    print("=" * 50)

    # Copiar datasets
    df_pesquisa_v1 = pesquisa_v1_menor_missing.copy()
    df_pesquisa_v2 = pesquisa_v2_sem_features_criticas.copy()
    df_vendas = vendas_unificado.copy()

    def processar_dataset(df_pesquisa, nome_versao):
        """Processa um dataset de pesquisa e adiciona target"""

        print(f"\nProcessando {nome_versao}...")

        # 1. NORMALIZAR EMAILS
        emails_pesquisa = {}
        for idx, email in df_pesquisa['E-mail'].items():
            email_norm = normalizar_email(email)
            if email_norm:
                emails_pesquisa[idx] = email_norm

        emails_vendas = set()
        for email in df_vendas['email']:
            email_norm = normalizar_email(email)
            if email_norm:
                emails_vendas.add(email_norm)

        print(f"  Emails √∫nicos na pesquisa: {len(emails_pesquisa):,}")
        print(f"  Emails √∫nicos nas vendas: {len(emails_vendas):,}")

        # 2. NORMALIZAR TELEFONES
        telefones_pesquisa = {}
        for idx, telefone in df_pesquisa['Telefone'].items():
            variantes = normalizar_telefone_completo(telefone)
            if variantes:
                telefones_pesquisa[idx] = variantes

        telefones_vendas = set()
        for telefone in df_vendas['telefone']:
            variantes = normalizar_telefone_completo(telefone)
            telefones_vendas.update(variantes)

        print(f"  Telefones v√°lidos na pesquisa: {len(telefones_pesquisa):,}")
        print(f"  Variantes de telefone nas vendas: {len(telefones_vendas):,}")

        # 3. FAZER MATCHING
        matches_email = set()
        matches_telefone = set()

        # Matching por email
        for idx, email in emails_pesquisa.items():
            if email in emails_vendas:
                matches_email.add(idx)

        # Matching por telefone
        for idx, variantes in telefones_pesquisa.items():
            if variantes & telefones_vendas:  # Interse√ß√£o de conjuntos
                matches_telefone.add(idx)

        # 4. CRIAR TARGET
        df_resultado = df_pesquisa.copy()
        df_resultado['target'] = 0

        # Marcar matches
        for idx in matches_email | matches_telefone:  # Uni√£o dos conjuntos
            df_resultado.loc[idx, 'target'] = 1

        # 5. ESTAT√çSTICAS
        total_registros = len(df_resultado)
        total_matches = df_resultado['target'].sum()
        matches_apenas_email = len(matches_email - matches_telefone)
        matches_apenas_telefone = len(matches_telefone - matches_email)
        matches_ambos = len(matches_email & matches_telefone)
        taxa_conversao = (total_matches / total_registros) * 100

        print(f"  Total de registros: {total_registros:,}")
        print(f"  Total de matches: {total_matches:,}")
        print(f"  Taxa de convers√£o: {taxa_conversao:.2f}%")
        print(f"  Matches apenas por email: {matches_apenas_email:,}")
        print(f"  Matches apenas por telefone: {matches_apenas_telefone:,}")
        print(f"  Matches por ambos: {matches_ambos:,}")

        return df_resultado

    # Processar ambas as vers√µes
    dataset_v1_final = processar_dataset(df_pesquisa_v1, "DATASET V1")
    dataset_v2_final = processar_dataset(df_pesquisa_v2, "DATASET V2")

    print(f"\n" + "=" * 50)
    print("DATASETS FINAIS CRIADOS!")
    print(f"dataset_v1_final: {len(dataset_v1_final):,} registros, {len(dataset_v1_final.columns)} colunas")
    print(f"dataset_v2_final: {len(dataset_v2_final):,} registros, {len(dataset_v2_final.columns)} colunas")
    print("Cada dataset cont√©m apenas colunas originais + target")

    # Listar vari√°veis mantidas em cada dataset
    print(f"\nüìã VARI√ÅVEIS DO DATASET V1 ({len(dataset_v1_final.columns)} colunas):")
    for i, col in enumerate(dataset_v1_final.columns, 1):
        print(f"  {i:2d}. {col}")

    print(f"\nüìã VARI√ÅVEIS DO DATASET V2 ({len(dataset_v2_final.columns)} colunas):")
    for i, col in enumerate(dataset_v2_final.columns, 1):
        print(f"  {i:2d}. {col}")

    # Mostrar diferen√ßas entre os datasets
    colunas_v1 = set(dataset_v1_final.columns)
    colunas_v2 = set(dataset_v2_final.columns)

    colunas_apenas_v1 = colunas_v1 - colunas_v2
    colunas_apenas_v2 = colunas_v2 - colunas_v1

    if colunas_apenas_v1:
        print(f"\nüîµ COLUNAS APENAS NO DATASET V1:")
        for col in sorted(colunas_apenas_v1):
            print(f"  ‚Ä¢ {col}")

    if colunas_apenas_v2:
        print(f"\nüü° COLUNAS APENAS NO DATASET V2:")
        for col in sorted(colunas_apenas_v2):
            print(f"  ‚Ä¢ {col}")

    if not colunas_apenas_v1 and not colunas_apenas_v2:
        print(f"\n‚úÖ Ambos os datasets possuem as mesmas colunas")

    return dataset_v1_final, dataset_v2_final

# Executar matching robusto
dataset_v1_final, dataset_v2_final = fazer_matching_robusto()

"""##16- Investiga√ß√£o
Representatividade da vari√°vel alvo por produto por per√≠odo temporal
"""

import pandas as pd
import matplotlib.pyplot as plt

def analisar_target_existente():
    """Analisa o target j√° criado pelo matching para entender a degrada√ß√£o temporal"""

    print("AN√ÅLISE DO TARGET EXISTENTE (MATCHING ROBUSTO)")
    print("=" * 50)

    # 1. ESTAT√çSTICAS B√ÅSICAS DOS DATASETS
    print("\n1. ESTAT√çSTICAS DOS DATASETS AP√ìS MATCHING")
    print("=" * 45)

    datasets = [
        (dataset_v1_final, "DATASET V1"),
        (dataset_v2_final, "DATASET V2")
    ]

    for df_dataset, nome in datasets:
        total_registros = len(df_dataset)
        targets_positivos = df_dataset['target'].sum()
        taxa_conversao = (targets_positivos / total_registros * 100)

        print(f"\n{nome}:")
        print(f"  Total registros: {total_registros:,}")
        print(f"  Targets positivos: {targets_positivos:,}")
        print(f"  Taxa convers√£o: {taxa_conversao:.2f}%")
        print(f"  Per√≠odo: {df_dataset['Data'].min()} a {df_dataset['Data'].max()}")

    # 2. EVOLU√á√ÉO TEMPORAL DO TARGET
    print("\n2. EVOLU√á√ÉO TEMPORAL DO TARGET")
    print("=" * 35)

    for df_dataset, nome in datasets:
        print(f"\n{nome}:")

        df_temp = df_dataset.copy()
        df_temp['Data'] = pd.to_datetime(df_temp['Data'], errors='coerce')
        df_temp['mes_ano'] = df_temp['Data'].dt.strftime('%Y-%m')

        # An√°lise mensal
        analise_mensal = df_temp.groupby('mes_ano').agg({
            'target': ['count', 'sum', 'mean']
        }).round(4)

        analise_mensal.columns = ['total_leads', 'targets_positivos', 'taxa_conversao']

        print(f"{'M√äS':<10} {'LEADS':<8} {'TARGETS':<8} {'TAXA %':<8}")
        print("-" * 40)

        for mes, row in analise_mensal.iterrows():
            taxa_pct = row['taxa_conversao'] * 100
            print(f"{mes:<10} {row['total_leads']:<8.0f} {row['targets_positivos']:<8.0f} {taxa_pct:<8.2f}")

    # 3. AN√ÅLISE DOS EMAILS QUE FIZERAM MATCH
    print("\n3. AN√ÅLISE DOS EMAILS QUE FIZERAM MATCH")
    print("=" * 45)

    # Identificar quais emails fizeram match
    df_vendas = vendas_unificado.copy()

    def normalizar_email(email):
        if pd.isna(email):
            return None
        email_str = str(email).strip().lower()
        if '@' in email_str and email_str != 'nan' and len(email_str) > 5:
            return email_str
        return None

    df_vendas['email_clean'] = df_vendas['email'].apply(normalizar_email)
    emails_compradores = set(df_vendas['email_clean'].dropna())

    print(f"Total de emails √∫nicos nas vendas: {len(emails_compradores):,}")

    # Analisar quais produtos compraram os emails que fizeram match
    produtos_por_email = df_vendas.groupby('email_clean')['produto'].apply(list).to_dict()

    # Para cada dataset, ver quais produtos compraram os targets positivos
    for df_dataset, nome in datasets:
        print(f"\n{nome} - Produtos comprados pelos targets positivos:")

        df_temp = df_dataset.copy()
        df_temp['email_clean'] = df_temp['E-mail'].apply(normalizar_email)

        targets_positivos = df_temp[df_temp['target'] == 1]['email_clean'].dropna()

        produtos_targets = []
        for email in targets_positivos:
            if email in produtos_por_email:
                produtos_targets.extend(produtos_por_email[email])

        produtos_count = pd.Series(produtos_targets).value_counts()

        print(f"{'PRODUTO':<40} {'MATCHES':<8} {'%':<6}")
        print("-" * 60)

        total_matches_produtos = len(produtos_targets)
        for produto, count in produtos_count.items():
            pct = (count / total_matches_produtos * 100)
            produto_nome = produto[:38] if len(produto) > 38 else produto
            print(f"{produto_nome:<40} {count:<8} {pct:<6.1f}")

    # 4. EVOLU√á√ÉO TEMPORAL POR PRODUTO NAS VENDAS
    print("\n4. EVOLU√á√ÉO TEMPORAL DAS VENDAS POR PRODUTO")
    print("=" * 50)

    # Converter data das vendas
    if 'data' in df_vendas.columns:
        coluna_data = 'data'
    else:
        colunas_data = [col for col in df_vendas.columns if 'data' in col.lower()]
        coluna_data = colunas_data[0] if colunas_data else None

    if coluna_data:
        df_vendas[coluna_data] = pd.to_datetime(df_vendas[coluna_data], errors='coerce')
        df_vendas = df_vendas[df_vendas[coluna_data].notna()].copy()
        df_vendas['mes_ano'] = df_vendas[coluna_data].dt.strftime('%Y-%m')

        # Vendas por m√™s e produto
        vendas_mensais = df_vendas.groupby(['mes_ano', 'produto']).size().unstack(fill_value=0)

        # Produtos principais
        produtos_principais = df_vendas['produto'].value_counts().head(6).index.tolist()

        print(f"{'M√äS':<10} " + " ".join([f"{p[:8]:<9}" for p in produtos_principais]))
        print("-" * 70)

        for mes in sorted(vendas_mensais.index):
            linha = f"{mes:<10} "
            for produto in produtos_principais:
                qtd = vendas_mensais.loc[mes, produto] if produto in vendas_mensais.columns else 0
                linha += f"{qtd:<9}"
            print(linha)

    # 5. DIAGN√ìSTICO DA DEGRADA√á√ÉO TEMPORAL
    print("\n5. DIAGN√ìSTICO DA DEGRADA√á√ÉO TEMPORAL")
    print("=" * 45)

    for df_dataset, nome in datasets:
        df_temp = df_dataset.copy()
        df_temp['Data'] = pd.to_datetime(df_temp['Data'], errors='coerce')
        df_temp['mes_ano'] = df_temp['Data'].dt.strftime('%Y-%m')

        analise_mensal = df_temp.groupby('mes_ano')['target'].agg(['count', 'sum', 'mean'])

        if len(analise_mensal) >= 2:
            taxa_inicial = analise_mensal['mean'].iloc[0] * 100
            taxa_final = analise_mensal['mean'].iloc[-1] * 100
            variacao = ((taxa_final - taxa_inicial) / taxa_inicial) * 100 if taxa_inicial > 0 else 0

            print(f"\n{nome}:")
            print(f"  Taxa inicial: {taxa_inicial:.2f}% ({analise_mensal.index[0]})")
            print(f"  Taxa final: {taxa_final:.2f}% ({analise_mensal.index[-1]})")
            print(f"  Varia√ß√£o: {variacao:+.1f}%")

            if abs(variacao) > 30:
                status = "QUEDA CR√çTICA"
            elif abs(variacao) > 15:
                status = "QUEDA SIGNIFICATIVA"
            elif abs(variacao) > 5:
                status = "QUEDA MODERADA"
            else:
                status = "EST√ÅVEL"

            print(f"  Status: {status}")

    # 6. CONCLUS√ÉO SOBRE O TARGET
    print("\n6. CONCLUS√ÉO SOBRE O TARGET ATUAL")
    print("=" * 35)

    print("O target atual (criado pelo matching robusto):")
    print("- Inclui TODOS os produtos das vendas")
    print("- Usa matching por email E telefone")
    print("- Mostra degrada√ß√£o temporal real")
    print()
    print("A degrada√ß√£o observada nos modelos √© explicada por:")
    print("- Mudan√ßa no mix de produtos ao longo do tempo")
    print("- Produtos desativados dominaram per√≠odo inicial")
    print("- Target atual captura essa transi√ß√£o real")
    print()
    print("RECOMENDA√á√ÉO:")
    print("- Target atual est√° correto para an√°lise geral")
    print("- Considerar features temporais nos modelos")
    print("- Avaliar re-treino peri√≥dico conforme evolu√ß√£o dos produtos")

# Executar an√°lise
analisar_target_existente()

"""## 16.1- Investiga√ß√£o
Do que se tratam as convers√µes nos meses que ainda n√£o chegaram
"""

import pandas as pd
from datetime import datetime

def validar_targets_temporais():
    """Valida targets identificando convers√µes com datas futuras ou inv√°lidas"""

    print("VALIDA√á√ÉO TEMPORAL DOS TARGETS")
    print("=" * 35)

    # Data atual para refer√™ncia
    data_atual = datetime.now().date()
    print(f"Data atual: {data_atual}")

    # 1. AN√ÅLISE DO DATASET DE VENDAS
    print(f"\n1. AN√ÅLISE TEMPORAL DO DATASET DE VENDAS")
    print("=" * 45)

    df_vendas = vendas_unificado.copy()

    # Identificar coluna de data
    if 'data' in df_vendas.columns:
        coluna_data = 'data'
    else:
        colunas_data = [col for col in df_vendas.columns if 'data' in col.lower()]
        coluna_data = colunas_data[0] if colunas_data else None

    if not coluna_data:
        print("Nenhuma coluna de data encontrada no dataset de vendas")
        return

    # Converter e analisar datas
    df_vendas[coluna_data] = pd.to_datetime(df_vendas[coluna_data], errors='coerce')
    df_vendas_validas = df_vendas[df_vendas[coluna_data].notna()].copy()
    df_vendas_validas['data_apenas'] = df_vendas_validas[coluna_data].dt.date

    # Identificar vendas futuras
    vendas_futuras = df_vendas_validas[df_vendas_validas['data_apenas'] > data_atual]
    vendas_passadas = df_vendas_validas[df_vendas_validas['data_apenas'] <= data_atual]

    print(f"Total de vendas: {len(df_vendas_validas):,}")
    print(f"Vendas no passado/presente: {len(vendas_passadas):,}")
    print(f"Vendas no futuro: {len(vendas_futuras):,}")
    print(f"Percentual futuras: {len(vendas_futuras)/len(df_vendas_validas)*100:.2f}%")

    if len(vendas_futuras) > 0:
        print(f"\nVendas futuras por m√™s:")
        vendas_futuras_mes = vendas_futuras.groupby(vendas_futuras[coluna_data].dt.strftime('%Y-%m')).size()
        for mes, count in vendas_futuras_mes.items():
            print(f"  {mes}: {count:,} vendas")

    # 2. IMPACTO NOS TARGETS DOS DATASETS
    print(f"\n2. IMPACTO NOS TARGETS DOS DATASETS")
    print("=" * 40)

    def normalizar_email(email):
        if pd.isna(email):
            return None
        email_str = str(email).strip().lower()
        if '@' in email_str and email_str != 'nan' and len(email_str) > 5:
            return email_str
        return None

    # CRIAR colunas email_clean para an√°lise
    vendas_futuras_copy = vendas_futuras.copy()
    vendas_passadas_copy = vendas_passadas.copy()

    vendas_futuras_copy['email_clean'] = vendas_futuras_copy['email'].apply(normalizar_email)
    vendas_passadas_copy['email_clean'] = vendas_passadas_copy['email'].apply(normalizar_email)

    # Emails de vendas futuras vs passadas
    emails_futuras = set(vendas_futuras_copy['email_clean'].dropna())
    emails_passadas = set(vendas_passadas_copy['email_clean'].dropna())
    emails_apenas_futuras = emails_futuras - emails_passadas

    print(f"Emails √∫nicos em vendas futuras: {len(emails_futuras):,}")
    print(f"Emails √∫nicos em vendas passadas: {len(emails_passadas):,}")
    print(f"Emails APENAS em vendas futuras: {len(emails_apenas_futuras):,}")

    # Analisar impacto em cada dataset
    datasets = [
        (dataset_v1_final, "DATASET V1"),
        (dataset_v2_final, "DATASET V2")
    ]

    for df_dataset, nome_dataset in datasets:
        print(f"\n{nome_dataset}:")

        df_temp = df_dataset.copy()
        df_temp['email_clean'] = df_temp['E-mail'].apply(normalizar_email)

        # Targets atuais
        targets_atuais = df_temp['target'].sum()

        # Targets que s√£o apenas de vendas futuras
        targets_apenas_futuras = df_temp['email_clean'].isin(emails_apenas_futuras).sum()

        # Targets que t√™m vendas passadas (v√°lidos)
        targets_validos = df_temp['email_clean'].isin(emails_passadas).sum()

        print(f"  Targets atuais: {targets_atuais:,}")
        print(f"  Targets v√°lidos (vendas passadas): {targets_validos:,}")
        print(f"  Targets apenas futuras: {targets_apenas_futuras:,}")
        print(f"  Diferen√ßa: {targets_atuais - targets_validos:,}")

        # Taxa de convers√£o corrigida
        taxa_atual = (targets_atuais / len(df_temp) * 100)
        taxa_corrigida = (targets_validos / len(df_temp) * 100)

        print(f"  Taxa atual: {taxa_atual:.2f}%")
        print(f"  Taxa corrigida: {taxa_corrigida:.2f}%")
        print(f"  Impacto: {taxa_atual - taxa_corrigida:+.2f}pp")

    # 3. VERIFICA√á√ÉO DE VALORES
    print(f"\n3. VERIFICA√á√ÉO DE VALORES DAS VENDAS FUTURAS")
    print("=" * 50)

    if len(vendas_futuras) > 0:
        print(f"Estat√≠sticas dos valores das vendas futuras:")
        print(f"  Contagem: {len(vendas_futuras):,}")
        print(f"  Valor m√©dio: R$ {vendas_futuras['valor'].mean():,.2f}")
        print(f"  Valor mediano: R$ {vendas_futuras['valor'].median():,.2f}")
        print(f"  Valor total: R$ {vendas_futuras['valor'].sum():,.2f}")
        print(f"  Valor m√≠nimo: R$ {vendas_futuras['valor'].min():,.2f}")
        print(f"  Valor m√°ximo: R$ {vendas_futuras['valor'].max():,.2f}")

        print(f"\nProdutos das vendas futuras:")
        produtos_futuros = vendas_futuras['produto'].value_counts()
        for produto, count in produtos_futuros.head(10).items():
            produto_nome = produto[:40] if len(produto) > 40 else produto
            print(f"  {produto_nome:<42} {count:>4,}")

    # 4. RECOMENDA√á√ÉO
    print(f"\n4. RECOMENDA√á√ÉO")
    print("=" * 15)

    if len(vendas_futuras) > 0:
        print("PROBLEMA IDENTIFICADO:")
        print("- Existem vendas com datas futuras no dataset")
        print("- Isso pode estar inflando artificialmente os targets")
        print("- Modelos podem estar aprendendo padr√µes inconsistentes")
        print()
        print("A√á√ïES NECESS√ÅRIAS:")
        print("1. Filtrar vendas apenas at√© a data atual")
        print("2. Refazer o matching com dataset de vendas filtrado")
        print("3. Re-treinar modelos com targets corrigidos")
        print("4. Investigar origem das datas futuras")
    else:
        print("Nenhuma venda futura identificada - targets est√£o corretos")

    return vendas_futuras_copy, vendas_passadas_copy

# Executar valida√ß√£o
vendas_futuras_resultado, vendas_passadas_resultado = validar_targets_temporais()

"""## 16.2- Investiga√ß√£o
Se houve mudan√ßa significativa nas caracter√≠sticas dos leads num per√≠odo que coincida com o per√≠odo da queda na taxa de convers√£o
"""

import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
import numpy as np

def analisar_mudanca_perfil_leads_cutoff():
    """Analisa mudan√ßas nas caracter√≠sticas dos leads com datasets filtrados por cutoff"""

    print("AN√ÅLISE DE MUDAN√áA NO PERFIL DOS LEADS - CUTOFF")
    print("=" * 50)

    # Datasets para an√°lise (com cutoff)
    datasets = [
        (dataset_v1_cutoff, "DATASET V1 CUTOFF"),
        (dataset_v2_cutoff, "DATASET V2 CUTOFF")
    ]

    # Vari√°veis a ignorar
    variaveis_ignorar = ['aba_origem', 'arquivo_origem', 'Data', 'mes_ano', 'periodo',
                        'target', 'Nome Completo', 'E-mail', 'Telefone']

    for df_original, nome_dataset in datasets:
        print(f"\n{'='*60}")
        print(f"AN√ÅLISE - {nome_dataset}")
        print(f"{'='*60}")

        df = df_original.copy()

        # Converter data e criar per√≠odos
        df['Data'] = pd.to_datetime(df['Data'], errors='coerce')
        df = df[df['Data'].notna()].copy()
        df['mes_ano'] = df['Data'].dt.strftime('%Y-%m')
        df['periodo'] = df['Data'].dt.to_period('M')

        print(f"Registros analisados: {len(df):,}")
        print(f"Per√≠odo: {df['Data'].min().date()} a {df['Data'].max().date()}")

        # Identificar colunas categ√≥ricas (excluindo as ignoradas)
        colunas_categoricas = []
        for col in df.columns:
            if col not in variaveis_ignorar:
                if df[col].dtype == 'object' or df[col].nunique() < 20:
                    colunas_categoricas.append(col)

        print(f"Colunas categ√≥ricas analisadas: {len(colunas_categoricas)}")

        # 1. AN√ÅLISE DE ESTABILIDADE TEMPORAL
        print(f"\n1. ESTABILIDADE TEMPORAL - {nome_dataset}")
        print("=" * 45)

        resultados_estabilidade = {}

        for coluna in colunas_categoricas:
            # Criar tabela de conting√™ncia temporal
            tabela_temporal = pd.crosstab(df['periodo'], df[coluna], normalize='index') * 100

            if len(tabela_temporal) >= 3 and len(tabela_temporal.columns) >= 2:
                # Calcular coeficiente de varia√ß√£o das propor√ß√µes
                cv_medio = tabela_temporal.std(axis=0).mean() / tabela_temporal.mean(axis=0).mean()

                # Teste qui-quadrado para independ√™ncia temporal
                tabela_counts = pd.crosstab(df['periodo'], df[coluna])
                try:
                    chi2, p_valor, _, _ = chi2_contingency(tabela_counts)
                    significativo = p_valor < 0.05
                except:
                    chi2, p_valor, significativo = np.nan, np.nan, False

                resultados_estabilidade[coluna] = {
                    'cv_medio': cv_medio,
                    'p_valor': p_valor,
                    'significativo': significativo,
                    'categorias': len(tabela_temporal.columns)
                }

        # Ordenar por instabilidade
        variaveis_instabilidade = sorted(resultados_estabilidade.items(),
                                       key=lambda x: x[1]['cv_medio'], reverse=True)

        print(f"{'VARI√ÅVEL':<30} {'CV_M√âDIO':<10} {'P_VALOR':<10} {'INST√ÅVEL':<10}")
        print("-" * 65)

        for variavel, stats in variaveis_instabilidade:
            cv = stats['cv_medio']
            p_val = stats['p_valor']
            instavel = "SIM" if stats['significativo'] else "N√ÉO"
            variavel_nome = variavel[:28] if len(variavel) > 28 else variavel

            p_str = f"{p_val:.3f}" if not pd.isna(p_val) else "N/A"
            print(f"{variavel_nome:<30} {cv:<10.3f} {p_str:<10} {instavel:<10}")

        # 2. AN√ÅLISE DETALHADA DAS TOP 3 VARI√ÅVEIS INST√ÅVEIS
        print(f"\n2. TOP 3 VARI√ÅVEIS INST√ÅVEIS - {nome_dataset}")
        print("=" * 50)

        top_variaveis_inst√°veis = []
        for var_nome, stats in variaveis_instabilidade[:3]:
            if stats['significativo']:
                top_variaveis_inst√°veis.append(var_nome)

        for variavel in top_variaveis_inst√°veis:
            print(f"\n{variavel}:")
            print("-" * min(len(variavel), 40))

            # Evolu√ß√£o temporal das propor√ß√µes
            tabela_prop = pd.crosstab(df['mes_ano'], df[variavel], normalize='index') * 100

            # Mostrar apenas categorias principais
            categorias_principais = tabela_prop.sum().nlargest(3).index
            tabela_filtrada = tabela_prop[categorias_principais]

            print(f"{'M√äS':<10} " + " ".join([f"{str(cat)[:8]:<9}" for cat in categorias_principais]))
            print("-" * (10 + 10 * len(categorias_principais)))

            for mes in tabela_filtrada.index:
                linha = f"{mes:<10} "
                for cat in categorias_principais:
                    valor = tabela_filtrada.loc[mes, cat]
                    linha += f"{valor:<9.1f}"
                print(linha)

        # 3. GR√ÅFICOS DE TEND√äNCIAS
        print(f"\n3. GR√ÅFICOS DE TEND√äNCIAS - {nome_dataset}")
        print("=" * 45)

        # Criar figura com subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'Evolu√ß√£o Temporal das Caracter√≠sticas - {nome_dataset}', fontsize=16)

        # Gr√°fico 1: Taxa de convers√£o
        conversao_temporal = df.groupby('mes_ano')['target'].agg(['count', 'sum', 'mean'])
        axes[0, 0].plot(conversao_temporal.index, conversao_temporal['mean'] * 100,
                       marker='o', linewidth=2, color='red')
        axes[0, 0].set_title('Taxa de Convers√£o ao Longo do Tempo')
        axes[0, 0].set_ylabel('Taxa de Convers√£o (%)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)

        # Gr√°ficos 2, 3, 4: Top 3 vari√°veis inst√°veis
        for idx, variavel in enumerate(top_variaveis_inst√°veis[:3]):
            ax = axes[0, 1] if idx == 0 else axes[1, 0] if idx == 1 else axes[1, 1]

            tabela_prop = pd.crosstab(df['mes_ano'], df[variavel], normalize='index') * 100
            categorias_top = tabela_prop.sum().nlargest(3).index

            for cat in categorias_top:
                ax.plot(tabela_prop.index, tabela_prop[cat],
                       marker='o', label=str(cat)[:15], linewidth=2)

            ax.set_title(f'{variavel[:25]}')
            ax.set_ylabel('Propor√ß√£o (%)')
            ax.tick_params(axis='x', rotation=45)
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # 4. DRIVERS DE MUDAN√áA
        print(f"\n4. DRIVERS DE MUDAN√áA - {nome_dataset}")
        print("=" * 40)

        # Per√≠odos para compara√ß√£o
        meses_iniciais = sorted(df['mes_ano'].unique())[:3]
        meses_finais = sorted(df['mes_ano'].unique())[-3:]

        periodo_inicial = df[df['mes_ano'].isin(meses_iniciais)]
        periodo_final = df[df['mes_ano'].isin(meses_finais)]

        drivers_mudanca = []

        for variavel in top_variaveis_inst√°veis:
            # Comparar distribui√ß√µes
            dist_inicial = periodo_inicial[variavel].value_counts(normalize=True) * 100
            dist_final = periodo_final[variavel].value_counts(normalize=True) * 100

            # Encontrar categoria com maior mudan√ßa
            categorias_comuns = set(dist_inicial.index) & set(dist_final.index)

            for categoria in categorias_comuns:
                mudanca = dist_final.get(categoria, 0) - dist_inicial.get(categoria, 0)

                if abs(mudanca) > 5:  # Mudan√ßa > 5pp
                    drivers_mudanca.append({
                        'variavel': variavel,
                        'categoria': categoria,
                        'mudanca': mudanca,
                        'inicial': dist_inicial.get(categoria, 0),
                        'final': dist_final.get(categoria, 0)
                    })

        # Ordenar por magnitude da mudan√ßa
        drivers_mudanca.sort(key=lambda x: abs(x['mudanca']), reverse=True)

        print(f"Mudan√ßas significativas (>5pp):")
        print(f"{'VARI√ÅVEL':<25} {'CATEGORIA':<15} {'INICIAL':<8} {'FINAL':<8} {'MUDAN√áA':<8}")
        print("-" * 75)

        for driver in drivers_mudanca:
            var_nome = driver['variavel'][:23] if len(driver['variavel']) > 23 else driver['variavel']
            cat_nome = str(driver['categoria'])[:13] if len(str(driver['categoria'])) > 13 else str(driver['categoria'])

            print(f"{var_nome:<25} {cat_nome:<15} {driver['inicial']:<8.1f} {driver['final']:<8.1f} {driver['mudanca']:<+8.1f}")

        # 5. CONCLUS√ÉO POR DATASET
        print(f"\n5. CONCLUS√ÉO - {nome_dataset}")
        print("=" * 30)

        num_variaveis_inst√°veis = sum(1 for _, stats in resultados_estabilidade.items()
                                     if stats['significativo'])

        taxa_inicial = df[df['mes_ano'] == sorted(df['mes_ano'].unique())[0]]['target'].mean() * 100
        taxa_final = df[df['mes_ano'] == sorted(df['mes_ano'].unique())[-1]]['target'].mean() * 100
        variacao_conversao = ((taxa_final - taxa_inicial) / taxa_inicial) * 100 if taxa_inicial > 0 else 0

        print(f"Vari√°veis inst√°veis: {num_variaveis_inst√°veis}/{len(colunas_categoricas)}")
        print(f"Drivers de mudan√ßa: {len(drivers_mudanca)}")
        print(f"Varia√ß√£o na convers√£o: {variacao_conversao:+.1f}%")

        if num_variaveis_inst√°veis > len(colunas_categoricas) * 0.5:
            print("Status: MUDAN√áA SIGNIFICATIVA detectada")
        else:
            print("Status: Mudan√ßas MODERADAS detectadas")

# Executar an√°lise
analisar_mudanca_perfil_leads_cutoff()



"""## 16.3- Investiga√ß√£o
Padr√µes temporais para dias da semana
"""

import pandas as pd
import matplotlib.pyplot as plt

def analisar_padroes_dia_semana():
    """Analisa padr√µes de cadastro e convers√£o por dia da semana"""

    print("AN√ÅLISE DE PADR√ïES POR DIA DA SEMANA")
    print("=" * 45)

    # Usar dataset V2 para ter mais dados
    df = dataset_v2_devclub.copy()
    df['Data'] = pd.to_datetime(df['Data'], errors='coerce')
    df_com_data = df.dropna(subset=['Data'])

    print(f"Total de registros com data v√°lida: {len(df_com_data):,}")
    print(f"Per√≠odo analisado: {df_com_data['Data'].min().strftime('%Y-%m-%d')} a {df_com_data['Data'].max().strftime('%Y-%m-%d')}")

    # Criar dia da semana
    df_com_data['dia_semana'] = df_com_data['Data'].dt.dayofweek
    df_com_data['nome_dia'] = df_com_data['Data'].dt.day_name()

    # 1. AN√ÅLISE DE VOLUME DE CADASTROS
    print(f"\n1. DISTRIBUI√á√ÉO DE VOLUME POR DIA:")
    print("(0=Segunda, 1=Ter√ßa, 2=Quarta, 3=Quinta, 4=Sexta, 5=S√°bado, 6=Domingo)")
    print()

    contagem_dias = df_com_data.groupby(['dia_semana', 'nome_dia']).size().reset_index(name='quantidade')
    contagem_dias = contagem_dias.sort_values('dia_semana')

    total_registros = contagem_dias['quantidade'].sum()
    for _, row in contagem_dias.iterrows():
        porcentagem = (row['quantidade'] / total_registros) * 100
        print(f"{row['dia_semana']} - {row['nome_dia']:<10}: {row['quantidade']:>6,} ({porcentagem:5.1f}%)")

    # Volume: Dias √∫teis vs fim de semana
    dias_uteis = contagem_dias[contagem_dias['dia_semana'].isin([0,1,2,3,4])]['quantidade'].sum()
    fim_semana = contagem_dias[contagem_dias['dia_semana'].isin([5,6])]['quantidade'].sum()

    print(f"\nCompara√ß√£o de VOLUME - Fim de Semana vs Dias √öteis:")
    print(f"Dias √∫teis (Seg-Sex): {dias_uteis:,} ({dias_uteis/total_registros*100:.1f}%)")
    print(f"Fim de semana (S√°b-Dom): {fim_semana:,} ({fim_semana/total_registros*100:.1f}%)")

    # 2. AN√ÅLISE DE CONVERS√ÉO POR DIA DA SEMANA
    print(f"\n2. AN√ÅLISE DE CONVERS√ÉO POR DIA:")
    print("=" * 40)

    # Agrupar por dia da semana e calcular convers√µes
    conversao_por_dia = df_com_data.groupby(['dia_semana', 'nome_dia']).agg({
        'target': ['count', 'sum', 'mean']
    }).round(4)

    # Simplificar nomes das colunas
    conversao_por_dia.columns = ['total_leads', 'leads_convertidos', 'taxa_conversao']
    conversao_por_dia = conversao_por_dia.reset_index()

    print(f"{'DIA':<3} {'NOME':<10} {'LEADS':<8} {'CONVERT.':<8} {'TAXA':<7}")
    print("-" * 45)

    for _, row in conversao_por_dia.iterrows():
        taxa_pct = row['taxa_conversao'] * 100
        print(f"{row['dia_semana']:<3} {row['nome_dia']:<10} {row['total_leads']:<8,} {row['leads_convertidos']:<8,} {taxa_pct:<7.2f}%")

    # Convers√£o: Dias √∫teis vs fim de semana
    dias_uteis_conv = conversao_por_dia[conversao_por_dia['dia_semana'].isin([0,1,2,3,4])]
    fim_semana_conv = conversao_por_dia[conversao_por_dia['dia_semana'].isin([5,6])]

    total_leads_dias_uteis = dias_uteis_conv['total_leads'].sum()
    total_conv_dias_uteis = dias_uteis_conv['leads_convertidos'].sum()
    taxa_dias_uteis = total_conv_dias_uteis / total_leads_dias_uteis

    total_leads_fim_semana = fim_semana_conv['total_leads'].sum()
    total_conv_fim_semana = fim_semana_conv['leads_convertidos'].sum()
    taxa_fim_semana = total_conv_fim_semana / total_leads_fim_semana

    print(f"\nCompara√ß√£o de CONVERS√ÉO - Fim de Semana vs Dias √öteis:")
    print(f"Dias √∫teis (Seg-Sex): {total_conv_dias_uteis:,}/{total_leads_dias_uteis:,} ({taxa_dias_uteis*100:.2f}%)")
    print(f"Fim de semana (S√°b-Dom): {total_conv_fim_semana:,}/{total_leads_fim_semana:,} ({taxa_fim_semana*100:.2f}%)")

    # Diferen√ßa na taxa de convers√£o
    diferenca_conversao = ((taxa_fim_semana - taxa_dias_uteis) / taxa_dias_uteis) * 100
    print(f"Diferen√ßa na convers√£o: {diferenca_conversao:+.1f}% (fim de semana vs dias √∫teis)")

    # An√°lise estat√≠stica simples
    taxas_individuais = conversao_por_dia['taxa_conversao'].values
    taxa_maxima = taxas_individuais.max()
    taxa_minima = taxas_individuais.min()
    variacao_maxima = ((taxa_maxima - taxa_minima) / taxa_minima) * 100

    print(f"\nVaria√ß√£o entre dias individuais:")
    print(f"Taxa m√°xima: {taxa_maxima*100:.2f}% ({conversao_por_dia.loc[conversao_por_dia['taxa_conversao'].idxmax(), 'nome_dia']})")
    print(f"Taxa m√≠nima: {taxa_minima*100:.2f}% ({conversao_por_dia.loc[conversao_por_dia['taxa_conversao'].idxmin(), 'nome_dia']})")
    print(f"Varia√ß√£o m√°xima: {variacao_maxima:.1f}%")

    # 3. GR√ÅFICO
    plt.figure(figsize=(12, 6))

    # Cores diferentes para dias √∫teis e fim de semana
    cores = ['steelblue' if dia < 5 else 'orange' for dia in contagem_dias['dia_semana']]

    plt.bar(contagem_dias['nome_dia'], contagem_dias['quantidade'], color=cores, alpha=0.7)
    plt.title('Distribui√ß√£o de Cadastros por Dia da Semana', fontsize=14, fontweight='bold')
    plt.xlabel('Dia da Semana')
    plt.ylabel('Quantidade de Cadastros')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3, axis='y')

    # Adicionar valores nas barras
    for i, v in enumerate(contagem_dias['quantidade']):
        plt.text(i, v + total_registros*0.01, f'{v:,}', ha='center', va='bottom', fontsize=10)

    # Adicionar linha de m√©dia
    plt.axhline(y=total_registros/7, color='red', linestyle='--', alpha=0.7, label=f'M√©dia: {total_registros/7:,.0f}')

    # Legenda
    plt.legend(['M√©dia di√°ria', 'Dias √∫teis', 'Fim de semana'], loc='upper right')

    plt.tight_layout()
    plt.show()

    # 4. RECOMENDA√á√ÉO BASEADA NA CONVERS√ÉO
    print(f"\nRECOMENDA√á√ÉO BASEADA NA CONVERS√ÉO:")
    if abs(diferenca_conversao) > 20:
        print(f"‚úÖ CRIAR FEATURE 'dia_semana' - diferen√ßa ALTA na convers√£o ({diferenca_conversao:+.1f}%)")
        print("   H√° padr√£o significativo de convers√£o entre dias da semana")
    elif abs(diferenca_conversao) > 10:
        print(f"üü° CONSIDERAR FEATURE 'dia_semana' - diferen√ßa MODERADA na convers√£o ({diferenca_conversao:+.1f}%)")
        print("   H√° alguma varia√ß√£o na convers√£o, pode ser √∫til")
    elif variacao_maxima > 30:
        print(f"üü° CONSIDERAR FEATURE 'dia_semana' - varia√ß√£o alta entre dias individuais ({variacao_maxima:.1f}%)")
        print("   Apesar da m√©dia similar, alguns dias espec√≠ficos convertem muito diferente")
    else:
        print(f"‚ùå N√ÉO CRIAR FEATURE 'dia_semana' - diferen√ßas pequenas")
        print(f"   Convers√£o fim de semana vs dias √∫teis: {diferenca_conversao:+.1f}%")
        print(f"   Varia√ß√£o m√°xima entre dias: {variacao_maxima:.1f}%")
        print("   Padr√£o de convers√£o muito uniforme entre dias da semana")

    return conversao_por_dia

# Executar an√°lise
padroes_dias = analisar_padroes_dia_semana()

"""## 17- Cria√ß√£o das vers√µes de datasets com apenas matches de DevClub:
* DevClub - Full Stack 2025
* DevClub FullStack Pro - OFICIAL
* Forma√ß√£o DevClub FullStack Pro
* DevClub - Full Stack 2025 - EV
* DevClub - FS - Vital√≠cio
* [Vital√≠cio] Forma√ß√£o DevClub FullStack
* Forma√ß√£o DevClub FullStack Pro - COMER
* DevClub Vital√≠cio
* DevClub 3.0 - 2024
* (Desativado) DevClub 3.0 - 2024
* (Desativado) DevClub 3.0 - 2024 - Novo

### Remover os tickets diferentes:
* (Desativado) FrontEnd Club
* FrontEnd Club - Oficial
* FrontEnd Club
"""

import pandas as pd

def criar_quatro_datasets():
    """Cria 4 datasets: V1/V2 x Todos/DevClub"""

    print("CRIA√á√ÉO DOS 4 DATASETS")
    print("=" * 40)

    # 1. CRIAR ALIASES DOS DATASETS ORIGINAIS
    dataset_v1_todos = dataset_v1_final.copy()
    dataset_v2_todos = dataset_v2_final.copy()

    # 2. PRODUTOS DEVCLUB A MANTER
    produtos_devclub_manter = [
        'DevClub - Full Stack 2025',
        'DevClub FullStack Pro - OFICIAL',
        'Forma√ß√£o DevClub FullStack Pro - OFICI',
        'DevClub - Full Stack 2025 - EV',
        'DevClub - FS - Vital√≠cio',
        '[Vital√≠cio] Forma√ß√£o DevClub FullStack',
        'Forma√ß√£o DevClub FullStack Pro - COMER',
        'DevClub Vital√≠cio',
        'DevClub 3.0 - 2024',
        '(Desativado) DevClub 3.0 - 2024',
        '(Desativado) DevClub 3.0 - 2024 - Novo'
    ]

    # 3. IDENTIFICAR COMPRADORES DEVCLUB
    df_vendas_devclub = vendas_unificado[vendas_unificado['produto'].isin(produtos_devclub_manter)].copy()

    def normalizar_email(email):
        if pd.isna(email):
            return None
        email_str = str(email).strip().lower()
        if '@' in email_str and email_str != 'nan' and len(email_str) > 5:
            return email_str
        return None

    df_vendas_devclub['email_clean'] = df_vendas_devclub['email'].apply(normalizar_email)
    emails_compradores_devclub = set(df_vendas_devclub['email_clean'].dropna())

    print(f"Produtos DevClub identificados: {len(produtos_devclub_manter)}")
    print(f"Vendas DevClub: {len(df_vendas_devclub):,}")
    print(f"Emails √∫nicos compradores DevClub: {len(emails_compradores_devclub):,}")

    # 4. CRIAR DATASETS DEVCLUB
    def criar_dataset_devclub(df_original, nome):
        df_devclub = df_original.copy()

        # Normalizar emails do dataset de pesquisa
        df_devclub['email_temp'] = df_devclub['E-mail'].apply(normalizar_email)

        # Criar novo target baseado apenas em DevClub
        df_devclub['target_devclub'] = df_devclub['email_temp'].isin(emails_compradores_devclub).astype(int)

        # Remover coluna tempor√°ria e target antigo
        df_devclub = df_devclub.drop(columns=['email_temp', 'target'])

        # Renomear para target final
        df_devclub = df_devclub.rename(columns={'target_devclub': 'target'})

        # Estat√≠sticas
        total_registros = len(df_devclub)
        leads_qualificados = df_devclub['target'].sum()
        taxa_conversao = (leads_qualificados / total_registros * 100) if total_registros > 0 else 0

        print(f"\n{nome}:")
        print(f"  Total de registros: {total_registros:,}")
        print(f"  Leads qualificados DevClub: {leads_qualificados:,}")
        print(f"  Taxa de convers√£o DevClub: {taxa_conversao:.2f}%")
        print(f"  Colunas: {len(df_devclub.columns)}")

        return df_devclub

    # Criar datasets DevClub
    dataset_v1_devclub = criar_dataset_devclub(dataset_v1_final, "DATASET V1 DEVCLUB")
    dataset_v2_devclub = criar_dataset_devclub(dataset_v2_final, "DATASET V2 DEVCLUB")

    # 5. COMPARA√á√ÉO FINAL
    print(f"\n" + "=" * 60)
    print("RESUMO DOS 4 DATASETS CRIADOS")
    print("=" * 60)

    datasets_info = [
        ("dataset_v1_todos", dataset_v1_todos),
        ("dataset_v1_devclub", dataset_v1_devclub),
        ("dataset_v2_todos", dataset_v2_todos),
        ("dataset_v2_devclub", dataset_v2_devclub)
    ]

    for nome, df in datasets_info:
        registros = len(df)
        colunas = len(df.columns)
        targets = df['target'].sum()
        taxa = (targets / registros * 100) if registros > 0 else 0

        print(f"{nome}:")
        print(f"  Registros: {registros:,} | Colunas: {colunas} | Targets: {targets:,} | Taxa: {taxa:.2f}%")

    # 6. COMPARA√á√ÉO TAXAS DE CONVERS√ÉO
    print(f"\nCOMPARA√á√ÉO TODOS vs DEVCLUB:")
    print(f"V1 - Todos: {dataset_v1_todos['target'].sum():,} targets | DevClub: {dataset_v1_devclub['target'].sum():,} targets")
    print(f"V2 - Todos: {dataset_v2_todos['target'].sum():,} targets | DevClub: {dataset_v2_devclub['target'].sum():,} targets")

    return dataset_v1_todos, dataset_v1_devclub, dataset_v2_todos, dataset_v2_devclub

# Executar cria√ß√£o dos 4 datasets
dataset_v1_todos, dataset_v1_devclub, dataset_v2_todos, dataset_v2_devclub = criar_quatro_datasets()

"""## 18- Remo√ß√£o de features desnecess√°rias restantes e cria√ß√£o das √∫teis:
1. Remo√ß√£o:
* aba_origem
* arquivo_origem
* Data, criando feature dia_semana e epoca_mes
* Nome, criando: nome_comprimento, nome_tem_sobrenome, nome_valido
* E-mail, criando: email_valido
* Telefone, criando: telefone_valido, telefone_comprimento
"""

import pandas as pd
import re

def normalizar_telefone_robusto(telefone):
    """Normaliza telefone considerando nota√ß√£o cient√≠fica e padr√µes brasileiros"""
    if pd.isna(telefone):
        return None

    # Converter para string e lidar com nota√ß√£o cient√≠fica
    # CORRE√á√ÉO: Se √© float, converter diretamente para int para remover .0
    if isinstance(telefone, float):
        tel_str = str(int(telefone))
    else:
        tel_str = str(telefone)

    # Se est√° em nota√ß√£o cient√≠fica, converter para n√∫mero inteiro
    if 'e+' in tel_str.lower() or 'E+' in tel_str:
        try:
            tel_str = str(int(float(tel_str)))
        except:
            pass

    # Extrair apenas d√≠gitos
    digitos = re.sub(r'\D', '', tel_str)

    if len(digitos) < 8:
        return None

    # Remover c√≥digo do pa√≠s (55) se presente
    if digitos.startswith('55') and len(digitos) > 10:
        digitos = digitos[2:]

    # Verificar se √© um telefone v√°lido brasileiro
    if len(digitos) in [10, 11]:  # DDD + 8 ou 9 d√≠gitos
        return digitos
    elif len(digitos) in [8, 9]:  # Sem DDD
        return digitos

    return None

def validar_email_robusto(email):
    """Valida email com regex rigoroso"""
    if pd.isna(email):
        return False

    email_str = str(email).strip().lower()

    # Regex b√°sico para email
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'

    return bool(re.match(pattern, email_str))

def validar_nome_robusto(nome):
    """Valida se nome n√£o √© apenas n√∫meros ou caracteres especiais"""
    if pd.isna(nome):
        return False

    nome_str = str(nome).strip()

    # Verificar se tem pelo menos algumas letras
    tem_letras = bool(re.search(r'[a-zA-Z√Ä-√ø]', nome_str))

    # Verificar se n√£o √© s√≥ n√∫meros
    nao_so_numeros = not nome_str.replace(' ', '').replace('.', '').replace('-', '').isdigit()

    return tem_letras and nao_so_numeros and len(nome_str) >= 2

def criar_features_derivadas_completo():
    """Cria todas as features derivadas e remove colunas desnecess√°rias para os 4 datasets"""

    print("FEATURE ENGINEERING COMPLETO - 4 DATASETS")
    print("=" * 45)

    def processar_dataset(df_original, nome_dataset):
        """Processa um dataset criando features derivadas"""

        df = df_original.copy()

        print(f"\nProcessando {nome_dataset}...")
        print(f"Registros: {len(df):,}")
        print(f"Colunas antes: {len(df.columns)}")

        # 1. FEATURES TEMPORAIS
        df['Data'] = pd.to_datetime(df['Data'], errors='coerce')
        df['dia_semana'] = df['Data'].dt.dayofweek

        # 2. FEATURES DE QUALIDADE DOS IDENTIFICADORES

        # Nome
        df['nome_comprimento'] = df['Nome Completo'].astype(str).str.len()
        df['nome_tem_sobrenome'] = df['Nome Completo'].astype(str).str.split().str.len() >= 2
        df['nome_valido'] = df['Nome Completo'].apply(validar_nome_robusto)

        # Email
        df['email_valido'] = df['E-mail'].apply(validar_email_robusto)

        # Telefone
        df['telefone_normalizado'] = df['Telefone'].apply(normalizar_telefone_robusto)
        df['telefone_valido'] = df['telefone_normalizado'].notna()
        df['telefone_comprimento'] = df['telefone_normalizado'].astype(str).str.len()

        # AN√ÅLISE DE TELEFONES V√ÅLIDOS POR ARQUIVO DE ORIGEM
        if 'arquivo_origem' in df.columns:
            print(f"\n% de telefones v√°lidos por arquivo de origem:")
            telefone_por_arquivo = df.groupby('arquivo_origem')['telefone_valido'].agg(['count', 'sum', 'mean']).round(3)
            telefone_por_arquivo['pct_valido'] = (telefone_por_arquivo['mean'] * 100).round(1)
            telefone_por_arquivo = telefone_por_arquivo.sort_values('pct_valido', ascending=False)

            for arquivo in telefone_por_arquivo.index:
                total = telefone_por_arquivo.loc[arquivo, 'count']
                validos = telefone_por_arquivo.loc[arquivo, 'sum']
                pct = telefone_por_arquivo.loc[arquivo, 'pct_valido']
                print(f"  {arquivo}: {validos:,}/{total:,} ({pct}%)")

        # 3. REMOVER COLUNAS DESNECESS√ÅRIAS
        colunas_remover = [
            'aba_origem', 'arquivo_origem', 'Data',
            'Nome Completo', 'E-mail', 'Telefone', 'telefone_normalizado'
        ]

        # Verificar quais colunas existem antes de remover
        colunas_existentes = [col for col in colunas_remover if col in df.columns]
        colunas_nao_existentes = [col for col in colunas_remover if col not in df.columns]

        if colunas_existentes:
            df = df.drop(columns=colunas_existentes)
            print(f"Colunas removidas: {len(colunas_existentes)}")
            for col in colunas_existentes:
                print(f"  - {col}")

        if colunas_nao_existentes:
            print(f"Colunas n√£o encontradas (ok): {len(colunas_nao_existentes)}")

        print(f"Colunas depois: {len(df.columns)}")

        # 4. ESTAT√çSTICAS DAS NOVAS FEATURES
        print(f"\nEstat√≠sticas das features criadas:")
        print(f"Nome v√°lido: {df['nome_valido'].sum():,} ({df['nome_valido'].mean()*100:.1f}%)")
        print(f"Nome com sobrenome: {df['nome_tem_sobrenome'].sum():,} ({df['nome_tem_sobrenome'].mean()*100:.1f}%)")
        print(f"Email v√°lido: {df['email_valido'].sum():,} ({df['email_valido'].mean()*100:.1f}%)")
        print(f"Telefone v√°lido: {df['telefone_valido'].sum():,} ({df['telefone_valido'].mean()*100:.1f}%)")

        # 5. DISTRIBUI√á√ÉO DA FEATURE TEMPORAL
        print(f"\nDistribui√ß√£o da feature temporal:")
        dia_semana_counts = df['dia_semana'].value_counts().sort_index()
        nomes_dias = ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo']
        for dia, count in dia_semana_counts.items():
            pct = (count / len(df)) * 100
            print(f"  {dia} ({nomes_dias[dia]}): {count:,} ({pct:.1f}%)")

        return df

    # Processar os 4 datasets
    dataset_v1_todos_fe = processar_dataset(dataset_v1_todos, "DATASET V1 TODOS")
    dataset_v1_devclub_fe = processar_dataset(dataset_v1_devclub, "DATASET V1 DEVCLUB")
    dataset_v2_todos_fe = processar_dataset(dataset_v2_todos, "DATASET V2 TODOS")
    dataset_v2_devclub_fe = processar_dataset(dataset_v2_devclub, "DATASET V2 DEVCLUB")

    # Listar colunas finais
    print(f"\n" + "=" * 60)
    print("DATASETS FINAIS PARA MODELAGEM")
    print("=" * 60)

    datasets_info = [
        ("DATASET V1 TODOS", dataset_v1_todos_fe),
        ("DATASET V1 DEVCLUB", dataset_v1_devclub_fe),
        ("DATASET V2 TODOS", dataset_v2_todos_fe),
        ("DATASET V2 DEVCLUB", dataset_v2_devclub_fe)
    ]

    for nome, df in datasets_info:
        print(f"\n{nome}:")
        print(f"  Registros: {len(df):,}")
        print(f"  Colunas: {len(df.columns)}")
        print(f"  Target positivo: {df['target'].sum():,} ({df['target'].mean()*100:.2f}%)")

    # Verificar diferen√ßas nas colunas entre V1 e V2
    colunas_v1 = set(dataset_v1_todos_fe.columns)
    colunas_v2 = set(dataset_v2_todos_fe.columns)

    colunas_apenas_v1 = colunas_v1 - colunas_v2
    colunas_apenas_v2 = colunas_v2 - colunas_v1

    print(f"\nDIFEREN√áAS ENTRE V1 E V2:")
    if colunas_apenas_v1:
        print(f"Colunas apenas no V1:")
        for col in sorted(colunas_apenas_v1):
            print(f"  - {col}")

    if colunas_apenas_v2:
        print(f"Colunas apenas no V2:")
        for col in sorted(colunas_apenas_v2):
            print(f"  - {col}")

    if not colunas_apenas_v1 and not colunas_apenas_v2:
        print(f"V1 e V2 possuem as mesmas colunas")

    print(f"\nTodos os 4 datasets prontos para encoding e modelagem!")

    return dataset_v1_todos_fe, dataset_v1_devclub_fe, dataset_v2_todos_fe, dataset_v2_devclub_fe

# Executar feature engineering completo nos 4 datasets
dataset_v1_todos_fe, dataset_v1_devclub_fe, dataset_v2_todos_fe, dataset_v2_devclub_fe = criar_features_derivadas_completo()

"""## 19- Investiga√ß√£o
Sobre o n√∫mero de telefones v√°lidos (~25% no V1 x ~45% no V2)
"""

import pandas as pd
import re

def debug_validacao_telefones():
    """Debug detalhado da valida√ß√£o de telefones para identificar onde est√° falhando"""

    print("DEBUG DETALHADO DA VALIDA√á√ÉO DE TELEFONES")
    print("=" * 50)

    datasets = [
        (dataset_v1_final, "DATASET V1"),
        (dataset_v2_final, "DATASET V2")
    ]

    def debug_telefone_dataset(df, nome_dataset):
        """Debug telefones em um dataset espec√≠fico"""

        print(f"\n{nome_dataset}:")
        print("=" * 30)

        telefones_df = df[df['Telefone'].notna()].copy()
        telefones_df['telefone_str'] = telefones_df['Telefone'].astype(str)

        total_telefones = len(telefones_df)
        print(f"Total de telefones: {total_telefones:,}")

        # Contadores para cada etapa
        etapas = {
            'entrada': 0,
            'pos_notacao_cientifica': 0,
            'identificados_plus_ddd': 0,
            'pos_remocao_plus': 0,
            'pos_extracao_digitos': 0,
            'muito_curtos': 0,
            'pos_remocao_55': 0,
            'telefones_validos': 0
        }

        exemplos_rejeitados = {
            'muito_curtos': [],
            'muito_longos': [],
            'pos_55_muito_longos': [],
            'outros': []
        }

        # DDDs brasileiros v√°lidos
        ddds_br = ['11', '12', '13', '14', '15', '16', '17', '18', '19',
                   '21', '22', '24', '27', '28', '31', '32', '33', '34', '35', '37', '38',
                   '41', '42', '43', '44', '45', '46', '47', '48', '49', '51', '53', '54', '55',
                   '61', '62', '63', '64', '65', '66', '67', '68', '69',
                   '71', '73', '74', '75', '77', '79',
                   '81', '82', '83', '84', '85', '86', '87', '88', '89',
                   '91', '92', '93', '94', '95', '96', '97', '98', '99']

        for telefone in telefones_df['telefone_str']:
            etapas['entrada'] += 1
            tel_str = str(telefone)
            original = tel_str

            # Etapa 1: Tratar nota√ß√£o cient√≠fica
            if 'e+' in tel_str.lower() or 'E+' in tel_str:
                try:
                    tel_str = str(int(float(tel_str)))
                except:
                    pass
            etapas['pos_notacao_cientifica'] += 1

            # Etapa 2: Identificar telefones com +DDD brasileiro
            tem_plus_ddd = False
            if tel_str.startswith('+') and not tel_str.startswith('+55'):
                tel_str_sem_plus = tel_str[1:]
                if len(tel_str_sem_plus) >= 2 and tel_str_sem_plus[:2] in ddds_br:
                    tem_plus_ddd = True
                    etapas['identificados_plus_ddd'] += 1
                    tel_str = tel_str_sem_plus

            etapas['pos_remocao_plus'] += 1

            # Etapa 3: Extrair apenas d√≠gitos
            digitos = re.sub(r'\D', '', tel_str)
            etapas['pos_extracao_digitos'] += 1

            # Etapa 4: Verificar se muito curto
            if len(digitos) < 7:
                etapas['muito_curtos'] += 1
                if len(exemplos_rejeitados['muito_curtos']) < 3:
                    exemplos_rejeitados['muito_curtos'].append(f"{original} -> {digitos} (len={len(digitos)})")
                continue

            # Etapa 5: Remover c√≥digo do pa√≠s (55) se presente
            digitos_original = digitos
            if digitos.startswith('55') and len(digitos) > 10:
                digitos = digitos[2:]
            etapas['pos_remocao_55'] += 1

            # Etapa 6: Valida√ß√£o final
            if len(digitos) in [7, 8, 9, 10, 11]:
                etapas['telefones_validos'] += 1
            else:
                # Categorizar por que foi rejeitado
                if len(digitos) > 11:
                    if len(digitos_original) > 13:
                        if len(exemplos_rejeitados['muito_longos']) < 3:
                            status = f"+DDD: {tem_plus_ddd}" if tem_plus_ddd else ""
                            exemplos_rejeitados['muito_longos'].append(f"{original} -> {digitos} (len={len(digitos)}) {status}")
                    else:
                        if len(exemplos_rejeitados['pos_55_muito_longos']) < 3:
                            status = f"+DDD: {tem_plus_ddd}" if tem_plus_ddd else ""
                            exemplos_rejeitados['pos_55_muito_longos'].append(f"{original} -> {digitos} (len={len(digitos)}) {status}")
                else:
                    if len(exemplos_rejeitados['outros']) < 3:
                        status = f"+DDD: {tem_plus_ddd}" if tem_plus_ddd else ""
                        exemplos_rejeitados['outros'].append(f"{original} -> {digitos} (len={len(digitos)}) {status}")

        # Relat√≥rio detalhado
        print(f"\nFLUXO DE PROCESSAMENTO:")
        for etapa, count in etapas.items():
            pct = (count / total_telefones * 100) if total_telefones > 0 else 0
            print(f"  {etapa}: {count:,} ({pct:.1f}%)")

        print(f"\nTelefones com +DDD identificados: {etapas['identificados_plus_ddd']:,}")

        print(f"\nEXEMPLOS DE REJEI√á√ïES:")
        for categoria, exemplos in exemplos_rejeitados.items():
            if exemplos:
                print(f"  {categoria}:")
                for exemplo in exemplos:
                    print(f"    - {exemplo}")

        # Calcular taxa de recupera√ß√£o esperada
        telefones_perdidos = total_telefones - etapas['telefones_validos']
        plus_ddd_perdidos = etapas['identificados_plus_ddd']

        print(f"\nAN√ÅLISE DE RECUPERA√á√ÉO:")
        print(f"  Telefones perdidos: {telefones_perdidos:,}")
        print(f"  +DDD identificados mas perdidos: {plus_ddd_perdidos:,}")
        print(f"  Taxa de recupera√ß√£o potencial: {plus_ddd_perdidos/telefones_perdidos*100:.1f}%")

        return etapas

    # Analisar ambos datasets
    for df, nome in datasets:
        debug_telefone_dataset(df, nome)

# Executar debug
debug_validacao_telefones()

"""## 19.1- Investiga√ß√£o
Da cardinalidade das features restantes para fazer encoding
"""

import pandas as pd

def analisar_cardinalidade_para_encoding():
    """Analisa cardinalidade das vari√°veis para definir estrat√©gia de encoding"""

    print("AN√ÅLISE DE CARDINALIDADE PARA ENCODING")
    print("=" * 50)

    def analisar_dataset(df, nome_dataset):
        """Analisa um dataset espec√≠fico"""

        print(f"\n{nome_dataset}:")
        print(f"Total de registros: {len(df):,}")
        print(f"Total de colunas: {len(df.columns)}")

        # Separar vari√°veis por tipo
        colunas_numericas = []
        colunas_booleanas = []
        colunas_categoricas = []

        for col in df.columns:
            if col == 'target':
                continue

            dtype = str(df[col].dtype)
            valores_unicos = df[col].nunique()
            valores_nao_nulos = df[col].notna().sum()

            # Determinar tipo da vari√°vel
            if dtype in ['int64', 'float64'] and valores_unicos > 10:
                if df[col].min() >= 0 and df[col].max() <= 1:
                    colunas_booleanas.append(col)
                else:
                    colunas_numericas.append(col)
            elif valores_unicos == 2 or (dtype == 'bool'):
                colunas_booleanas.append(col)
            else:
                colunas_categoricas.append(col)

        print(f"\nTIPOS DE VARI√ÅVEIS IDENTIFICADOS:")
        print(f"Num√©ricas: {len(colunas_numericas)}")
        print(f"Booleanas: {len(colunas_booleanas)}")
        print(f"Categ√≥ricas: {len(colunas_categoricas)}")

        # An√°lise detalhada das categ√≥ricas
        if colunas_categoricas:
            print(f"\nAN√ÅLISE DE CARDINALIDADE - CATEG√ìRICAS:")
            print(f"{'COLUNA':<45} {'√öNICOS':<8} {'% MISS':<8} {'TIPO SUGERIDO':<15}")
            print("-" * 80)

            for col in colunas_categoricas:
                valores_unicos = df[col].nunique()
                total_registros = len(df)
                missing_rate = (df[col].isnull().sum() / total_registros) * 100

                # Sugerir tipo de encoding
                if valores_unicos <= 2:
                    tipo_sugerido = "Booleana"
                elif valores_unicos <= 5:
                    tipo_sugerido = "One-Hot"
                elif valores_unicos <= 15:
                    tipo_sugerido = "Label/One-Hot"
                elif valores_unicos <= 50:
                    tipo_sugerido = "Label/Target"
                else:
                    tipo_sugerido = "Target/Freq"

                col_display = col[:42] if len(col) > 42 else col
                print(f"{col_display:<45} {valores_unicos:<8} {missing_rate:<8.1f} {tipo_sugerido:<15}")

        # An√°lise das booleanas
        if colunas_booleanas:
            print(f"\nAN√ÅLISE - BOOLEANAS:")
            print(f"{'COLUNA':<45} {'VALORES':<15} {'% MISS':<8}")
            print("-" * 70)

            for col in colunas_booleanas:
                valores_unicos = sorted(df[col].dropna().unique())
                missing_rate = (df[col].isnull().sum() / len(df)) * 100
                valores_str = str(valores_unicos)[:12]

                col_display = col[:42] if len(col) > 42 else col
                print(f"{col_display:<45} {valores_str:<15} {missing_rate:<8.1f}")

        # An√°lise das num√©ricas
        if colunas_numericas:
            print(f"\nAN√ÅLISE - NUM√âRICAS:")
            print(f"{'COLUNA':<45} {'MIN':<8} {'MAX':<8} {'% MISS':<8}")
            print("-" * 70)

            for col in colunas_numericas:
                min_val = df[col].min()
                max_val = df[col].max()
                missing_rate = (df[col].isnull().sum() / len(df)) * 100

                col_display = col[:42] if len(col) > 42 else col
                print(f"{col_display:<45} {min_val:<8.0f} {max_val:<8.0f} {missing_rate:<8.1f}")

        return {
            'numericas': colunas_numericas,
            'booleanas': colunas_booleanas,
            'categoricas': colunas_categoricas
        }

    # Analisar ambos os datasets
    analise_v1 = analisar_dataset(dataset_v1_final, "DATASET V1")
    analise_v2 = analisar_dataset(dataset_v2_final, "DATASET V2")

    # Recomenda√ß√µes espec√≠ficas
    print(f"\n" + "=" * 60)
    print("RECOMENDA√á√ïES DE ENCODING")
    print("=" * 60)

    def gerar_recomendacoes(df, nome):
        """Gera recomenda√ß√µes espec√≠ficas de encoding"""

        print(f"\n{nome}:")

        encoding_strategy = {
            'manter_numericas': [],
            'one_hot': [],
            'label_encoding': [],
            'target_encoding': [],
            'frequency_encoding': []
        }

        for col in df.columns:
            if col == 'target':
                continue

            valores_unicos = df[col].nunique()

            # Aplicar l√≥gica de decis√£o
            if df[col].dtype in ['int64', 'float64'] and valores_unicos > 10:
                if df[col].min() >= 0 and df[col].max() <= 1:
                    encoding_strategy['one_hot'].append(col)
                else:
                    encoding_strategy['manter_numericas'].append(col)
            elif valores_unicos <= 2:
                encoding_strategy['one_hot'].append(col)
            elif valores_unicos <= 5:
                encoding_strategy['one_hot'].append(col)
            elif valores_unicos <= 15:
                encoding_strategy['label_encoding'].append(col)
            elif valores_unicos <= 50:
                encoding_strategy['target_encoding'].append(col)
            else:
                encoding_strategy['frequency_encoding'].append(col)

        # Imprimir recomenda√ß√µes
        for strategy, colunas in encoding_strategy.items():
            if colunas:
                strategy_name = strategy.replace('_', ' ').title()
                print(f"\n{strategy_name}: {len(colunas)} colunas")
                for col in colunas:
                    nunique = df[col].nunique()
                    print(f"  - {col} ({nunique} valores √∫nicos)")

        return encoding_strategy

    # Gerar recomenda√ß√µes para ambos
    recomendacoes_v1 = gerar_recomendacoes(dataset_v1_final, "DATASET V1")
    recomendacoes_v2 = gerar_recomendacoes(dataset_v2_final, "DATASET V2")

    # Verificar vari√°veis ordinais potenciais
    print(f"\n" + "=" * 60)
    print("VERIFICA√á√ÉO DE VARI√ÅVEIS ORDINAIS")
    print("=" * 60)

    variaveis_ordinais_suspeitas = [
        'Qual a sua idade?',
        'Atualmente, qual a sua faixa salarial?',
        'dia_semana'
    ]

    print("Vari√°veis que podem ser ordinais:")
    for var in variaveis_ordinais_suspeitas:
        if var in dataset_v2_final.columns:
            valores = sorted(dataset_v2_final[var].dropna().unique())
            print(f"\n{var}:")
            print(f"  Valores: {valores}")
            if len(valores) <= 10:
                for val in valores[:10]:
                    count = (dataset_v2_final[var] == val).sum()
                    print(f"    '{val}': {count:,} registros")

    return recomendacoes_v1, recomendacoes_v2

# Executar an√°lise
recomendacoes_v1, recomendacoes_v2 = analisar_cardinalidade_para_encoding()

"""## 19.2- Investiga√ß√£o
Da diferen√ßa no n√∫mero de categorias da vari√°vel "O que voc√™ faz atualmente?"
"""

import pandas as pd

def investigar_diferenca_categorias():
    """Investiga diferen√ßas no n√∫mero de categorias entre V1 e V2"""

    print("INVESTIGA√á√ÉO DE DIFEREN√áAS DE CATEGORIAS")
    print("=" * 50)

    # Vari√°vel com diferen√ßa identificada
    variavel_diferente = "O que voc√™ faz atualmente?"

    print(f"Analisando vari√°vel: {variavel_diferente}")
    print(f"V1 tem 5 categorias, V2 tem 9 categorias")

    # Verificar se a vari√°vel existe em ambos
    if variavel_diferente in dataset_v1_final.columns and variavel_diferente in dataset_v2_final.columns:

        # Obter categorias √∫nicas de cada dataset
        categorias_v1 = set(dataset_v1_final[variavel_diferente].dropna().unique())
        categorias_v2 = set(dataset_v2_final[variavel_diferente].dropna().unique())

        print(f"\nCategorias no V1 ({len(categorias_v1)}):")
        for cat in sorted(categorias_v1):
            count = (dataset_v1_final[variavel_diferente] == cat).sum()
            print(f"  - '{cat}': {count:,} registros")

        print(f"\nCategorias no V2 ({len(categorias_v2)}):")
        for cat in sorted(categorias_v2):
            count = (dataset_v2_final[variavel_diferente] == cat).sum()
            print(f"  - '{cat}': {count:,} registros")

        # Identificar diferen√ßas
        apenas_v1 = categorias_v1 - categorias_v2
        apenas_v2 = categorias_v2 - categorias_v1
        comuns = categorias_v1 & categorias_v2

        print(f"\nDIFEREN√áAS IDENTIFICADAS:")
        print(f"Categorias comuns: {len(comuns)}")

        if apenas_v1:
            print(f"\nApenas no V1 ({len(apenas_v1)}):")
            for cat in sorted(apenas_v1):
                count = (dataset_v1_final[variavel_diferente] == cat).sum()
                print(f"  - '{cat}': {count:,} registros")

        if apenas_v2:
            print(f"\nApenas no V2 ({len(apenas_v2)}):")
            for cat in sorted(apenas_v2):
                count = (dataset_v2_final[variavel_diferente] == cat).sum()
                print(f"  - '{cat}': {count:,} registros")

        # Explica√ß√£o da diferen√ßa
        print(f"\nEXPLICA√á√ÉO DA DIFEREN√áA:")
        if apenas_v2 and not apenas_v1:
            print("V2 (per√≠odo completo) tem categorias antigas que foram removidas/unificadas no V1 (per√≠odo recente)")
            print("Isso √© esperado se houve evolu√ß√£o no formul√°rio ao longo do tempo")
        elif apenas_v1 and not apenas_v2:
            print("V1 (per√≠odo recente) tem categorias novas que n√£o existiam no per√≠odo anterior")
        else:
            print("H√° categorias diferentes em ambos os datasets")

    else:
        print(f"Vari√°vel '{variavel_diferente}' n√£o encontrada em um dos datasets")

    # Verificar outras vari√°veis categ√≥ricas para poss√≠veis diferen√ßas n√£o detectadas
    print(f"\n" + "=" * 60)
    print("VERIFICA√á√ÉO R√ÅPIDA DE OUTRAS VARI√ÅVEIS")
    print("=" * 60)

    variaveis_categoricas = [
        'Qual a sua idade?',
        'Atualmente, qual a sua faixa salarial?',
        'O que mais voc√™ quer ver no evento?',
        'Source',
        'Medium',
        'Term',
        'interesse_programacao'
    ]

    print(f"{'VARI√ÅVEL':<45} {'V1':<5} {'V2':<5} {'DIFEREN√áA'}")
    print("-" * 65)

    for var in variaveis_categoricas:
        if var in dataset_v1_final.columns and var in dataset_v2_final.columns:
            nunique_v1 = dataset_v1_final[var].nunique()
            nunique_v2 = dataset_v2_final[var].nunique()
            diferenca = "SIM" if nunique_v1 != nunique_v2 else "N√ÉO"

            var_display = var[:42] if len(var) > 42 else var
            print(f"{var_display:<45} {nunique_v1:<5} {nunique_v2:<5} {diferenca}")

# Executar investiga√ß√£o
investigar_diferenca_categorias()

"""## 20- Encoding"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

def aplicar_encoding_estrategico():
    """Aplica encoding seguindo a estrat√©gia recomendada nos 4 datasets"""

    print("ENCODING ESTRAT√âGICO DOS 4 DATASETS")
    print("=" * 45)

    def processar_dataset(df_original, nome_dataset):
        """Aplica encoding em um dataset espec√≠fico"""

        df = df_original.copy()

        print(f"\nProcessando {nome_dataset}...")
        print(f"Colunas antes do encoding: {len(df.columns)}")

        # 1. ENCODING ORDINAL para vari√°veis com ordem natural
        variaveis_ordinais = {
            'Qual a sua idade?': ['Menos de 18 anos', '18 - 24 anos', '25 - 34 anos',
                                  '35 - 44 anos', '45 - 54 anos', 'Mais de 55 anos'],
            'Atualmente, qual a sua faixa salarial?': ['N√£o tenho renda', 'Entre R$1.000 a R$2.000 reais ao m√™s',
                                                       'Entre R$2.001 a R$3.000 reais ao m√™s',
                                                       'Entre R$3.001 a R$5.000 reais ao m√™s',
                                                       'Mais de R$5.001 reais ao m√™s'],
            'dia_semana': [0, 1, 2, 3, 4, 5, 6]  # J√° √© num√©rico
        }

        print(f"\nAplicando ORDINAL ENCODING:")
        for var, ordem in variaveis_ordinais.items():
            if var in df.columns:
                if var == 'dia_semana':
                    # J√° √© num√©rico, apenas reportar
                    print(f"  {var}: mantido como num√©rico (0-6)")
                else:
                    # Criar mapeamento ordinal
                    mapeamento = {categoria: i for i, categoria in enumerate(ordem)}
                    df[var] = df[var].map(mapeamento)
                    print(f"  {var}: {len(ordem)} categorias ‚Üí 0-{len(ordem)-1}")

        # 2. ONE-HOT ENCODING para vari√°veis categ√≥ricas nominais
        variaveis_one_hot = []

        # Identificar vari√°veis categ√≥ricas (excluindo ordinais j√° processadas e target)
        for col in df.columns:
            if col not in ['target'] and col not in variaveis_ordinais and col != 'nome_comprimento':
                # Verificar se √© categ√≥rica (object ou poucos valores √∫nicos)
                if df[col].dtype == 'object' or df[col].nunique() <= 20:
                    variaveis_one_hot.append(col)

        print(f"\nAplicando ONE-HOT ENCODING para {len(variaveis_one_hot)} vari√°veis:")

        # Aplicar one-hot encoding
        df_encoded = pd.get_dummies(df, columns=variaveis_one_hot, prefix_sep='_', dtype=int)

        # REMOVER telefone_comprimento_8
        if 'telefone_comprimento_8' in df_encoded.columns:
            df_encoded = df_encoded.drop(columns=['telefone_comprimento_8'])

        # Reportar cria√ß√£o de colunas
        colunas_criadas = len(df_encoded.columns) - len(df.columns)
        for var in variaveis_one_hot:
            categorias_unicas = df[var].nunique()
            print(f"  {var}: {categorias_unicas} categorias ‚Üí {categorias_unicas} colunas bin√°rias")

        print(f"\nResultado:")
        print(f"  Colunas one-hot originais: {len(variaveis_one_hot)}")
        print(f"  Colunas bin√°rias criadas: {colunas_criadas}")
        print(f"  Total de colunas final: {len(df_encoded.columns)}")

        # Verificar tipos de dados finais
        tipos_dados = df_encoded.dtypes.value_counts()
        print(f"\nTipos de dados no dataset final:")
        for tipo, count in tipos_dados.items():
            print(f"  {tipo}: {count} colunas")

        return df_encoded

    # Processar os 4 datasets usando as vari√°veis corretas do feature engineering
    dataset_v1_todos_encoded = processar_dataset(dataset_v1_todos_fe, "DATASET V1 TODOS")
    dataset_v1_devclub_encoded = processar_dataset(dataset_v1_devclub_fe, "DATASET V1 DEVCLUB")
    dataset_v2_todos_encoded = processar_dataset(dataset_v2_todos_fe, "DATASET V2 TODOS")
    dataset_v2_devclub_encoded = processar_dataset(dataset_v2_devclub_fe, "DATASET V2 DEVCLUB")

    # Compara√ß√£o final
    print(f"\n" + "=" * 60)
    print("RESUMO DOS 4 DATASETS ENCODADOS")
    print("=" * 60)

    datasets_info = [
        ("DATASET V1 TODOS", dataset_v1_todos_encoded),
        ("DATASET V1 DEVCLUB", dataset_v1_devclub_encoded),
        ("DATASET V2 TODOS", dataset_v2_todos_encoded),
        ("DATASET V2 DEVCLUB", dataset_v2_devclub_encoded)
    ]

    for nome, df in datasets_info:
        print(f"\n{nome}:")
        print(f"  Registros: {len(df):,}")
        print(f"  Colunas: {len(df.columns)}")
        print(f"  Target positivo: {df['target'].sum():,} ({df['target'].mean()*100:.2f}%)")

    # Verificar se V1 e V2 t√™m as mesmas colunas para cada vers√£o (todos/devclub)
    colunas_v1_todos = set(dataset_v1_todos_encoded.columns)
    colunas_v2_todos = set(dataset_v2_todos_encoded.columns)
    colunas_v1_devclub = set(dataset_v1_devclub_encoded.columns)
    colunas_v2_devclub = set(dataset_v2_devclub_encoded.columns)

    # Comparar colunas entre V1 e V2 para vers√£o TODOS
    colunas_apenas_v1_todos = colunas_v1_todos - colunas_v2_todos
    colunas_apenas_v2_todos = colunas_v2_todos - colunas_v1_todos

    # Comparar colunas entre V1 e V2 para vers√£o DEVCLUB
    colunas_apenas_v1_devclub = colunas_v1_devclub - colunas_v2_devclub
    colunas_apenas_v2_devclub = colunas_v2_devclub - colunas_v1_devclub

    print(f"\nCOMPARA√á√ÉO DE COLUNAS V1 vs V2:")
    print(f"VERS√ÉO TODOS:")
    if colunas_apenas_v1_todos:
        print(f"  Colunas apenas no V1 TODOS ({len(colunas_apenas_v1_todos)}):")
        for col in sorted(colunas_apenas_v1_todos):
            print(f"    - {col}")

    if colunas_apenas_v2_todos:
        print(f"  Colunas apenas no V2 TODOS ({len(colunas_apenas_v2_todos)}):")
        for col in sorted(colunas_apenas_v2_todos):
            print(f"    - {col}")

    if not colunas_apenas_v1_todos and not colunas_apenas_v2_todos:
        print(f"  V1 e V2 TODOS t√™m as mesmas colunas ‚úì")

    print(f"\nVERS√ÉO DEVCLUB:")
    if colunas_apenas_v1_devclub:
        print(f"  Colunas apenas no V1 DEVCLUB ({len(colunas_apenas_v1_devclub)}):")
        for col in sorted(colunas_apenas_v1_devclub):
            print(f"    - {col}")

    if colunas_apenas_v2_devclub:
        print(f"  Colunas apenas no V2 DEVCLUB ({len(colunas_apenas_v2_devclub)}):")
        for col in sorted(colunas_apenas_v2_devclub):
            print(f"    - {col}")

    if not colunas_apenas_v1_devclub and not colunas_apenas_v2_devclub:
        print(f"  V1 e V2 DEVCLUB t√™m as mesmas colunas ‚úì")

    # Verificar presen√ßa da feature telefone_comprimento_8
    print(f"\nVERIFICA√á√ÉO DA FEATURE telefone_comprimento_8:")
    datasets_verificar = [
        ("DATASET V1 TODOS", dataset_v1_todos_encoded),
        ("DATASET V1 DEVCLUB", dataset_v1_devclub_encoded),
        ("DATASET V2 TODOS", dataset_v2_todos_encoded),
        ("DATASET V2 DEVCLUB", dataset_v2_devclub_encoded)
    ]

    for nome, df in datasets_verificar:
        status = "PRESENTE" if 'telefone_comprimento_8' in df.columns else "AUSENTE"
        print(f"  {nome}: {status}")

    print(f"\nTodos os 4 datasets encodados est√£o prontos para modelagem!")

    return dataset_v1_todos_encoded, dataset_v1_devclub_encoded, dataset_v2_todos_encoded, dataset_v2_devclub_encoded

# Executar encoding estrat√©gico nos 4 datasets
dataset_v1_todos_encoded, dataset_v1_devclub_encoded, dataset_v2_todos_encoded, dataset_v2_devclub_encoded = aplicar_encoding_estrategico()

"""## 21- Modelagem Baseline
Splita os dados 80/20 para os dois datasets e treina um random forest e um lgbm com par√¢metro de desbalanceamento de classes ativado, produzindo 10 decis de probabilidade
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
import xgboost as xgb

def validacao_temporal():
    """Valida modelos usando split temporal nos 4 datasets"""

    print("VALIDA√á√ÉO TEMPORAL DOS MODELOS - 4 DATASETS")
    print("=" * 50)

    def recuperar_data_original():
        """Recupera coluna de data dos datasets originais antes do feature engineering"""

        # Recuperar data dos datasets V1 e V2 originais (antes do FE que removeu a coluna Data)
        data_v1_todos = dataset_v1_todos['Data'].copy()
        data_v1_devclub = dataset_v1_devclub['Data'].copy()
        data_v2_todos = dataset_v2_todos['Data'].copy()
        data_v2_devclub = dataset_v2_devclub['Data'].copy()

        return data_v1_todos, data_v1_devclub, data_v2_todos, data_v2_devclub

    def split_temporal(df_encoded, data_original, nome_dataset):
        """Faz split temporal dos dados"""

        print(f"\nAnalisando per√≠odo temporal - {nome_dataset}:")

        # Converter data para datetime
        data_dt = pd.to_datetime(data_original, errors='coerce')

        # Estat√≠sticas temporais
        data_min = data_dt.min()
        data_max = data_dt.max()

        print(f"Per√≠odo total: {data_min.strftime('%Y-%m-%d')} a {data_max.strftime('%Y-%m-%d')}")

        # Definir data de corte (70% para treino, 30% para teste)
        dias_totais = (data_max - data_min).days
        dias_treino = int(dias_totais * 0.7)
        data_corte = data_min + pd.Timedelta(days=dias_treino)

        print(f"Data de corte: {data_corte.strftime('%Y-%m-%d')}")
        print(f"Treino: {data_min.strftime('%Y-%m-%d')} a {data_corte.strftime('%Y-%m-%d')}")
        print(f"Teste: {data_corte.strftime('%Y-%m-%d')} a {data_max.strftime('%Y-%m-%d')}")

        # Criar m√°scaras temporais
        mask_treino = data_dt <= data_corte
        mask_teste = data_dt > data_corte

        # Separar dados
        X = df_encoded.drop(columns=['target'])
        y = df_encoded['target']

        # Limpar nomes das colunas
        X.columns = X.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
        X.columns = X.columns.str.replace('__+', '_', regex=True)
        X.columns = X.columns.str.strip('_')

        X_train_temporal = X[mask_treino]
        X_test_temporal = X[mask_teste]
        y_train_temporal = y[mask_treino]
        y_test_temporal = y[mask_teste]

        print(f"Registros treino: {len(X_train_temporal):,}")
        print(f"Registros teste: {len(X_test_temporal):,}")
        print(f"Taxa positivos treino: {y_train_temporal.mean()*100:.2f}%")
        print(f"Taxa positivos teste: {y_test_temporal.mean()*100:.2f}%")

        return X_train_temporal, X_test_temporal, y_train_temporal, y_test_temporal

    def treinar_modelos_temporal(X_train, X_test, y_train, y_test, nome_dataset):
        """Treina modelos com split temporal"""

        print(f"\nTreinando modelos temporais - {nome_dataset}...")

        resultados = {}

        # Random Forest
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )

        rf.fit(X_train, y_train)
        y_prob_rf = rf.predict_proba(X_test)[:, 1]
        auc_rf = roc_auc_score(y_test, y_prob_rf)

        print(f"Random Forest AUC: {auc_rf:.3f}")
        resultados['rf'] = {'model': rf, 'y_prob': y_prob_rf, 'auc': auc_rf}

        # LightGBM
        lgbm_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 100,
            'verbose': -1,
            'random_state': 42,
            'is_unbalance': True
        }

        train_data = lgb.Dataset(X_train, label=y_train)
        lgbm_model = lgb.train(lgbm_params, train_data, num_boost_round=500, callbacks=[lgb.log_evaluation(0)])

        y_prob_lgbm = lgbm_model.predict(X_test, num_iteration=lgbm_model.best_iteration)
        auc_lgbm = roc_auc_score(y_test, y_prob_lgbm)

        print(f"LightGBM AUC: {auc_lgbm:.3f}")
        resultados['lgbm'] = {'model': lgbm_model, 'y_prob': y_prob_lgbm, 'auc': auc_lgbm}

        # XGBoost
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

        xgb_model = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            scale_pos_weight=scale_pos_weight,
            eval_metric='logloss'
        )

        xgb_model.fit(X_train, y_train)
        y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]
        auc_xgb = roc_auc_score(y_test, y_prob_xgb)

        print(f"XGBoost AUC: {auc_xgb:.3f}")
        resultados['xgb'] = {'model': xgb_model, 'y_prob': y_prob_xgb, 'auc': auc_xgb}

        return resultados, y_test

    def analisar_decis_temporal(y_test, y_prob, nome_modelo):
        """Analisa decis para valida√ß√£o temporal"""

        df_analise = pd.DataFrame({
            'probabilidade': y_prob,
            'target_real': y_test
        })

        df_analise['decil'] = pd.qcut(
            df_analise['probabilidade'],
            q=10,
            labels=[f'D{i}' for i in range(1, 11)],
            duplicates='drop'
        )

        analise_decis = df_analise.groupby('decil', observed=True).agg({
            'target_real': ['count', 'sum', 'mean']
        }).round(4)

        analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']

        analise_decis['pct_total_conversoes'] = (
            analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
        ).round(2)

        taxa_base = y_test.mean()
        analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)

        # M√©tricas de ranking
        top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
        lift_maximo = analise_decis['lift'].max()

        taxas = analise_decis['taxa_conversao'].values
        crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
        monotonia = crescimentos / (len(taxas) - 1)

        print(f"\nDecis Temporais - {nome_modelo}:")
        print(f"Top 3 decis: {top3_conversoes:.1f}% das convers√µes")
        print(f"Lift m√°ximo: {lift_maximo:.1f}x")
        print(f"Monotonia: {monotonia*100:.1f}%")

        return {
            'top3_conversoes': top3_conversoes,
            'lift_maximo': lift_maximo,
            'monotonia': monotonia,
            'analise_decis': analise_decis
        }

    # Recuperar dados originais
    data_v1_todos, data_v1_devclub, data_v2_todos, data_v2_devclub = recuperar_data_original()

    # Datasets e suas respectivas datas
    datasets_info = [
        (dataset_v1_todos_encoded, data_v1_todos, "V1 TODOS"),
        (dataset_v1_devclub_encoded, data_v1_devclub, "V1 DEVCLUB"),
        (dataset_v2_todos_encoded, data_v2_todos, "V2 TODOS"),
        (dataset_v2_devclub_encoded, data_v2_devclub, "V2 DEVCLUB")
    ]

    resultados_temporais = {}
    metricas_temporais = {}

    # Processar cada dataset
    for df_encoded, data_original, nome_dataset in datasets_info:
        print(f"\n{'='*60}")
        print(f"VALIDA√á√ÉO TEMPORAL - DATASET {nome_dataset}")
        print("=" * 60)

        # Split temporal
        X_train_temp, X_test_temp, y_train_temp, y_test_temp = split_temporal(
            df_encoded, data_original, nome_dataset
        )

        # Treinar modelos
        resultados_dataset, y_test_dataset = treinar_modelos_temporal(
            X_train_temp, X_test_temp, y_train_temp, y_test_temp, nome_dataset
        )

        # Analisar decis
        metricas_dataset = {}
        for modelo in ['rf', 'lgbm', 'xgb']:
            nome_modelo = f"{nome_dataset} {modelo.upper()}"
            metricas_dataset[modelo] = analisar_decis_temporal(
                y_test_dataset, resultados_dataset[modelo]['y_prob'], nome_modelo
            )

        # Salvar resultados
        resultados_temporais[nome_dataset.lower().replace(' ', '_')] = resultados_dataset
        metricas_temporais[nome_dataset.lower().replace(' ', '_')] = metricas_dataset

    # Resumo final dos resultados
    print(f"\n{'='*60}")
    print("RESUMO FINAL - VALIDA√á√ÉO TEMPORAL")
    print("=" * 60)

    # Criar tabela resumo
    resumo_dados = []
    for dataset_key, resultados in resultados_temporais.items():
        for modelo in ['rf', 'lgbm', 'xgb']:
            dataset_nome = dataset_key.replace('_', ' ').upper()
            modelo_nome = modelo.upper()
            auc = resultados[modelo]['auc']
            top3 = metricas_temporais[dataset_key][modelo]['top3_conversoes']
            monotonia = metricas_temporais[dataset_key][modelo]['monotonia'] * 100

            resumo_dados.append({
                'Dataset': dataset_nome,
                'Modelo': modelo_nome,
                'AUC': f"{auc:.3f}",
                'Top3_Conv': f"{top3:.1f}%",
                'Monotonia': f"{monotonia:.1f}%"
            })

    resumo_df = pd.DataFrame(resumo_dados)
    print(resumo_df.to_string(index=False))

    return {
        'resultados_temporais': resultados_temporais,
        'metricas_temporais': metricas_temporais,
        'resumo_final': resumo_df
    }

# Executar valida√ß√£o temporal nos 4 datasets
resultados_validacao_temporal = validacao_temporal()

"""## 22- Baseline + teste cutoff 10/08 + sem UTMs"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
import xgboost as xgb

def validacao_temporal_completa():
    """Valida modelos com testes de UTM e cutoff temporal nos 4 datasets"""

    print("VALIDA√á√ÉO TEMPORAL COMPLETA - TESTES UTM E CUTOFF")
    print("=" * 60)

    def criar_datasets_sem_utm():
        """Cria vers√µes dos datasets sem as features de UTM"""

        print("Criando datasets sem UTM...")

        colunas_utm = ['Source', 'Medium', 'Term']

        datasets_sem_utm = {}

        # Para cada dataset encoded, remover colunas UTM e suas vers√µes one-hot
        for nome, df in [
            ('v1_todos', dataset_v1_todos_encoded),
            ('v1_devclub', dataset_v1_devclub_encoded),
            ('v2_todos', dataset_v2_todos_encoded),
            ('v2_devclub', dataset_v2_devclub_encoded)
        ]:
            df_sem_utm = df.copy()

            # Identificar colunas UTM (originais e one-hot)
            colunas_remover = []
            for col in df_sem_utm.columns:
                for utm in colunas_utm:
                    if col.startswith(f'{utm}_') or col == utm:
                        colunas_remover.append(col)

            # Remover colunas UTM
            df_sem_utm = df_sem_utm.drop(columns=colunas_remover, errors='ignore')
            datasets_sem_utm[nome] = df_sem_utm

            print(f"  {nome}: {len(colunas_remover)} colunas UTM removidas")

        return datasets_sem_utm

    def criar_datasets_cutoff():
        """Cria vers√µes dos datasets com cutoff em 10/08"""

        print("\nCriando datasets com cutoff 10/08...")

        data_cutoff = pd.to_datetime('2025-08-10')

        datasets_cutoff = {}
        datasets_originais = [
            ('v1_todos', dataset_v1_todos, dataset_v1_todos_encoded),
            ('v1_devclub', dataset_v1_devclub, dataset_v1_devclub_encoded),
            ('v2_todos', dataset_v2_todos, dataset_v2_todos_encoded),
            ('v2_devclub', dataset_v2_devclub, dataset_v2_devclub_encoded)
        ]

        for nome, df_original, df_encoded in datasets_originais:
            # Criar m√°scara baseada na data original
            data_dt = pd.to_datetime(df_original['Data'], errors='coerce')
            mask_cutoff = data_dt <= data_cutoff

            # Aplicar cutoff no dataset encoded
            df_cutoff = df_encoded[mask_cutoff].copy()
            datasets_cutoff[nome] = df_cutoff

            total_original = len(df_encoded)
            total_cutoff = len(df_cutoff)
            removidos = total_original - total_cutoff

            print(f"  {nome}: {removidos:,} registros removidos ({removidos/total_original*100:.1f}%)")

        return datasets_cutoff

    def recuperar_datas_originais():
        """Recupera datas originais dos datasets"""
        return {
            'v1_todos': dataset_v1_todos['Data'].copy(),
            'v1_devclub': dataset_v1_devclub['Data'].copy(),
            'v2_todos': dataset_v2_todos['Data'].copy(),
            'v2_devclub': dataset_v2_devclub['Data'].copy()
        }

    def split_temporal_flexivel(df_encoded, data_original, nome_dataset, cutoff_10_08=False):
        """Faz split temporal com op√ß√£o de cutoff em 10/08"""

        # Converter data para datetime
        data_dt = pd.to_datetime(data_original, errors='coerce')

        if cutoff_10_08:
            # Aplicar cutoff em 10/08 nos dados de data tamb√©m
            data_cutoff = pd.to_datetime('2025-08-10')
            mask_cutoff = data_dt <= data_cutoff
            data_dt = data_dt[mask_cutoff]

        # Estat√≠sticas temporais
        data_min = data_dt.min()
        data_max = data_dt.max()

        # Definir data de corte (70% para treino, 30% para teste)
        dias_totais = (data_max - data_min).days
        dias_treino = int(dias_totais * 0.7)
        data_corte = data_min + pd.Timedelta(days=dias_treino)

        # Criar m√°scaras temporais
        mask_treino = data_dt <= data_corte
        mask_teste = data_dt > data_corte

        # Separar dados
        X = df_encoded.drop(columns=['target'])
        y = df_encoded['target']

        # Limpar nomes das colunas
        X.columns = X.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
        X.columns = X.columns.str.replace('__+', '_', regex=True)
        X.columns = X.columns.str.strip('_')

        X_train = X[mask_treino]
        X_test = X[mask_teste]
        y_train = y[mask_treino]
        y_test = y[mask_teste]

        return X_train, X_test, y_train, y_test, data_min, data_max, data_corte

    def treinar_modelos_padrao(X_train, X_test, y_train, y_test):
        """Treina os 3 modelos com par√¢metros fixos"""

        resultados = {}

        # Random Forest
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        rf.fit(X_train, y_train)
        y_prob_rf = rf.predict_proba(X_test)[:, 1]
        auc_rf = roc_auc_score(y_test, y_prob_rf)
        resultados['rf'] = {'model': rf, 'y_prob': y_prob_rf, 'auc': auc_rf}

        # LightGBM
        lgbm_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 100,
            'verbose': -1,
            'random_state': 42,
            'is_unbalance': True
        }
        train_data = lgb.Dataset(X_train, label=y_train)
        lgbm_model = lgb.train(lgbm_params, train_data, num_boost_round=500, callbacks=[lgb.log_evaluation(0)])
        y_prob_lgbm = lgbm_model.predict(X_test, num_iteration=lgbm_model.best_iteration)
        auc_lgbm = roc_auc_score(y_test, y_prob_lgbm)
        resultados['lgbm'] = {'model': lgbm_model, 'y_prob': y_prob_lgbm, 'auc': auc_lgbm}

        # XGBoost
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        xgb_model = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            scale_pos_weight=scale_pos_weight,
            eval_metric='logloss'
        )
        xgb_model.fit(X_train, y_train)
        y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]
        auc_xgb = roc_auc_score(y_test, y_prob_xgb)
        resultados['xgb'] = {'model': xgb_model, 'y_prob': y_prob_xgb, 'auc': auc_xgb}

        return resultados

    def analisar_decis_rapido(y_test, y_prob):
        """An√°lise r√°pida de decis"""

        df_analise = pd.DataFrame({
            'probabilidade': y_prob,
            'target_real': y_test
        })

        df_analise['decil'] = pd.qcut(
            df_analise['probabilidade'],
            q=10,
            labels=[f'D{i}' for i in range(1, 11)],
            duplicates='drop'
        )

        analise_decis = df_analise.groupby('decil', observed=True).agg({
            'target_real': ['count', 'sum', 'mean']
        }).round(4)

        analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']

        analise_decis['pct_total_conversoes'] = (
            analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
        ).round(2)

        taxa_base = y_test.mean()
        analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)

        # M√©tricas
        top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
        lift_maximo = analise_decis['lift'].max()

        taxas = analise_decis['taxa_conversao'].values
        crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
        monotonia = crescimentos / (len(taxas) - 1)

        return {
            'top3_conversoes': top3_conversoes,
            'lift_maximo': lift_maximo,
            'monotonia': monotonia
        }

    # Preparar dados
    datasets_sem_utm = criar_datasets_sem_utm()
    datasets_cutoff = criar_datasets_cutoff()
    datas_originais = recuperar_datas_originais()

    # Lista de todos os testes
    configuracoes = [
        # Teste UTM
        ('v1_todos', dataset_v1_todos_encoded, 'Com_UTM'),
        ('v1_todos', datasets_sem_utm['v1_todos'], 'Sem_UTM'),
        ('v1_devclub', dataset_v1_devclub_encoded, 'Com_UTM'),
        ('v1_devclub', datasets_sem_utm['v1_devclub'], 'Sem_UTM'),
        ('v2_todos', dataset_v2_todos_encoded, 'Com_UTM'),
        ('v2_todos', datasets_sem_utm['v2_todos'], 'Sem_UTM'),
        ('v2_devclub', dataset_v2_devclub_encoded, 'Com_UTM'),
        ('v2_devclub', datasets_sem_utm['v2_devclub'], 'Sem_UTM'),

        # Teste Cutoff
        ('v1_todos', dataset_v1_todos_encoded, 'Data_Completa'),
        ('v1_todos', datasets_cutoff['v1_todos'], 'Cutoff_10_08'),
        ('v1_devclub', dataset_v1_devclub_encoded, 'Data_Completa'),
        ('v1_devclub', datasets_cutoff['v1_devclub'], 'Cutoff_10_08'),
        ('v2_todos', dataset_v2_todos_encoded, 'Data_Completa'),
        ('v2_todos', datasets_cutoff['v2_todos'], 'Cutoff_10_08'),
        ('v2_devclub', dataset_v2_devclub_encoded, 'Data_Completa'),
        ('v2_devclub', datasets_cutoff['v2_devclub'], 'Cutoff_10_08'),
    ]

    # Executar todos os testes
    resultados_completos = []

    print(f"\n{'='*60}")
    print("EXECUTANDO TODOS OS TESTES")
    print("=" * 60)

    for i, (dataset_nome, df_encoded, condicao) in enumerate(configuracoes, 1):
        print(f"\n[{i:2d}/24] {dataset_nome.upper()} - {condicao}")

        # Determinar se usar cutoff nos dados de data
        usar_cutoff_data = condicao == 'Cutoff_10_08'
        data_original = datas_originais[dataset_nome]

        if usar_cutoff_data:
            # Aplicar cutoff tamb√©m nos dados de data
            data_cutoff = pd.to_datetime('2025-08-10')
            data_dt = pd.to_datetime(data_original, errors='coerce')
            mask_cutoff = data_dt <= data_cutoff
            data_original = data_original[mask_cutoff]

        # Split temporal
        X_train, X_test, y_train, y_test, data_min, data_max, data_corte = split_temporal_flexivel(
            df_encoded, data_original, dataset_nome, usar_cutoff_data
        )

        print(f"  Per√≠odo: {data_min.strftime('%Y-%m-%d')} a {data_max.strftime('%Y-%m-%d')}")
        print(f"  Corte: {data_corte.strftime('%Y-%m-%d')}")
        print(f"  Treino: {len(X_train):,} | Teste: {len(X_test):,}")
        print(f"  Taxa treino: {y_train.mean()*100:.2f}% | Taxa teste: {y_test.mean()*100:.2f}%")

        # Treinar modelos
        resultados_modelos = treinar_modelos_padrao(X_train, X_test, y_train, y_test)

        # Analisar cada modelo
        for modelo in ['rf', 'lgbm', 'xgb']:
            metricas = analisar_decis_rapido(y_test, resultados_modelos[modelo]['y_prob'])

            resultados_completos.append({
                'Dataset': dataset_nome.replace('_', ' ').upper(),
                'Modelo': modelo.upper(),
                'Condicao': condicao,
                'AUC': resultados_modelos[modelo]['auc'],
                'Top3_Conv': metricas['top3_conversoes'],
                'Lift_Max': metricas['lift_maximo'],
                'Monotonia': metricas['monotonia'] * 100
            })

    # Criar tabela final
    print(f"\n{'='*80}")
    print("TABELA COMPARATIVA COMPLETA")
    print("=" * 80)

    df_resultados = pd.DataFrame(resultados_completos)

    # Formatar para exibi√ß√£o
    df_display = df_resultados.copy()
    df_display['AUC'] = df_display['AUC'].apply(lambda x: f"{x:.3f}")
    df_display['Top3_Conv'] = df_display['Top3_Conv'].apply(lambda x: f"{x:.1f}%")
    df_display['Lift_Max'] = df_display['Lift_Max'].apply(lambda x: f"{x:.1f}x")
    df_display['Monotonia'] = df_display['Monotonia'].apply(lambda x: f"{x:.1f}%")

    print(df_display.to_string(index=False))

    # An√°lises espec√≠ficas
    print(f"\n{'='*80}")
    print("AN√ÅLISES COMPARATIVAS")
    print("=" * 80)

    # Impacto das UTMs
    print("\nIMPACTO DAS UTMs (AUC m√©dio por dataset):")
    print("-" * 50)

    df_utm = df_resultados[df_resultados['Condicao'].isin(['Com_UTM', 'Sem_UTM'])]
    utm_analysis = df_utm.groupby(['Dataset', 'Condicao'])['AUC'].mean().unstack()

    for dataset in utm_analysis.index:
        com_utm = utm_analysis.loc[dataset, 'Com_UTM']
        sem_utm = utm_analysis.loc[dataset, 'Sem_UTM']
        diferenca = ((com_utm - sem_utm) / sem_utm) * 100
        status = "MELHORA" if diferenca > 2 else "EST√ÅVEL" if abs(diferenca) <= 2 else "PIORA"
        print(f"{dataset}: {diferenca:+.1f}% - {status}")

    # Impacto do cutoff
    print(f"\nIMPACTO DO CUTOFF 10/08 (AUC m√©dio por dataset):")
    print("-" * 50)

    df_cutoff = df_resultados[df_resultados['Condicao'].isin(['Data_Completa', 'Cutoff_10_08'])]
    cutoff_analysis = df_cutoff.groupby(['Dataset', 'Condicao'])['AUC'].mean().unstack()

    for dataset in cutoff_analysis.index:
        completa = cutoff_analysis.loc[dataset, 'Data_Completa']
        cutoff = cutoff_analysis.loc[dataset, 'Cutoff_10_08']
        diferenca = ((cutoff - completa) / completa) * 100
        status = "MELHORA" if diferenca > 2 else "EST√ÅVEL" if abs(diferenca) <= 2 else "PIORA"
        print(f"{dataset}: {diferenca:+.1f}% - {status}")

    return df_resultados

# Executar valida√ß√£o temporal completa
resultados_validacao_completa = validacao_temporal_completa()

"""## 23- Investiga√ß√£o
Teste de ensemble
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import ParameterGrid
import lightgbm as lgb
import xgboost as xgb

def teste_ensemble_votacao_ponderada():
    """Testa ensemble por vota√ß√£o ponderada nos melhores datasets"""

    print("TESTE DE ENSEMBLE POR VOTA√á√ÉO PONDERADA")
    print("=" * 50)

    def recuperar_datas_melhores_datasets():
        """Recupera datas dos melhores datasets identificados"""
        return {
            'v1_devclub_utm': dataset_v1_devclub['Data'].copy(),
            'v2_devclub_utm': dataset_v2_devclub['Data'].copy(),
            'v1_devclub_cutoff': dataset_v1_devclub['Data'].copy()
        }

    def criar_datasets_cutoff_v1():
        """Cria vers√£o cutoff do V1 DEVCLUB"""

        data_cutoff = pd.to_datetime('2025-08-10')
        data_dt = pd.to_datetime(dataset_v1_devclub['Data'], errors='coerce')
        mask_cutoff = data_dt <= data_cutoff

        v1_devclub_cutoff = dataset_v1_devclub_encoded[mask_cutoff].copy()

        print(f"V1 DEVCLUB CUTOFF criado: {len(v1_devclub_cutoff):,} registros")
        return v1_devclub_cutoff

    def split_temporal_ensemble(df_encoded, data_original, nome_dataset, usar_cutoff=False):
        """Split temporal para ensemble"""

        data_dt = pd.to_datetime(data_original, errors='coerce')

        if usar_cutoff:
            data_cutoff = pd.to_datetime('2025-08-10')
            mask_cutoff = data_dt <= data_cutoff
            data_dt = data_dt[mask_cutoff]

        # Split 70/30
        data_min = data_dt.min()
        data_max = data_dt.max()
        dias_totais = (data_max - data_min).days
        dias_treino = int(dias_totais * 0.7)
        data_corte = data_min + pd.Timedelta(days=dias_treino)

        mask_treino = data_dt <= data_corte
        mask_teste = data_dt > data_corte

        X = df_encoded.drop(columns=['target'])
        y = df_encoded['target']

        # Limpar nomes das colunas
        X.columns = X.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
        X.columns = X.columns.str.replace('__+', '_', regex=True)
        X.columns = X.columns.str.strip('_')

        X_train = X[mask_treino]
        X_test = X[mask_teste]
        y_train = y[mask_treino]
        y_test = y[mask_teste]

        return X_train, X_test, y_train, y_test

    def treinar_modelos_base(X_train, X_test, y_train, y_test):
        """Treina os 3 modelos base com par√¢metros fixos"""

        modelos = {}

        # Random Forest
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        rf.fit(X_train, y_train)
        prob_rf = rf.predict_proba(X_test)[:, 1]
        auc_rf = roc_auc_score(y_test, prob_rf)
        modelos['rf'] = {'model': rf, 'prob': prob_rf, 'auc': auc_rf}

        # LightGBM
        lgbm_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 100,
            'verbose': -1,
            'random_state': 42,
            'is_unbalance': True
        }
        train_data = lgb.Dataset(X_train, label=y_train)
        lgbm = lgb.train(lgbm_params, train_data, num_boost_round=500, callbacks=[lgb.log_evaluation(0)])
        prob_lgbm = lgbm.predict(X_test, num_iteration=lgbm.best_iteration)
        auc_lgbm = roc_auc_score(y_test, prob_lgbm)
        modelos['lgbm'] = {'model': lgbm, 'prob': prob_lgbm, 'auc': auc_lgbm}

        # XGBoost
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        xgb_model = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            scale_pos_weight=scale_pos_weight,
            eval_metric='logloss'
        )
        xgb_model.fit(X_train, y_train)
        prob_xgb = xgb_model.predict_proba(X_test)[:, 1]
        auc_xgb = roc_auc_score(y_test, prob_xgb)
        modelos['xgb'] = {'model': xgb_model, 'prob': prob_xgb, 'auc': auc_xgb}

        return modelos

    def criar_ensembles(modelos, y_test):
        """Cria diferentes tipos de ensemble"""

        prob_rf = modelos['rf']['prob']
        prob_lgbm = modelos['lgbm']['prob']
        prob_xgb = modelos['xgb']['prob']

        auc_rf = modelos['rf']['auc']
        auc_lgbm = modelos['lgbm']['auc']
        auc_xgb = modelos['xgb']['auc']

        ensembles = {}

        # 1. Ensemble Uniforme
        prob_uniforme = (prob_rf + prob_lgbm + prob_xgb) / 3
        auc_uniforme = roc_auc_score(y_test, prob_uniforme)
        ensembles['uniforme'] = {'prob': prob_uniforme, 'auc': auc_uniforme, 'pesos': [1/3, 1/3, 1/3]}

        # 2. Ensemble Baseado em AUC
        soma_aucs = auc_rf + auc_lgbm + auc_xgb
        peso_rf = auc_rf / soma_aucs
        peso_lgbm = auc_lgbm / soma_aucs
        peso_xgb = auc_xgb / soma_aucs

        prob_auc = peso_rf * prob_rf + peso_lgbm * prob_lgbm + peso_xgb * prob_xgb
        auc_auc = roc_auc_score(y_test, prob_auc)
        ensembles['baseado_auc'] = {
            'prob': prob_auc,
            'auc': auc_auc,
            'pesos': [peso_rf, peso_lgbm, peso_xgb]
        }

        # 3. Ensemble Otimizado por Grid Search
        melhor_auc = 0
        melhores_pesos = None
        melhor_prob = None

        # Grid de pesos (soma = 1.0)
        pesos_grid = []
        for w1 in np.arange(0.0, 1.1, 0.1):
            for w2 in np.arange(0.0, 1.1 - w1, 0.1):
                w3 = 1.0 - w1 - w2
                if abs(w1 + w2 + w3 - 1.0) < 0.001:  # Garantir soma = 1
                    pesos_grid.append([w1, w2, w3])

        for pesos in pesos_grid:
            w1, w2, w3 = pesos
            prob_teste = w1 * prob_rf + w2 * prob_lgbm + w3 * prob_xgb
            auc_teste = roc_auc_score(y_test, prob_teste)

            if auc_teste > melhor_auc:
                melhor_auc = auc_teste
                melhores_pesos = pesos
                melhor_prob = prob_teste

        ensembles['otimizado'] = {
            'prob': melhor_prob,
            'auc': melhor_auc,
            'pesos': melhores_pesos
        }

        return ensembles

    def analisar_ensemble_decis(y_test, prob_ensemble, nome_ensemble):
        """An√°lise de decis para ensemble"""

        df_analise = pd.DataFrame({
            'probabilidade': prob_ensemble,
            'target_real': y_test
        })

        df_analise['decil'] = pd.qcut(
            df_analise['probabilidade'],
            q=10,
            labels=[f'D{i}' for i in range(1, 11)],
            duplicates='drop'
        )

        analise_decis = df_analise.groupby('decil', observed=True).agg({
            'target_real': ['count', 'sum', 'mean']
        }).round(4)

        analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']

        analise_decis['pct_total_conversoes'] = (
            analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
        ).round(2)

        taxa_base = y_test.mean()
        analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)

        # M√©tricas
        top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
        lift_maximo = analise_decis['lift'].max()

        taxas = analise_decis['taxa_conversao'].values
        crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
        monotonia = crescimentos / (len(taxas) - 1)

        return {
            'top3_conversoes': top3_conversoes,
            'lift_maximo': lift_maximo,
            'monotonia': monotonia,
            'analise_decis': analise_decis
        }

    # Preparar dados
    datas_originais = recuperar_datas_melhores_datasets()
    v1_devclub_cutoff_encoded = criar_datasets_cutoff_v1()

    # Configura√ß√µes de teste
    configuracoes = [
        {
            'nome': 'V1_DEVCLUB_UTM',
            'dataset': dataset_v1_devclub_encoded,
            'data': datas_originais['v1_devclub_utm'],
            'usar_cutoff': False
        },
        {
            'nome': 'V2_DEVCLUB_UTM',
            'dataset': dataset_v2_devclub_encoded,
            'data': datas_originais['v2_devclub_utm'],
            'usar_cutoff': False
        },
        {
            'nome': 'V1_DEVCLUB_CUTOFF',
            'dataset': v1_devclub_cutoff_encoded,
            'data': datas_originais['v1_devclub_cutoff'],
            'usar_cutoff': True
        }
    ]

    # Executar testes
    resultados_finais = []

    for config in configuracoes:
        print(f"\n{'='*60}")
        print(f"TESTANDO ENSEMBLE - {config['nome']}")
        print("=" * 60)

        # Split temporal
        X_train, X_test, y_train, y_test = split_temporal_ensemble(
            config['dataset'],
            config['data'],
            config['nome'],
            config['usar_cutoff']
        )

        print(f"Treino: {len(X_train):,} | Teste: {len(X_test):,}")
        print(f"Taxa treino: {y_train.mean()*100:.2f}% | Taxa teste: {y_test.mean()*100:.2f}%")

        # Treinar modelos base
        print(f"\nTreinando modelos base...")
        modelos_base = treinar_modelos_base(X_train, X_test, y_train, y_test)

        # Criar ensembles
        print(f"Criando ensembles...")
        ensembles = criar_ensembles(modelos_base, y_test)

        # Resultados dos modelos individuais
        for modelo in ['rf', 'lgbm', 'xgb']:
            metricas = analisar_ensemble_decis(
                y_test,
                modelos_base[modelo]['prob'],
                f"{config['nome']}_{modelo.upper()}"
            )

            resultados_finais.append({
                'Dataset': config['nome'],
                'Tipo': 'Individual',
                'Modelo': modelo.upper(),
                'AUC': modelos_base[modelo]['auc'],
                'Top3_Conv': metricas['top3_conversoes'],
                'Lift_Max': metricas['lift_maximo'],
                'Monotonia': metricas['monotonia'] * 100,
                'Pesos': 'N/A'
            })

        # Resultados dos ensembles
        for tipo_ensemble, dados in ensembles.items():
            metricas = analisar_ensemble_decis(
                y_test,
                dados['prob'],
                f"{config['nome']}_{tipo_ensemble}"
            )

            pesos_str = f"RF:{dados['pesos'][0]:.2f}, LGBM:{dados['pesos'][1]:.2f}, XGB:{dados['pesos'][2]:.2f}"

            resultados_finais.append({
                'Dataset': config['nome'],
                'Tipo': 'Ensemble',
                'Modelo': tipo_ensemble.upper(),
                'AUC': dados['auc'],
                'Top3_Conv': metricas['top3_conversoes'],
                'Lift_Max': metricas['lift_maximo'],
                'Monotonia': metricas['monotonia'] * 100,
                'Pesos': pesos_str
            })

        # Imprimir resultados do dataset atual
        print(f"\nResultados {config['nome']}:")
        print("-" * 40)
        print("MODELOS INDIVIDUAIS:")
        for modelo in ['rf', 'lgbm', 'xgb']:
            auc = modelos_base[modelo]['auc']
            print(f"  {modelo.upper()}: AUC {auc:.3f}")

        print("\nENSEMBLES:")
        for tipo, dados in ensembles.items():
            print(f"  {tipo.upper()}: AUC {dados['auc']:.3f} | Pesos: {dados['pesos'][0]:.2f}, {dados['pesos'][1]:.2f}, {dados['pesos'][2]:.2f}")

    # Tabela final completa
    print(f"\n{'='*80}")
    print("TABELA COMPARATIVA COMPLETA - ENSEMBLE vs INDIVIDUAIS")
    print("=" * 80)

    df_resultados = pd.DataFrame(resultados_finais)

    # Formatar para exibi√ß√£o
    df_display = df_resultados.copy()
    df_display['AUC'] = df_display['AUC'].apply(lambda x: f"{x:.3f}")
    df_display['Top3_Conv'] = df_display['Top3_Conv'].apply(lambda x: f"{x:.1f}%")
    df_display['Lift_Max'] = df_display['Lift_Max'].apply(lambda x: f"{x:.1f}x")
    df_display['Monotonia'] = df_display['Monotonia'].apply(lambda x: f"{x:.1f}%")

    print(df_display.to_string(index=False))

    # An√°lise de melhoria
    print(f"\n{'='*80}")
    print("AN√ÅLISE DE MELHORIA DOS ENSEMBLES")
    print("=" * 80)

    for dataset in df_resultados['Dataset'].unique():
        print(f"\n{dataset}:")
        print("-" * 30)

        # Melhor individual
        individuais = df_resultados[
            (df_resultados['Dataset'] == dataset) &
            (df_resultados['Tipo'] == 'Individual')
        ]
        melhor_individual = individuais.loc[individuais['AUC'].idxmax()]

        # Melhor ensemble
        ensembles_dataset = df_resultados[
            (df_resultados['Dataset'] == dataset) &
            (df_resultados['Tipo'] == 'Ensemble')
        ]
        melhor_ensemble = ensembles_dataset.loc[ensembles_dataset['AUC'].idxmax()]

        melhoria_auc = ((melhor_ensemble['AUC'] - melhor_individual['AUC']) / melhor_individual['AUC']) * 100

        print(f"Melhor Individual: {melhor_individual['Modelo']} (AUC: {melhor_individual['AUC']:.3f})")
        print(f"Melhor Ensemble: {melhor_ensemble['Modelo']} (AUC: {melhor_ensemble['AUC']:.3f})")
        print(f"Melhoria: {melhoria_auc:+.1f}%")
        print(f"Pesos √≥timos: {melhor_ensemble['Pesos']}")

    return df_resultados

# Executar teste de ensemble por vota√ß√£o ponderada
resultados_ensemble = teste_ensemble_votacao_ponderada()

"""## 23.1- Investiga√ß√£o
Teste de estabilidade do ranking
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
import xgboost as xgb

def teste_estabilidade_ranking():
    """Testa estabilidade do ranking dos top 10 modelos ao longo do tempo"""

    print("TESTE DE ESTABILIDADE DO RANKING - TOP 10 MODELOS")
    print("=" * 60)

    def recuperar_top_10_modelos():
        """Recupera top 10 modelos baseado nos resultados dos 24 testes"""

        # Dados dos 24 testes (simulados baseados nos seus resultados)
        modelos_testados = [
            # V1 DEVCLUB
            {'dataset': 'V1_DEVCLUB', 'condicao': 'Com_UTM', 'modelo': 'RF', 'auc': 0.649, 'top3': 48.7, 'lift_max': 2.3, 'monotonia': 77.8},
            {'dataset': 'V1_DEVCLUB', 'condicao': 'Cutoff_10_08', 'modelo': 'RF', 'auc': 0.641, 'top3': 41.5, 'lift_max': 2.1, 'monotonia': 66.7},
            {'dataset': 'V1_DEVCLUB', 'condicao': 'Cutoff_10_08', 'modelo': 'LGBM', 'auc': 0.644, 'top3': 45.4, 'lift_max': 2.0, 'monotonia': 66.7},
            {'dataset': 'V1_DEVCLUB', 'condicao': 'Com_UTM', 'modelo': 'LGBM', 'auc': 0.612, 'top3': 40.3, 'lift_max': 1.9, 'monotonia': 66.7},
            {'dataset': 'V1_DEVCLUB', 'condicao': 'Cutoff_10_08', 'modelo': 'XGB', 'auc': 0.606, 'top3': 44.6, 'lift_max': 2.0, 'monotonia': 55.6},
            # V2 DEVCLUB
            {'dataset': 'V2_DEVCLUB', 'condicao': 'Cutoff_10_08', 'modelo': 'RF', 'auc': 0.620, 'top3': 43.8, 'lift_max': 2.0, 'monotonia': 55.6},
            {'dataset': 'V2_DEVCLUB', 'condicao': 'Com_UTM', 'modelo': 'RF', 'auc': 0.607, 'top3': 42.4, 'lift_max': 1.6, 'monotonia': 77.8},
            {'dataset': 'V2_DEVCLUB', 'condicao': 'Cutoff_10_08', 'modelo': 'LGBM', 'auc': 0.602, 'top3': 42.5, 'lift_max': 1.9, 'monotonia': 77.8},
            {'dataset': 'V2_DEVCLUB', 'condicao': 'Com_UTM', 'modelo': 'LGBM', 'auc': 0.597, 'top3': 40.0, 'lift_max': 1.6, 'monotonia': 77.8},
            {'dataset': 'V2_DEVCLUB', 'condicao': 'Com_UTM', 'modelo': 'XGB', 'auc': 0.596, 'top3': 43.6, 'lift_max': 1.6, 'monotonia': 66.7}
        ]

        # Calcular score composto para ranking
        for modelo in modelos_testados:
            score_composto = (
                modelo['auc'] * 0.4 +  # 40% peso no AUC
                (modelo['top3'] / 100) * 0.25 +  # 25% peso no Top3
                (modelo['lift_max'] / 3) * 0.25 +  # 25% peso no Lift (normalizado por 3)
                (modelo['monotonia'] / 100) * 0.1  # 10% peso na Monotonia
            )
            modelo['score_composto'] = score_composto

        # Ordenar por score composto e pegar top 10
        modelos_ordenados = sorted(modelos_testados, key=lambda x: x['score_composto'], reverse=True)
        top_10 = modelos_ordenados[:10]

        print("TOP 10 MODELOS SELECIONADOS (por score composto):")
        print("-" * 50)
        for i, modelo in enumerate(top_10, 1):
            print(f"{i:2d}. {modelo['dataset']} {modelo['modelo']} ({modelo['condicao']}) - Score: {modelo['score_composto']:.3f}")

        return top_10

    def preparar_datasets_teste():
        """Prepara datasets para teste de estabilidade temporal"""

        # Mapear configura√ß√µes para datasets reais
        configuracoes_datasets = {
            ('V1_DEVCLUB', 'Com_UTM'): (dataset_v1_devclub_encoded, dataset_v1_devclub['Data']),
            ('V1_DEVCLUB', 'Cutoff_10_08'): (None, dataset_v1_devclub['Data']),  # Ser√° criado
            ('V2_DEVCLUB', 'Com_UTM'): (dataset_v2_devclub_encoded, dataset_v2_devclub['Data']),
            ('V2_DEVCLUB', 'Cutoff_10_08'): (None, dataset_v2_devclub['Data'])   # Ser√° criado
        }

        # Criar datasets cutoff se necess√°rio
        for key, (df, data) in configuracoes_datasets.items():
            if df is None and 'Cutoff_10_08' in key[1]:
                if 'V1_DEVCLUB' in key[0]:
                    base_df = dataset_v1_devclub_encoded
                else:
                    base_df = dataset_v2_devclub_encoded

                data_cutoff = pd.to_datetime('2025-08-10')
                data_dt = pd.to_datetime(data, errors='coerce')
                mask_cutoff = data_dt <= data_cutoff
                df_cutoff = base_df[mask_cutoff].copy()
                configuracoes_datasets[key] = (df_cutoff, data[mask_cutoff])

        return configuracoes_datasets

    def split_temporal_multiplo(df_encoded, data_original, n_splits=5):
        """Cria m√∫ltiplos splits temporais para teste de estabilidade"""

        data_dt = pd.to_datetime(data_original, errors='coerce')
        data_min = data_dt.min()
        data_max = data_dt.max()

        # Criar janelas temporais deslizantes
        dias_totais = (data_max - data_min).days
        tamanho_janela = max(90, dias_totais // (n_splits + 1))  # M√≠nimo 90 dias por janela

        splits = []

        for i in range(n_splits):
            # Janela de treino fixa (primeiros 70% dos dados dispon√≠veis at√© esse ponto)
            dias_treino_fim = tamanho_janela * (i + 2)
            data_treino_max = data_min + pd.Timedelta(days=int(dias_treino_fim * 0.7))

            # Janela de teste (pr√≥ximos 30%)
            data_teste_min = data_treino_max
            data_teste_max = data_min + pd.Timedelta(days=dias_treino_fim)

            # Verificar se h√° dados suficientes
            if data_teste_max > data_max:
                break

            mask_treino = (data_dt >= data_min) & (data_dt <= data_treino_max)
            mask_teste = (data_dt > data_teste_min) & (data_dt <= data_teste_max)

            # Verificar se h√° positivos suficientes
            if df_encoded[mask_teste]['target'].sum() < 10:  # M√≠nimo 10 positivos
                continue

            splits.append({
                'treino_mask': mask_treino,
                'teste_mask': mask_teste,
                'periodo': f"{data_treino_max.strftime('%m/%d')} a {data_teste_max.strftime('%m/%d')}"
            })

        return splits

    def treinar_modelo_config(X_train, X_test, y_train, y_test, modelo_tipo):
        """Treina modelo espec√≠fico com par√¢metros fixos"""

        # Limpar nomes das colunas
        X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
        X_train.columns = X_train.columns.str.replace('__+', '_', regex=True)
        X_train.columns = X_train.columns.str.strip('_')

        X_test.columns = X_test.columns

        if modelo_tipo == 'RF':
            modelo = RandomForestClassifier(
                n_estimators=100, max_depth=10, random_state=42,
                class_weight='balanced', n_jobs=-1
            )
            modelo.fit(X_train, y_train)
            prob = modelo.predict_proba(X_test)[:, 1]

        elif modelo_tipo == 'LGBM':
            params = {
                'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt',
                'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8,
                'bagging_fraction': 0.8, 'bagging_freq': 5, 'min_child_samples': 100,
                'verbose': -1, 'random_state': 42, 'is_unbalance': True
            }
            train_data = lgb.Dataset(X_train, label=y_train)
            modelo = lgb.train(params, train_data, num_boost_round=500, callbacks=[lgb.log_evaluation(0)])
            prob = modelo.predict(X_test, num_iteration=modelo.best_iteration)

        elif modelo_tipo == 'XGB':
            scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
            modelo = xgb.XGBClassifier(
                n_estimators=500, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8, random_state=42,
                scale_pos_weight=scale_pos_weight, eval_metric='logloss'
            )
            modelo.fit(X_train, y_train)
            prob = modelo.predict_proba(X_test)[:, 1]

        return prob

    def calcular_metricas_estabilidade(y_test, prob):
        """Calcula m√©tricas de ranking para um per√≠odo"""

        df_analise = pd.DataFrame({
            'probabilidade': prob,
            'target_real': y_test
        })

        df_analise['decil'] = pd.qcut(
            df_analise['probabilidade'], q=10,
            labels=[f'D{i}' for i in range(1, 11)], duplicates='drop'
        )

        analise_decis = df_analise.groupby('decil', observed=True).agg({
            'target_real': ['count', 'sum', 'mean']
        }).round(4)

        analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']

        if analise_decis['conversoes'].sum() == 0:
            return None

        analise_decis['pct_total_conversoes'] = (
            analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
        ).round(2)

        taxa_base = y_test.mean()
        if taxa_base > 0:
            analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)
        else:
            analise_decis['lift'] = 0

        # M√©tricas
        top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
        lift_maximo = analise_decis['lift'].max() if len(analise_decis) > 0 else 0

        taxas = analise_decis['taxa_conversao'].values
        if len(taxas) > 1:
            crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
            monotonia = crescimentos / (len(taxas) - 1)
        else:
            monotonia = 1.0

        auc = roc_auc_score(y_test, prob)

        return {
            'auc': auc,
            'top3_conversoes': top3_conversoes,
            'lift_maximo': lift_maximo,
            'monotonia': monotonia,
            'n_positivos': y_test.sum(),
            'n_total': len(y_test)
        }

    # Executar teste de estabilidade
    top_10_modelos = recuperar_top_10_modelos()
    datasets_config = preparar_datasets_teste()

    resultados_estabilidade = []

    print(f"\n{'='*60}")
    print("EXECUTANDO TESTE DE ESTABILIDADE TEMPORAL")
    print("=" * 60)

    for i, modelo_config in enumerate(top_10_modelos[:5], 1):  # Testar top 5 por tempo
        dataset_key = (modelo_config['dataset'], modelo_config['condicao'])
        df_encoded, data_original = datasets_config[dataset_key]

        print(f"\n[{i}/5] {modelo_config['dataset']} {modelo_config['modelo']} ({modelo_config['condicao']})")

        # Criar splits temporais
        splits = split_temporal_multiplo(df_encoded, data_original, n_splits=4)

        if len(splits) < 3:
            print(f"  Dados insuficientes para splits temporais (apenas {len(splits)} per√≠odos)")
            continue

        metricas_periodo = []

        for j, split in enumerate(splits):
            X = df_encoded.drop(columns=['target'])
            y = df_encoded['target']

            X_train = X[split['treino_mask']]
            X_test = X[split['teste_mask']]
            y_train = y[split['treino_mask']]
            y_test = y[split['teste_mask']]

            if len(X_train) < 1000 or len(X_test) < 100:
                continue

            print(f"  Per√≠odo {j+1} ({split['periodo']}): Treino {len(X_train):,} | Teste {len(X_test):,} | Positivos {y_test.sum()}")

            try:
                prob = treinar_modelo_config(X_train, X_test, y_train, y_test, modelo_config['modelo'])
                metricas = calcular_metricas_estabilidade(y_test, prob)

                if metricas:
                    metricas['periodo'] = split['periodo']
                    metricas_periodo.append(metricas)

            except Exception as e:
                print(f"    Erro no per√≠odo {j+1}: {str(e)[:50]}...")
                continue

        if len(metricas_periodo) >= 3:
            # Calcular estabilidade
            aucs = [m['auc'] for m in metricas_periodo]
            top3s = [m['top3_conversoes'] for m in metricas_periodo]
            lifts = [m['lift_maximo'] for m in metricas_periodo]
            monotonias = [m['monotonia'] for m in metricas_periodo]

            estabilidade = {
                'modelo': f"{modelo_config['dataset']} {modelo_config['modelo']} ({modelo_config['condicao']})",
                'n_periodos': len(metricas_periodo),
                'auc_media': np.mean(aucs),
                'auc_std': np.std(aucs),
                'auc_cv': np.std(aucs) / np.mean(aucs),  # Coeficiente de varia√ß√£o
                'top3_media': np.mean(top3s),
                'top3_std': np.std(top3s),
                'lift_media': np.mean(lifts),
                'lift_std': np.std(lifts),
                'monotonia_media': np.mean(monotonias),
                'monotonia_std': np.std(monotonias),
                'score_estabilidade': 1 / (1 + np.std(aucs))  # Quanto menor std, maior estabilidade
            }

            resultados_estabilidade.append(estabilidade)

            print(f"  Estabilidade AUC: {np.mean(aucs):.3f} ¬± {np.std(aucs):.3f} (CV: {np.std(aucs)/np.mean(aucs)*100:.1f}%)")

        else:
            print(f"  Per√≠odos insuficientes para an√°lise de estabilidade ({len(metricas_periodo)})")

    # Tabela final de estabilidade
    if resultados_estabilidade:
        print(f"\n{'='*80}")
        print("RANKING DE ESTABILIDADE TEMPORAL")
        print("=" * 80)

        df_estabilidade = pd.DataFrame(resultados_estabilidade)
        df_estabilidade = df_estabilidade.sort_values('score_estabilidade', ascending=False)

        # Formata√ß√£o para exibi√ß√£o
        df_display = df_estabilidade.copy()
        df_display['auc_media'] = df_display['auc_media'].apply(lambda x: f"{x:.3f}")
        df_display['auc_std'] = df_display['auc_std'].apply(lambda x: f"¬±{x:.3f}")
        df_display['auc_cv'] = df_display['auc_cv'].apply(lambda x: f"{x*100:.1f}%")
        df_display['top3_media'] = df_display['top3_media'].apply(lambda x: f"{x:.1f}%")
        df_display['monotonia_media'] = df_display['monotonia_media'].apply(lambda x: f"{x*100:.1f}%")
        df_display['score_estabilidade'] = df_display['score_estabilidade'].apply(lambda x: f"{x:.3f}")

        print(df_display[['modelo', 'n_periodos', 'auc_media', 'auc_std', 'auc_cv',
                         'top3_media', 'monotonia_media', 'score_estabilidade']].to_string(index=False))

        # An√°lise de estabilidade
        print(f"\n{'='*80}")
        print("AN√ÅLISE DE ESTABILIDADE")
        print("=" * 80)

        modelo_mais_estavel = df_estabilidade.iloc[0]
        modelo_menos_estavel = df_estabilidade.iloc[-1]

        print(f"MAIS EST√ÅVEL: {modelo_mais_estavel['modelo']}")
        print(f"  AUC: {modelo_mais_estavel['auc_media']:.3f} ¬± {modelo_mais_estavel['auc_std']:.3f}")
        print(f"  Coef. Varia√ß√£o: {modelo_mais_estavel['auc_cv']*100:.1f}%")
        print(f"  Score Estabilidade: {modelo_mais_estavel['score_estabilidade']:.3f}")

        print(f"\nMENOS EST√ÅVEL: {modelo_menos_estavel['modelo']}")
        print(f"  AUC: {modelo_menos_estavel['auc_media']:.3f} ¬± {modelo_menos_estavel['auc_std']:.3f}")
        print(f"  Coef. Varia√ß√£o: {modelo_menos_estavel['auc_cv']*100:.1f}%")
        print(f"  Score Estabilidade: {modelo_menos_estavel['score_estabilidade']:.3f}")

        print(f"\nRECOMENDA√á√ÉO:")
        cv_limite = 0.05  # 5% de coeficiente de varia√ß√£o como limite
        modelos_estaveis = df_estabilidade[df_estabilidade['auc_cv'] < cv_limite]

        if len(modelos_estaveis) > 0:
            print(f"Modelos com CV < {cv_limite*100:.0f}%: {len(modelos_estaveis)}")
            print(f"Recomendado para produ√ß√£o: {modelos_estaveis.iloc[0]['modelo']}")
        else:
            print("Nenhum modelo atende crit√©rio de estabilidade (CV < 5%)")
            print("Considere retreino mais frequente ou revis√£o das features")

    else:
        print("Nenhum resultado de estabilidade gerado - dados insuficientes")

    return resultados_estabilidade

# Executar teste de estabilidade
resultados_estabilidade_ranking = teste_estabilidade_ranking()

"""## 23.2- Investiga√ß√£o
Calibra√ß√£o de probabilidades
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, brier_score_loss
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
import xgboost as xgb
import matplotlib.pyplot as plt

def calibracao_probabilidades():
    """Testa calibra√ß√£o de probabilidades dos melhores modelos"""

    print("CALIBRA√á√ÉO DE PROBABILIDADES - MODELOS DE LEAD SCORING")
    print("=" * 60)

    def preparar_datasets_calibracao():
        """Prepara os 3 melhores datasets para calibra√ß√£o"""

        configuracoes = {
            'V1_DEVCLUB_UTM': {
                'dataset': dataset_v1_devclub_encoded,
                'data_original': dataset_v1_devclub['Data'],
                'nome': 'V1_DEVCLUB_UTM'
            },
            'V2_DEVCLUB_UTM': {
                'dataset': dataset_v2_devclub_encoded,
                'data_original': dataset_v2_devclub['Data'],
                'nome': 'V2_DEVCLUB_UTM'
            },
            'V1_DEVCLUB_CUTOFF': {
                'dataset': None,  # Ser√° criado
                'data_original': dataset_v1_devclub['Data'],
                'nome': 'V1_DEVCLUB_CUTOFF'
            }
        }

        # Criar dataset cutoff
        data_cutoff = pd.to_datetime('2025-08-10')
        data_dt = pd.to_datetime(dataset_v1_devclub['Data'], errors='coerce')
        mask_cutoff = data_dt <= data_cutoff
        configuracoes['V1_DEVCLUB_CUTOFF']['dataset'] = dataset_v1_devclub_encoded[mask_cutoff].copy()
        configuracoes['V1_DEVCLUB_CUTOFF']['data_original'] = dataset_v1_devclub['Data'][mask_cutoff]

        return configuracoes

    def split_temporal_calibracao(df_encoded, data_original):
        """Split temporal 70/30 para calibra√ß√£o"""

        data_dt = pd.to_datetime(data_original, errors='coerce')
        data_min = data_dt.min()
        data_max = data_dt.max()

        dias_totais = (data_max - data_min).days
        dias_treino = int(dias_totais * 0.7)
        data_corte = data_min + pd.Timedelta(days=dias_treino)

        mask_treino = data_dt <= data_corte
        mask_teste = data_dt > data_corte

        X = df_encoded.drop(columns=['target'])
        y = df_encoded['target']

        # Limpar nomes das colunas
        X.columns = X.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
        X.columns = X.columns.str.replace('__+', '_', regex=True)
        X.columns = X.columns.str.strip('_')

        return X[mask_treino], X[mask_teste], y[mask_treino], y[mask_teste]

    def treinar_modelo_base(X_train, y_train, modelo_tipo):
        """Treina modelo base sem calibra√ß√£o"""

        if modelo_tipo == 'RF':
            modelo = RandomForestClassifier(
                n_estimators=100, max_depth=10, random_state=42,
                class_weight='balanced', n_jobs=-1
            )
            modelo.fit(X_train, y_train)
            return modelo

        elif modelo_tipo == 'LGBM':
            params = {
                'objective': 'binary', 'metric': 'binary_logloss',
                'boosting_type': 'gbdt', 'num_leaves': 31,
                'learning_rate': 0.05, 'feature_fraction': 0.8,
                'bagging_fraction': 0.8, 'bagging_freq': 5,
                'min_child_samples': 100, 'verbose': -1,
                'random_state': 42, 'is_unbalance': True
            }
            train_data = lgb.Dataset(X_train, label=y_train)
            modelo = lgb.train(params, train_data, num_boost_round=500, callbacks=[lgb.log_evaluation(0)])
            return modelo

        elif modelo_tipo == 'XGB':
            scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
            modelo = xgb.XGBClassifier(
                n_estimators=500, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8, random_state=42,
                scale_pos_weight=scale_pos_weight, eval_metric='logloss'
            )
            modelo.fit(X_train, y_train)
            return modelo

    def obter_probabilidades(modelo, X_test, modelo_tipo):
        """Obt√©m probabilidades do modelo"""

        if modelo_tipo == 'LGBM':
            return modelo.predict(X_test, num_iteration=modelo.best_iteration)
        else:
            return modelo.predict_proba(X_test)[:, 1]

    def calibrar_probabilidades(modelo, X_train, y_train, X_test, modelo_tipo, metodo='isotonic'):
        """Calibra probabilidades usando Isotonic Regression ou Platt Scaling"""

        # Obter probabilidades de treino para calibra√ß√£o
        if modelo_tipo == 'LGBM':
            prob_train = modelo.predict(X_train, num_iteration=modelo.best_iteration)
            prob_test = modelo.predict(X_test, num_iteration=modelo.best_iteration)
        else:
            prob_train = modelo.predict_proba(X_train)[:, 1]
            prob_test = modelo.predict_proba(X_test)[:, 1]

        if metodo == 'isotonic':
            # Isotonic Regression
            calibrador = IsotonicRegression(out_of_bounds='clip')
            calibrador.fit(prob_train, y_train)
            prob_calibrada = calibrador.transform(prob_test)

        elif metodo == 'platt':
            # Platt Scaling (Logistic Regression)
            calibrador = LogisticRegression()
            calibrador.fit(prob_train.reshape(-1, 1), y_train)
            prob_calibrada = calibrador.predict_proba(prob_test.reshape(-1, 1))[:, 1]

        return prob_calibrada, prob_test

    def calcular_metricas_calibracao(y_true, prob_original, prob_calibrada):
        """Calcula m√©tricas de qualidade da calibra√ß√£o"""

        # Brier Score (menor √© melhor)
        brier_original = brier_score_loss(y_true, prob_original)
        brier_calibrada = brier_score_loss(y_true, prob_calibrada)

        # AUC (deve se manter)
        auc_original = roc_auc_score(y_true, prob_original)
        auc_calibrada = roc_auc_score(y_true, prob_calibrada)

        # Reliability (calibration curve)
        frac_pos_orig, mean_pred_orig = calibration_curve(
            y_true, prob_original, n_bins=10
        )
        frac_pos_cal, mean_pred_cal = calibration_curve(
            y_true, prob_calibrada, n_bins=10
        )

        # Expected Calibration Error (ECE)
        def calcular_ece(y_true, probs, n_bins=10):
            bin_boundaries = np.linspace(0, 1, n_bins + 1)
            bin_lowers = bin_boundaries[:-1]
            bin_uppers = bin_boundaries[1:]

            ece = 0.0
            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
                in_bin = (probs > bin_lower) & (probs <= bin_upper)
                prop_in_bin = in_bin.mean()

                if prop_in_bin > 0:
                    accuracy_in_bin = y_true[in_bin].mean()
                    avg_confidence_in_bin = probs[in_bin].mean()
                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin

            return ece

        ece_original = calcular_ece(y_true, prob_original)
        ece_calibrada = calcular_ece(y_true, prob_calibrada)

        return {
            'brier_original': brier_original,
            'brier_calibrada': brier_calibrada,
            'brier_melhoria': brier_original - brier_calibrada,
            'auc_original': auc_original,
            'auc_calibrada': auc_calibrada,
            'auc_diferenca': auc_calibrada - auc_original,
            'ece_original': ece_original,
            'ece_calibrada': ece_calibrada,
            'ece_melhoria': ece_original - ece_calibrada,
            'calibration_data_orig': (frac_pos_orig, mean_pred_orig),
            'calibration_data_cal': (frac_pos_cal, mean_pred_cal)
        }

    def analisar_bins_probabilidade(y_true, prob_original, prob_calibrada):
        """Analisa distribui√ß√£o por bins de probabilidade"""

        def criar_analise_bins(y_true, probs, nome):
            df_bins = pd.DataFrame({
                'y_true': y_true,
                'prob': probs
            })

            # Criar bins de probabilidade
            df_bins['bin'] = pd.cut(
                df_bins['prob'],
                bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%',
                       '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'],
                include_lowest=True
            )

            analise = df_bins.groupby('bin', observed=True).agg({
                'y_true': ['count', 'sum', 'mean'],
                'prob': 'mean'
            }).round(4)

            analise.columns = ['n_leads', 'n_conversoes', 'taxa_real', 'prob_media']
            analise['diferenca'] = np.abs(analise['prob_media'] - analise['taxa_real'])

            return analise

        analise_original = criar_analise_bins(y_true, prob_original, "Original")
        analise_calibrada = criar_analise_bins(y_true, prob_calibrada, "Calibrada")

        return analise_original, analise_calibrada

    # Executar calibra√ß√£o
    configuracoes = preparar_datasets_calibracao()
    resultados_calibracao = []

    # Testar apenas os 3 melhores: V1_DEVCLUB_UTM, V2_DEVCLUB_UTM, V1_DEVCLUB_CUTOFF
    modelos_testar = ['RF', 'LGBM']  # Focar nos 2 melhores algoritmos

    for config_nome, config_data in configuracoes.items():
        print(f"\n{'='*60}")
        print(f"CALIBRA√á√ÉO - {config_nome}")
        print("=" * 60)

        # Split temporal
        X_train, X_test, y_train, y_test = split_temporal_calibracao(
            config_data['dataset'],
            config_data['data_original']
        )

        print(f"Treino: {len(X_train):,} | Teste: {len(X_test):,}")
        print(f"Taxa real teste: {y_test.mean()*100:.2f}%")

        for modelo_tipo in modelos_testar:
            print(f"\n--- {modelo_tipo} ---")

            # Treinar modelo base
            modelo = treinar_modelo_base(X_train, y_train, modelo_tipo)

            # Calibra√ß√£o Isotonic
            prob_isotonic, prob_original = calibrar_probabilidades(
                modelo, X_train, y_train, X_test, modelo_tipo, 'isotonic'
            )

            # Calibra√ß√£o Platt
            prob_platt, _ = calibrar_probabilidades(
                modelo, X_train, y_train, X_test, modelo_tipo, 'platt'
            )

            # M√©tricas de calibra√ß√£o
            metricas_isotonic = calcular_metricas_calibracao(
                y_test, prob_original, prob_isotonic
            )
            metricas_platt = calcular_metricas_calibracao(
                y_test, prob_original, prob_platt
            )

            print(f"Probabilidade m√©dia predita (Original): {prob_original.mean()*100:.2f}%")
            print(f"Probabilidade m√©dia predita (Isotonic): {prob_isotonic.mean()*100:.2f}%")
            print(f"Probabilidade m√©dia predita (Platt): {prob_platt.mean()*100:.2f}%")
            print(f"Taxa real: {y_test.mean()*100:.2f}%")

            print(f"\nBrier Score:")
            print(f"  Original: {metricas_isotonic['brier_original']:.4f}")
            print(f"  Isotonic: {metricas_isotonic['brier_calibrada']:.4f} ({metricas_isotonic['brier_melhoria']:+.4f})")
            print(f"  Platt: {metricas_platt['brier_calibrada']:.4f} ({metricas_platt['brier_melhoria']:+.4f})")

            print(f"\nExpected Calibration Error:")
            print(f"  Original: {metricas_isotonic['ece_original']:.4f}")
            print(f"  Isotonic: {metricas_isotonic['ece_calibrada']:.4f} ({metricas_isotonic['ece_melhoria']:+.4f})")
            print(f"  Platt: {metricas_platt['ece_calibrada']:.4f} ({metricas_platt['ece_melhoria']:+.4f})")

            # Salvar resultados
            resultados_calibracao.extend([
                {
                    'Dataset': config_nome,
                    'Modelo': modelo_tipo,
                    'Tipo': 'Original',
                    'AUC': metricas_isotonic['auc_original'],
                    'Brier_Score': metricas_isotonic['brier_original'],
                    'ECE': metricas_isotonic['ece_original'],
                    'Prob_Media': prob_original.mean(),
                    'Taxa_Real': y_test.mean()
                },
                {
                    'Dataset': config_nome,
                    'Modelo': modelo_tipo,
                    'Tipo': 'Isotonic',
                    'AUC': metricas_isotonic['auc_calibrada'],
                    'Brier_Score': metricas_isotonic['brier_calibrada'],
                    'ECE': metricas_isotonic['ece_calibrada'],
                    'Prob_Media': prob_isotonic.mean(),
                    'Taxa_Real': y_test.mean()
                },
                {
                    'Dataset': config_nome,
                    'Modelo': modelo_tipo,
                    'Tipo': 'Platt',
                    'AUC': metricas_platt['auc_calibrada'],
                    'Brier_Score': metricas_platt['brier_calibrada'],
                    'ECE': metricas_platt['ece_calibrada'],
                    'Prob_Media': prob_platt.mean(),
                    'Taxa_Real': y_test.mean()
                }
            ])

    # Tabela comparativa final
    print(f"\n{'='*80}")
    print("TABELA COMPARATIVA - CALIBRA√á√ÉO DE PROBABILIDADES")
    print("=" * 80)

    df_resultados = pd.DataFrame(resultados_calibracao)

    # Formatar para exibi√ß√£o
    df_display = df_resultados.copy()
    df_display['AUC'] = df_display['AUC'].apply(lambda x: f"{x:.3f}")
    df_display['Brier_Score'] = df_display['Brier_Score'].apply(lambda x: f"{x:.4f}")
    df_display['ECE'] = df_display['ECE'].apply(lambda x: f"{x:.4f}")
    df_display['Prob_Media'] = df_display['Prob_Media'].apply(lambda x: f"{x*100:.2f}%")
    df_display['Taxa_Real'] = df_display['Taxa_Real'].apply(lambda x: f"{x*100:.2f}%")

    print(df_display.to_string(index=False))

    # An√°lise de melhoria
    print(f"\n{'='*80}")
    print("AN√ÅLISE DE MELHORIA DA CALIBRA√á√ÉO")
    print("=" * 80)

    # Agrupar por dataset e modelo para comparar m√©todos
    for dataset in df_resultados['Dataset'].unique():
        for modelo in df_resultados['Modelo'].unique():
            subset = df_resultados[
                (df_resultados['Dataset'] == dataset) &
                (df_resultados['Modelo'] == modelo)
            ]

            if len(subset) == 3:
                original = subset[subset['Tipo'] == 'Original'].iloc[0]
                isotonic = subset[subset['Tipo'] == 'Isotonic'].iloc[0]
                platt = subset[subset['Tipo'] == 'Platt'].iloc[0]

                print(f"\n{dataset} - {modelo}:")
                print(f"  Taxa real: {original['Taxa_Real']*100:.2f}%")
                print(f"  Prob m√©dia original: {original['Prob_Media']*100:.2f}%")
                print(f"  Prob m√©dia isotonic: {isotonic['Prob_Media']*100:.2f}%")
                print(f"  Prob m√©dia platt: {platt['Prob_Media']*100:.2f}%")

                # Melhor m√©todo por ECE
                melhor_ece = min(isotonic['ECE'], platt['ECE'])
                melhor_metodo = 'Isotonic' if isotonic['ECE'] == melhor_ece else 'Platt'
                melhoria_ece = original['ECE'] - melhor_ece

                print(f"  Melhor calibra√ß√£o: {melhor_metodo}")
                print(f"  Melhoria ECE: {melhoria_ece:+.4f}")

                # Status da calibra√ß√£o
                if abs(original['Prob_Media'] - original['Taxa_Real']) > 0.01:  # Mais de 1% de diferen√ßa
                    print(f"  Status: NECESS√ÅRIA (descalibrado)")
                else:
                    print(f"  Status: J√° bem calibrado")

    # Recomenda√ß√£o final
    print(f"\n{'='*80}")
    print("RECOMENDA√á√ÉO FINAL")
    print("=" * 80)

    # Encontrar modelo com melhor calibra√ß√£o
    df_calibrados = df_resultados[df_resultados['Tipo'].isin(['Isotonic', 'Platt'])]
    melhor_calibrado = df_calibrados.loc[df_calibrados['ECE'].idxmin()]

    print(f"MODELO RECOMENDADO PARA PRODU√á√ÉO:")
    print(f"Dataset: {melhor_calibrado['Dataset']}")
    print(f"Algoritmo: {melhor_calibrado['Modelo']}")
    print(f"Calibra√ß√£o: {melhor_calibrado['Tipo']}")
    print(f"AUC: {melhor_calibrado['AUC']:.3f}")
    print(f"ECE: {melhor_calibrado['ECE']:.4f}")
    print(f"Probabilidade m√©dia: {melhor_calibrado['Prob_Media']*100:.2f}%")
    print(f"Taxa real: {melhor_calibrado['Taxa_Real']*100:.2f}%")

    return df_resultados

# Executar calibra√ß√£o de probabilidades
resultados_calibracao_final = calibracao_probabilidades()

"""## 23.3- Investiga√ß√£o
Hyperparameter Tuning
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import ParameterGrid
import time

def hyperparameter_tuning_rapido():
    """Hyperparameter tuning focado e r√°pido para V1_DEVCLUB_UTM RF"""

    print("HYPERPARAMETER TUNING R√ÅPIDO - V1_DEVCLUB_UTM RF")
    print("=" * 55)

    def split_temporal_tuning():
        """Split temporal 70/30 para tuning"""

        data_dt = pd.to_datetime(dataset_v1_devclub['Data'], errors='coerce')
        data_min = data_dt.min()
        data_max = data_dt.max()

        dias_totais = (data_max - data_min).days
        dias_treino = int(dias_totais * 0.7)
        data_corte = data_min + pd.Timedelta(days=dias_treino)

        mask_treino = data_dt <= data_corte
        mask_teste = data_dt > data_corte

        X = dataset_v1_devclub_encoded.drop(columns=['target'])
        y = dataset_v1_devclub_encoded['target']

        # Limpar nomes das colunas
        X.columns = X.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
        X.columns = X.columns.str.replace('__+', '_', regex=True)
        X.columns = X.columns.str.strip('_')

        X_train = X[mask_treino]
        X_test = X[mask_teste]
        y_train = y[mask_treino]
        y_test = y[mask_teste]

        print(f"Split temporal criado:")
        print(f"  Treino: {len(X_train):,} registros | Taxa: {y_train.mean()*100:.2f}%")
        print(f"  Teste: {len(X_test):,} registros | Taxa: {y_test.mean()*100:.2f}%")

        return X_train, X_test, y_train, y_test

    def calcular_metricas_ranking(y_test, y_prob):
        """Calcula m√©tricas de ranking focadas no caso de uso"""

        df_analise = pd.DataFrame({
            'probabilidade': y_prob,
            'target_real': y_test.reset_index(drop=True)
        })

        # Decis
        df_analise['decil'] = pd.qcut(
            df_analise['probabilidade'],
            q=10,
            labels=[f'D{i}' for i in range(1, 11)],
            duplicates='drop'
        )

        analise_decis = df_analise.groupby('decil', observed=True).agg({
            'target_real': ['count', 'sum', 'mean']
        }).round(4)

        analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']

        if analise_decis['conversoes'].sum() == 0:
            return None

        analise_decis['pct_total_conversoes'] = (
            analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
        ).round(2)

        taxa_base = y_test.mean()
        analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)

        # M√©tricas
        top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
        top5_conversoes = analise_decis.tail(5)['pct_total_conversoes'].sum()
        lift_maximo = analise_decis['lift'].max()

        # Monotonia
        taxas = analise_decis['taxa_conversao'].values
        crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
        monotonia = crescimentos / (len(taxas) - 1) if len(taxas) > 1 else 1.0

        # AUC
        auc = roc_auc_score(y_test, y_prob)

        return {
            'auc': auc,
            'top3_conv': top3_conversoes,
            'top5_conv': top5_conversoes,
            'lift_max': lift_maximo,
            'monotonia': monotonia * 100,
            'n_conversoes': analise_decis['conversoes'].sum(),
            'analise_decis': analise_decis
        }

    # Grid de hiperpar√¢metros focado e eficiente
    print(f"\nDefinindo grid de hiperpar√¢metros...")

    # Baseline atual
    baseline_params = {
        'n_estimators': 100,
        'max_depth': 10,
        'min_samples_split': 2,
        'min_samples_leaf': 1,
        'max_features': 'sqrt',
        'class_weight': 'balanced',
        'random_state': 42,
        'n_jobs': -1
    }

    # Grid otimizado (18 combina√ß√µes total)
    param_grid = {
        'n_estimators': [100, 200, 300],  # Mais √°rvores pode melhorar
        'max_depth': [8, 10, 12, None],   # Profundidade variada
        'min_samples_split': [2, 5],      # Controle de overfitting
        'min_samples_leaf': [1, 3],       # Controle de overfitting
        'max_features': ['sqrt', 'log2']  # Features por split
    }

    # Fixar alguns par√¢metros para reduzir combina√ß√µes
    fixed_params = {
        'class_weight': 'balanced',
        'random_state': 42,
        'n_jobs': -1
    }

    print(f"Grid de hiperpar√¢metros:")
    for param, values in param_grid.items():
        print(f"  {param}: {values}")

    total_combinations = 1
    for values in param_grid.values():
        total_combinations *= len(values)
    print(f"\nTotal de combina√ß√µes: {total_combinations}")

    # Split dados uma vez
    X_train, X_test, y_train, y_test = split_temporal_tuning()

    # Treinar baseline
    print(f"\n{'='*60}")
    print("TREINANDO BASELINE")
    print("=" * 60)

    start_time = time.time()

    baseline_rf = RandomForestClassifier(**baseline_params)
    baseline_rf.fit(X_train, y_train)
    baseline_prob = baseline_rf.predict_proba(X_test)[:, 1]
    baseline_metricas = calcular_metricas_ranking(y_test, baseline_prob)

    baseline_time = time.time() - start_time

    print(f"Baseline treinado em {baseline_time:.1f}s")
    print(f"AUC: {baseline_metricas['auc']:.3f}")
    print(f"Top 3 decis: {baseline_metricas['top3_conv']:.1f}%")
    print(f"Top 5 decis: {baseline_metricas['top5_conv']:.1f}%")
    print(f"Lift m√°ximo: {baseline_metricas['lift_max']:.1f}x")
    print(f"Monotonia: {baseline_metricas['monotonia']:.1f}%")

    # Grid search manual
    print(f"\n{'='*60}")
    print("EXECUTANDO GRID SEARCH")
    print("=" * 60)

    resultados = []
    melhor_auc = baseline_metricas['auc']
    melhores_params = baseline_params.copy()

    start_grid = time.time()

    for i, params in enumerate(ParameterGrid(param_grid), 1):
        # Combinar com par√¢metros fixos
        full_params = {**params, **fixed_params}

        try:
            # Treinar modelo
            start_model = time.time()
            rf = RandomForestClassifier(**full_params)
            rf.fit(X_train, y_train)
            y_prob = rf.predict_proba(X_test)[:, 1]
            model_time = time.time() - start_model

            # Calcular m√©tricas
            metricas = calcular_metricas_ranking(y_test, y_prob)

            if metricas:
                # Salvar resultado
                resultado = {
                    'combinacao': i,
                    'params': full_params.copy(),
                    'tempo': model_time,
                    **metricas
                }
                resultados.append(resultado)

                # Verificar se √© o melhor
                if metricas['auc'] > melhor_auc:
                    melhor_auc = metricas['auc']
                    melhores_params = full_params.copy()

                # Progress
                if i % 10 == 0 or i == total_combinations:
                    print(f"[{i:2d}/{total_combinations}] AUC: {metricas['auc']:.3f} | "
                          f"Top3: {metricas['top3_conv']:.1f}% | "
                          f"Tempo: {model_time:.1f}s")

        except Exception as e:
            print(f"Erro na combina√ß√£o {i}: {str(e)[:30]}...")
            continue

    grid_time = time.time() - start_grid

    print(f"\nGrid search conclu√≠do em {grid_time:.1f}s")
    print(f"Modelos v√°lidos treinados: {len(resultados)}")

    # An√°lise dos resultados
    if len(resultados) > 0:
        print(f"\n{'='*60}")
        print("TOP 10 MELHORES CONFIGURA√á√ïES")
        print("=" * 60)

        # Ordenar por AUC
        resultados_sorted = sorted(resultados, key=lambda x: x['auc'], reverse=True)

        print(f"{'Rank':<4} {'AUC':<6} {'Top3':<6} {'Top5':<6} {'Lift':<6} {'Mono':<6} {'Params'}")
        print("-" * 80)

        for i, resultado in enumerate(resultados_sorted[:10], 1):
            params_str = f"n_est:{resultado['params']['n_estimators']}, " \
                        f"depth:{resultado['params']['max_depth']}, " \
                        f"split:{resultado['params']['min_samples_split']}, " \
                        f"leaf:{resultado['params']['min_samples_leaf']}, " \
                        f"feat:{resultado['params']['max_features']}"

            print(f"{i:<4} {resultado['auc']:.3f}  {resultado['top3_conv']:5.1f}% "
                  f"{resultado['top5_conv']:5.1f}% {resultado['lift_max']:5.1f}x "
                  f"{resultado['monotonia']:5.1f}% {params_str}")

        # Compara√ß√£o com baseline
        melhor_resultado = resultados_sorted[0]

        print(f"\n{'='*60}")
        print("COMPARA√á√ÉO: BASELINE vs MELHOR TUNADO")
        print("=" * 60)

        print(f"BASELINE:")
        print(f"  AUC: {baseline_metricas['auc']:.3f}")
        print(f"  Top 3 decis: {baseline_metricas['top3_conv']:.1f}%")
        print(f"  Top 5 decis: {baseline_metricas['top5_conv']:.1f}%")
        print(f"  Lift m√°ximo: {baseline_metricas['lift_max']:.1f}x")
        print(f"  Monotonia: {baseline_metricas['monotonia']:.1f}%")

        print(f"\nMELHOR TUNADO:")
        print(f"  AUC: {melhor_resultado['auc']:.3f}")
        print(f"  Top 3 decis: {melhor_resultado['top3_conv']:.1f}%")
        print(f"  Top 5 decis: {melhor_resultado['top5_conv']:.1f}%")
        print(f"  Lift m√°ximo: {melhor_resultado['lift_max']:.1f}x")
        print(f"  Monotonia: {melhor_resultado['monotonia']:.1f}%")

        # Melhorias
        melhoria_auc = ((melhor_resultado['auc'] - baseline_metricas['auc']) / baseline_metricas['auc']) * 100
        melhoria_top3 = melhor_resultado['top3_conv'] - baseline_metricas['top3_conv']
        melhoria_top5 = melhor_resultado['top5_conv'] - baseline_metricas['top5_conv']

        print(f"\nMELHORIAS:")
        print(f"  AUC: {melhoria_auc:+.2f}%")
        print(f"  Top 3 decis: {melhoria_top3:+.1f} pontos percentuais")
        print(f"  Top 5 decis: {melhoria_top5:+.1f} pontos percentuais")

        # Par√¢metros do melhor modelo
        print(f"\nMELHORES HIPERPAR√ÇMETROS:")
        for param, value in melhor_resultado['params'].items():
            if param not in ['random_state', 'n_jobs']:
                print(f"  {param}: {value}")

        # Recomenda√ß√£o
        print(f"\n{'='*60}")
        print("RECOMENDA√á√ÉO")
        print("=" * 60)

        if melhoria_auc > 1.0:  # Melhoria > 1%
            print(f"‚úÖ RECOMENDADO: Usar hiperpar√¢metros tunados")
            print(f"   Melhoria significativa no AUC (+{melhoria_auc:.2f}%)")
            print(f"   Melhor concentra√ß√£o nos top decis")
        elif melhoria_auc > 0.5:  # Melhoria > 0.5%
            print(f"‚öñÔ∏è  CONSIDERAR: Melhoria marginal (+{melhoria_auc:.2f}%)")
            print(f"   Avaliar trade-off entre complexidade e ganho")
        else:
            print(f"‚ùå N√ÉO RECOMENDADO: Melhoria insignificante (+{melhoria_auc:.2f}%)")
            print(f"   Manter baseline para simplicidade")

        print(f"\nTempo total de tuning: {grid_time:.1f}s")
        print(f"Tempo m√©dio por modelo: {grid_time/len(resultados):.1f}s")

        return {
            'baseline': baseline_metricas,
            'melhor_tunado': melhor_resultado,
            'todos_resultados': resultados_sorted,
            'melhores_params': melhor_resultado['params']
        }

    else:
        print("‚ùå Nenhum modelo v√°lido foi treinado durante o grid search")
        return None

# Executar hyperparameter tuning
resultados_tuning = hyperparameter_tuning_rapido()

"""##23.4 - Investiga√ß√£o se as UTMs realmente est√£o no dataset final"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier

def verificar_features_modelo_final():
    """Verifica exatamente quais features est√£o no modelo final recomendado"""

    print("VERIFICA√á√ÉO DAS FEATURES DO MODELO FINAL")
    print("=" * 50)

    # Dataset que seria usado em produ√ß√£o
    dataset_final = dataset_v1_devclub_encoded.copy()

    print(f"Dataset: dataset_v1_devclub_encoded")
    print(f"Total de registros: {len(dataset_final):,}")
    print(f"Total de colunas: {len(dataset_final.columns)}")

    # Listar TODAS as colunas
    print(f"\nTODAS AS COLUNAS NO DATASET FINAL:")
    print("-" * 40)

    colunas_utm = []
    colunas_pesquisa = []
    colunas_derivadas = []
    colunas_outras = []

    for i, col in enumerate(dataset_final.columns, 1):
        print(f"{i:2d}. {col}")

        # Categorizar colunas
        if any(utm in col for utm in ['Source_', 'Medium_', 'Term_']):
            colunas_utm.append(col)
        elif any(pesq in col for pesq in ['g√™nero', 'idade', 'faz', 'faixa', 'cart√£o', 'estudou', 'faculdade', 'evento']):
            colunas_pesquisa.append(col)
        elif any(deriv in col for deriv in ['nome_', 'email_', 'telefone_', 'dia_semana']):
            colunas_derivadas.append(col)
        else:
            colunas_outras.append(col)

    # An√°lise por categoria
    print(f"\nAN√ÅLISE POR CATEGORIA DE FEATURES:")
    print("=" * 50)

    print(f"\nüìç FEATURES UTM ({len(colunas_utm)} colunas):")
    if colunas_utm:
        for col in colunas_utm:
            print(f"  ‚Ä¢ {col}")
    else:
        print("  ‚ùå NENHUMA FEATURE UTM ENCONTRADA")

    print(f"\nüìã FEATURES DE PESQUISA ({len(colunas_pesquisa)} colunas):")
    for col in colunas_pesquisa:
        print(f"  ‚Ä¢ {col}")

    print(f"\nüîß FEATURES DERIVADAS ({len(colunas_derivadas)} colunas):")
    for col in colunas_derivadas:
        print(f"  ‚Ä¢ {col}")

    print(f"\nüì¶ OUTRAS FEATURES ({len(colunas_outras)} colunas):")
    for col in colunas_outras:
        print(f"  ‚Ä¢ {col}")

    # Verificar features espec√≠ficas cr√≠ticas
    print(f"\nVERIFICA√á√ÉO DE FEATURES CR√çTICAS:")
    print("-" * 40)

    features_criticas = {
        'Source': any('Source_' in col for col in dataset_final.columns),
        'Medium': any('Medium_' in col for col in dataset_final.columns),
        'Term': any('Term_' in col for col in dataset_final.columns),
        'target': 'target' in dataset_final.columns,
        'dia_semana': 'dia_semana' in dataset_final.columns,
        'nome_valido': any('nome_valido' in col for col in dataset_final.columns),
        'email_valido': any('email_valido' in col for col in dataset_final.columns),
        'telefone_valido': any('telefone_valido' in col for col in dataset_final.columns)
    }

    for feature, presente in features_criticas.items():
        status = "‚úÖ PRESENTE" if presente else "‚ùå AUSENTE"
        print(f"  {feature}: {status}")

    # Preparar dados para modelo
    X = dataset_final.drop(columns=['target'])
    y = dataset_final['target']

    # Limpar nomes das colunas
    X_clean = X.copy()
    X_clean.columns = X_clean.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
    X_clean.columns = X_clean.columns.str.replace('__+', '_', regex=True)
    X_clean.columns = X_clean.columns.str.strip('_')

    print(f"\nFEATURES AP√ìS LIMPEZA DE NOMES:")
    print("-" * 40)
    print(f"Features para o modelo: {len(X_clean.columns)}")

    # Mostrar diferen√ßas de nomenclatura
    if len(X.columns) == len(X_clean.columns):
        diferencas = []
        for orig, limpo in zip(X.columns, X_clean.columns):
            if orig != limpo:
                diferencas.append((orig, limpo))

        if diferencas:
            print(f"\nMudan√ßas de nomenclatura ({len(diferencas)}):")
            for orig, limpo in diferencas[:10]:  # Mostrar apenas as primeiras 10
                print(f"  '{orig}' ‚Üí '{limpo}'")
            if len(diferencas) > 10:
                print(f"  ... e mais {len(diferencas)-10} mudan√ßas")

    # Treinar modelo final para confirmar
    print(f"\nTREINO DO MODELO FINAL PARA CONFIRMA√á√ÉO:")
    print("-" * 50)

    modelo_final = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    )

    # Split temporal
    data_dt = pd.to_datetime(dataset_v1_devclub['Data'], errors='coerce')
    data_min = data_dt.min()
    data_max = data_dt.max()

    dias_totais = (data_max - data_min).days
    dias_treino = int(dias_totais * 0.7)
    data_corte = data_min + pd.Timedelta(days=dias_treino)

    mask_treino = data_dt <= data_corte
    mask_teste = data_dt > data_corte

    X_train = X_clean[mask_treino]
    X_test = X_clean[mask_teste]
    y_train = y[mask_treino]
    y_test = y[mask_teste]

    print(f"Per√≠odo de treino: {data_min.strftime('%Y-%m-%d')} a {data_corte.strftime('%Y-%m-%d')}")
    print(f"Per√≠odo de teste: {data_corte.strftime('%Y-%m-%d')} a {data_max.strftime('%Y-%m-%d')}")
    print(f"Registros treino: {len(X_train):,}")
    print(f"Registros teste: {len(X_test):,}")
    print(f"Taxa convers√£o treino: {y_train.mean()*100:.2f}%")
    print(f"Taxa convers√£o teste: {y_test.mean()*100:.2f}%")

    # Treinar modelo
    modelo_final.fit(X_train, y_train)
    prob_final = modelo_final.predict_proba(X_test)[:, 1]

    from sklearn.metrics import roc_auc_score
    auc_final = roc_auc_score(y_test, prob_final)

    print(f"AUC do modelo final: {auc_final:.3f}")

    # Feature importance das UTMs especificamente
    feature_importance = pd.DataFrame({
        'feature': X_clean.columns,
        'importance': modelo_final.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\nTOP 10 FEATURES MAIS IMPORTANTES:")
    print("-" * 50)
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        print(f"{i:2d}. {row['feature']}: {row['importance']:.4f}")

    # Features UTM especificamente
    utm_features = feature_importance[
        feature_importance['feature'].str.contains('Source_|Medium_|Term_', case=False, na=False)
    ]

    print(f"\nIMPORT√ÇNCIA DAS FEATURES UTM:")
    print("-" * 50)
    if len(utm_features) > 0:
        print(f"Total de features UTM: {len(utm_features)}")
        print(f"Import√¢ncia total UTM: {utm_features['importance'].sum():.4f}")
        print(f"\nTop 5 UTM features:")
        for i, (_, row) in enumerate(utm_features.head(5).iterrows(), 1):
            print(f"  {i}. {row['feature']}: {row['importance']:.4f}")
    else:
        print("‚ùå NENHUMA FEATURE UTM ENCONTRADA NO MODELO")

    print(f"\nCONCLUS√ÉO DEFINITIVA:")
    print("=" * 50)

    tem_utms = len(utm_features) > 0
    tem_target = 'target' in dataset_final.columns

    print(f"‚úì Dataset: dataset_v1_devclub_encoded")
    print(f"‚úì Registros: {len(dataset_final):,}")
    print(f"‚úì Features total: {len(X_clean.columns)}")
    print(f"‚úì Features UTM: {len(utm_features)} {'(PRESENTE)' if tem_utms else '(AUSENTE)'}")
    print(f"‚úì Target: {'PRESENTE' if tem_target else 'AUSENTE'}")
    print(f"‚úì AUC: {auc_final:.3f}")

    if tem_utms:
        print(f"\nüéØ CONFIRMADO: O modelo CONT√âM features UTM")
        print(f"   Import√¢ncia total das UTMs: {utm_features['importance'].sum():.1%}")
    else:
        print(f"\n‚ö†Ô∏è  ATEN√á√ÉO: O modelo N√ÉO CONT√âM features UTM")

    return {
        'dataset_info': {
            'nome': 'dataset_v1_devclub_encoded',
            'registros': len(dataset_final),
            'colunas_total': len(dataset_final.columns),
            'features_modelo': len(X_clean.columns)
        },
        'features_por_tipo': {
            'utm': len(colunas_utm),
            'pesquisa': len(colunas_pesquisa),
            'derivadas': len(colunas_derivadas),
            'outras': len(colunas_outras)
        },
        'tem_utms': tem_utms,
        'auc_final': auc_final,
        'utm_importance': utm_features['importance'].sum() if tem_utms else 0
    }

# Executar verifica√ß√£o completa
resultado_verificacao = verificar_features_modelo_final()

"""##24- Salvamento do modelo e features esperadas (√öNICO MODELO)"""

import pandas as pd
import numpy as np
import json
import joblib
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import sklearn

def registrar_features_e_modelo():
    """Registra features e salva modelo para produ√ß√£o"""

    print("REGISTRO DE FEATURES E MODELO PARA PRODU√á√ÉO")
    print("=" * 55)

    # 1. PREPARAR DADOS E TREINAR MODELO FINAL
    print("\n1. PREPARANDO DADOS E TREINANDO MODELO FINAL")
    print("-" * 50)

    # Dataset final
    dataset_final = dataset_v1_devclub_encoded.copy()

    # Split temporal
    data_dt = pd.to_datetime(dataset_v1_devclub['Data'], errors='coerce')
    data_min = data_dt.min()
    data_max = data_dt.max()
    dias_totais = (data_max - data_min).days
    dias_treino = int(dias_totais * 0.7)
    data_corte = data_min + pd.Timedelta(days=dias_treino)

    mask_treino = data_dt <= data_corte
    mask_teste = data_dt > data_corte

    X = dataset_final.drop(columns=['target'])
    y = dataset_final['target']

    # Limpar nomes das colunas
    X_clean = X.copy()
    X_clean.columns = X_clean.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
    X_clean.columns = X_clean.columns.str.replace('__+', '_', regex=True)
    X_clean.columns = X_clean.columns.str.strip('_')

    X_train = X_clean[mask_treino]
    X_test = X_clean[mask_teste]
    y_train = y[mask_treino]
    y_test = y[mask_teste]

    print(f"Dataset: {len(dataset_final):,} registros")
    print(f"Features: {len(X_clean.columns)} colunas")
    print(f"Per√≠odo treino: {data_min.strftime('%Y-%m-%d')} a {data_corte.strftime('%Y-%m-%d')}")
    print(f"Per√≠odo teste: {data_corte.strftime('%Y-%m-%d')} a {data_max.strftime('%Y-%m-%d')}")
    print(f"Split: {len(X_train):,} treino / {len(X_test):,} teste")

    # Treinar modelo final
    modelo_final = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    modelo_final.fit(X_train, y_train)
    y_prob = modelo_final.predict_proba(X_test)[:, 1]
    auc_final = roc_auc_score(y_test, y_prob)

    print(f"Modelo treinado - AUC: {auc_final:.3f}")

    # 2. CRIAR REGISTRY DE FEATURES
    print("\n2. CRIANDO FEATURE REGISTRY")
    print("-" * 50)

    # Categorizar features
    features_utm = []
    features_pesquisa = []
    features_derivadas = []
    features_outras = []

    for col in X.columns:
        if any(utm in col for utm in ['Source_', 'Medium_', 'Term_']):
            features_utm.append(col)
        elif any(pesq in col for pesq in ['g√™nero', 'idade', 'faz', 'faixa', 'cart√£o', 'estudou', 'faculdade', 'evento']):
            features_pesquisa.append(col)
        elif any(deriv in col for deriv in ['nome_', 'email_', 'telefone_', 'dia_semana']):
            features_derivadas.append(col)
        else:
            features_outras.append(col)

    # Mapeamento nome original -> nome limpo
    mapeamento_nomes = {}
    for orig, limpo in zip(X.columns, X_clean.columns):
        mapeamento_nomes[orig] = limpo

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature_original': X.columns,
        'feature_clean': X_clean.columns,
        'importance': modelo_final.feature_importances_
    }).sort_values('importance', ascending=False)

    # Criar registry completo
    feature_registry = {
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "dataset_name": "dataset_v1_devclub_encoded",
            "total_features": len(X.columns),
            "total_records": len(dataset_final),
            "target_column": "target",
            "model_type": "RandomForestClassifier",
            "sklearn_version": sklearn.__version__
        },
        "data_split": {
            "method": "temporal",
            "train_start": data_min.strftime('%Y-%m-%d'),
            "train_end": data_corte.strftime('%Y-%m-%d'),
            "test_start": data_corte.strftime('%Y-%m-%d'),
            "test_end": data_max.strftime('%Y-%m-%d'),
            "train_records": len(X_train),
            "test_records": len(X_test),
            "train_positive_rate": float(y_train.mean()),
            "test_positive_rate": float(y_test.mean())
        },
        "feature_categories": {
            "utm_features": {
                "count": len(features_utm),
                "description": "Features derived from UTM parameters (Source, Medium, Term)",
                "features": features_utm
            },
            "survey_features": {
                "count": len(features_pesquisa),
                "description": "Features from lead survey responses",
                "features": features_pesquisa
            },
            "derived_features": {
                "count": len(features_derivadas),
                "description": "Features engineered from raw data (name, email, phone, temporal)",
                "features": features_derivadas
            },
            "other_features": {
                "count": len(features_outras),
                "description": "Additional features not in main categories",
                "features": features_outras
            }
        },
        "feature_transformations": {
            "description": "Mapping from original feature names to model-ready names",
            "name_mapping": mapeamento_nomes,
            "encoding_applied": {
                "categorical_encoding": "one-hot",
                "ordinal_features": ["Qual a sua idade?", "Atualmente, qual a sua faixa salarial?"],
                "binary_features": ["dia_semana"]
            },
            "column_cleaning": {
                "regex_pattern": "[^A-Za-z0-9_] -> _",
                "multiple_underscores": "__ -> _",
                "strip_underscores": "leading/trailing removed"
            }
        },
        "feature_importance": {
            "description": "Feature importance from trained RandomForest",
            "top_10_features": [
                {
                    "rank": i+1,
                    "feature_original": row['feature_original'],
                    "feature_clean": row['feature_clean'],
                    "importance": float(row['importance'])
                }
                for i, (_, row) in enumerate(feature_importance.head(10).iterrows())
            ],
            "utm_total_importance": float(
                feature_importance[
                    feature_importance['feature_original'].str.contains('Source_|Medium_|Term_', case=False, na=False)
                ]['importance'].sum()
            )
        },
        "expected_dtypes": {
            feature: str(dataset_final[feature].dtype) if feature in dataset_final.columns else "float64"
            for feature in X.columns
        },
        "validation_rules": {
            "required_features": list(X.columns),
            "optional_features": [],
            "total_expected_features": len(X.columns),
            "target_required": True,
            "missing_value_strategy": "model_will_fail_if_missing_features"
        }
    }

    print(f"Feature registry criado:")
    print(f"  - Features UTM: {len(features_utm)}")
    print(f"  - Features Pesquisa: {len(features_pesquisa)}")
    print(f"  - Features Derivadas: {len(features_derivadas)}")
    print(f"  - Features Outras: {len(features_outras)}")
    print(f"  - Mapeamentos de nomes: {len(mapeamento_nomes)}")
    print(f"  - Feature importance calculada para {len(feature_importance)} features")

    # 3. CRIAR METADADOS DO MODELO
    print("\n3. CRIANDO METADADOS DO MODELO")
    print("-" * 50)

    # Calcular m√©tricas detalhadas
    df_analise = pd.DataFrame({
        'probabilidade': y_prob,
        'target_real': y_test.reset_index(drop=True)
    })

    df_analise['decil'] = pd.qcut(
        df_analise['probabilidade'],
        q=10,
        labels=[f'D{i}' for i in range(1, 11)],
        duplicates='drop'
    )

    analise_decis = df_analise.groupby('decil', observed=True).agg({
        'target_real': ['count', 'sum', 'mean']
    }).round(4)

    analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']
    analise_decis['pct_total_conversoes'] = (
        analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
    ).round(2)

    taxa_base = y_test.mean()
    analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)

    top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
    top5_conversoes = analise_decis.tail(5)['pct_total_conversoes'].sum()
    lift_maximo = analise_decis['lift'].max()

    # Monotonia
    taxas = analise_decis['taxa_conversao'].values
    crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
    monotonia = (crescimentos / (len(taxas) - 1)) * 100 if len(taxas) > 1 else 100.0

    # Metadados do modelo
    model_metadata = {
        "model_info": {
            "model_type": "RandomForestClassifier",
            "library": "scikit-learn",
            "library_version": sklearn.__version__,
            "trained_at": datetime.now().isoformat(),
            "training_duration_info": "Trained on full pipeline with temporal split"
        },
        "hyperparameters": {
            "n_estimators": 100,
            "max_depth": 10,
            "min_samples_split": 2,
            "min_samples_leaf": 1,
            "max_features": "sqrt",
            "class_weight": "balanced",
            "random_state": 42,
            "n_jobs": -1
        },
        "training_data": {
            "dataset_name": "V1_DEVCLUB_UTM",
            "total_records": len(dataset_final),
            "training_records": len(X_train),
            "test_records": len(X_test),
            "features_count": len(X_clean.columns),
            "target_distribution": {
                "training_positive_rate": float(y_train.mean()),
                "test_positive_rate": float(y_test.mean()),
                "training_positive_count": int(y_train.sum()),
                "test_positive_count": int(y_test.sum())
            }
        },
        "performance_metrics": {
            "auc": float(auc_final),
            "top3_decil_concentration": float(top3_conversoes),
            "top5_decil_concentration": float(top5_conversoes),
            "lift_maximum": float(lift_maximo),
            "monotonia_percentage": float(monotonia),
            "baseline_conversion_rate": float(taxa_base)
        },
        "decil_analysis": {
            f"decil_{i+1}": {
                "total_leads": int(row['total_leads']),
                "conversions": int(row['conversoes']),
                "conversion_rate": float(row['taxa_conversao']),
                "pct_total_conversions": float(row['pct_total_conversoes']),
                "lift": float(row['lift'])
            }
            for i, (_, row) in enumerate(analise_decis.iterrows())
        },
        "production_notes": {
            "use_case": "Lead scoring for budget allocation",
            "prediction_interpretation": "Higher probability = higher priority for budget allocation",
            "calibration_status": "Not calibrated - use for ranking only",
            "recommended_deployment": "Batch scoring with temporal validation",
            "monitoring_requirements": "Track AUC degradation and decil stability over time"
        }
    }

    print(f"Metadados do modelo criados:")
    print(f"  - AUC: {auc_final:.3f}")
    print(f"  - Top 3 decis: {top3_conversoes:.1f}%")
    print(f"  - Top 5 decis: {top5_conversoes:.1f}%")
    print(f"  - Lift m√°ximo: {lift_maximo:.1f}x")
    print(f"  - Monotonia: {monotonia:.1f}%")
    print(f"  - An√°lise de 10 decis inclu√≠da")

    # 4. SALVAR ARQUIVOS
    print("\n4. SALVANDO ARQUIVOS")
    print("-" * 50)

    # Salvar feature registry
    with open('feature_registry.json', 'w', encoding='utf-8') as f:
        json.dump(feature_registry, f, indent=2, ensure_ascii=False)
    print("‚úì feature_registry.json salvo")

    # Salvar metadados do modelo
    with open('model_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(model_metadata, f, indent=2, ensure_ascii=False)
    print("‚úì model_metadata.json salvo")

    # Salvar modelo
    joblib.dump(modelo_final, 'modelo_lead_scoring_v1_devclub.pkl')
    print("‚úì modelo_lead_scoring_v1_devclub.pkl salvo")

    # Salvar features ordenadas (para garantir ordem correta em produ√ß√£o)
    features_ordenadas = {
        "feature_names": list(X_clean.columns),
        "feature_count": len(X_clean.columns),
        "created_at": datetime.now().isoformat()
    }
    with open('features_ordenadas.json', 'w', encoding='utf-8') as f:
        json.dump(features_ordenadas, f, indent=2, ensure_ascii=False)
    print("‚úì features_ordenadas.json salvo")

    # 5. RESUMO FINAL
    print("\n5. RESUMO DOS ARQUIVOS SALVOS")
    print("=" * 50)

    arquivos_salvos = [
        {
            "arquivo": "feature_registry.json",
            "tamanho_kb": round(len(json.dumps(feature_registry)) / 1024, 1),
            "descri√ß√£o": "Registry completo com 65 features, categoriza√ß√£o e mapeamentos"
        },
        {
            "arquivo": "model_metadata.json",
            "tamanho_kb": round(len(json.dumps(model_metadata)) / 1024, 1),
            "descri√ß√£o": f"Metadados do modelo com AUC {auc_final:.3f} e an√°lise de decis"
        },
        {
            "arquivo": "modelo_lead_scoring_v1_devclub.pkl",
            "tamanho_kb": "~500-1000",
            "descri√ß√£o": "Modelo Random Forest treinado e serializado"
        },
        {
            "arquivo": "features_ordenadas.json",
            "tamanho_kb": round(len(json.dumps(features_ordenadas)) / 1024, 1),
            "descri√ß√£o": "Lista ordenada de features para garantir ordem em produ√ß√£o"
        }
    ]

    print("ARQUIVOS CRIADOS:")
    for arquivo in arquivos_salvos:
        print(f"  üìÅ {arquivo['arquivo']}")
        print(f"     Tamanho: ~{arquivo['tamanho_kb']} KB")
        print(f"     Descri√ß√£o: {arquivo['descri√ß√£o']}")
        print()

    print("PR√ìXIMOS PASSOS:")
    print("1. Copiar arquivos para ambiente de produ√ß√£o")
    print("2. Criar pipeline de transforma√ß√£o baseado no feature_registry.json")
    print("3. Carregar modelo com joblib.load('modelo_lead_scoring_v1_devclub.pkl')")
    print("4. Validar pipeline com features_ordenadas.json")
    print("5. Implementar monitoramento baseado em model_metadata.json")

    return {
        "feature_registry": feature_registry,
        "model_metadata": model_metadata,
        "modelo_treinado": modelo_final,
        "auc_final": auc_final,
        "arquivos_salvos": [arq["arquivo"] for arq in arquivos_salvos]
    }

# Executar registro completo
resultado_registro = registrar_features_e_modelo()

"""## 25 - Salvamento dos top 4 modelos"""

import pandas as pd
import numpy as np
import json
import joblib
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
import sklearn

def registrar_features_e_modelos():
  """Registra features e salva os 4 modelos para produ√ß√£o"""

  print("REGISTRO DE FEATURES E MODELOS PARA PRODU√á√ÉO")
  print("=" * 55)

  # Definir os 4 modelos baseados nos resultados da valida√ß√£o
  modelos_para_salvar = [
      {
          "nome": "v1_devclub_rf_sem_utm",
          "dataset": dataset_v1_devclub_encoded,
          "dados_originais": dataset_v1_devclub,
          "remover_utm": True,
          "usar_cutoff": False,
          "algoritmo": "RandomForest"
      },
      {
          "nome": "v1_devclub_lgbm_cutoff",
          "dataset": dataset_v1_devclub_encoded,
          "dados_originais": dataset_v1_devclub,
          "remover_utm": False,
          "usar_cutoff": True,
          "algoritmo": "LightGBM"
      },
      {
          "nome": "v2_devclub_rf_cutoff",
          "dataset": dataset_v2_devclub_encoded,
          "dados_originais": dataset_v2_devclub,
          "remover_utm": False,
          "usar_cutoff": True,
          "algoritmo": "RandomForest"
      },
      {
          "nome": "v2_todos_rf_cutoff",
          "dataset": dataset_v2_todos_encoded,
          "dados_originais": dataset_v2_todos,
          "remover_utm": False,
          "usar_cutoff": True,
          "algoritmo": "RandomForest"
      }
  ]

  resultados_salvos = []

  for modelo_config in modelos_para_salvar:

      print(f"\n{'='*60}")
      print(f"PROCESSANDO: {modelo_config['nome'].upper()}")
      print("=" * 60)

      # 1. PREPARAR DADOS E TREINAR MODELO FINAL
      print("\n1. PREPARANDO DADOS E TREINANDO MODELO FINAL")
      print("-" * 50)

      # Dataset final
      dataset_final = modelo_config['dataset'].copy()

      # Remover UTM se necess√°rio
      if modelo_config['remover_utm']:
          colunas_utm = ['Source', 'Medium', 'Term']
          colunas_remover = []
          for col in dataset_final.columns:
              for utm in colunas_utm:
                  if col.startswith(f'{utm}_') or col == utm:
                      colunas_remover.append(col)
          dataset_final = dataset_final.drop(columns=colunas_remover, errors='ignore')

      # Split temporal
      data_dt = pd.to_datetime(modelo_config['dados_originais']['Data'], errors='coerce')

      # Aplicar cutoff se necess√°rio
      if modelo_config['usar_cutoff']:
          data_cutoff = pd.to_datetime('2025-08-10')
          mask_cutoff = data_dt <= data_cutoff
          data_dt = data_dt[mask_cutoff]
          dataset_final = dataset_final[mask_cutoff]

      data_min = data_dt.min()
      data_max = data_dt.max()
      dias_totais = (data_max - data_min).days
      dias_treino = int(dias_totais * 0.7)
      data_corte = data_min + pd.Timedelta(days=dias_treino)

      mask_treino = data_dt <= data_corte
      mask_teste = data_dt > data_corte

      X = dataset_final.drop(columns=['target'])
      y = dataset_final['target']

      # Limpar nomes das colunas
      X_clean = X.copy()
      X_clean.columns = X_clean.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)
      X_clean.columns = X_clean.columns.str.replace('__+', '_', regex=True)
      X_clean.columns = X_clean.columns.str.strip('_')

      X_train = X_clean[mask_treino]
      X_test = X_clean[mask_teste]
      y_train = y[mask_treino]
      y_test = y[mask_teste]

      print(f"Dataset: {len(dataset_final):,} registros")
      print(f"Features: {len(X_clean.columns)} colunas")
      print(f"Per√≠odo treino: {data_min.strftime('%Y-%m-%d')} a {data_corte.strftime('%Y-%m-%d')}")
      print(f"Per√≠odo teste: {data_corte.strftime('%Y-%m-%d')} a {data_max.strftime('%Y-%m-%d')}")
      print(f"Split: {len(X_train):,} treino / {len(X_test):,} teste")

      # Treinar modelo final baseado no algoritmo
      if modelo_config['algoritmo'] == 'RandomForest':
          modelo_final = RandomForestClassifier(
              n_estimators=100,
              max_depth=10,
              min_samples_split=2,
              min_samples_leaf=1,
              max_features='sqrt',
              class_weight='balanced',
              random_state=42,
              n_jobs=-1
          )
          modelo_final.fit(X_train, y_train)
          y_prob = modelo_final.predict_proba(X_test)[:, 1]

      elif modelo_config['algoritmo'] == 'LightGBM':
          lgbm_params = {
              'objective': 'binary',
              'metric': 'binary_logloss',
              'boosting_type': 'gbdt',
              'num_leaves': 31,
              'learning_rate': 0.05,
              'feature_fraction': 0.8,
              'bagging_fraction': 0.8,
              'bagging_freq': 5,
              'min_child_samples': 100,
              'verbose': -1,
              'random_state': 42,
              'is_unbalance': True
          }
          train_data = lgb.Dataset(X_train, label=y_train)
          modelo_final = lgb.train(lgbm_params, train_data, num_boost_round=500, callbacks=[lgb.log_evaluation(0)])
          y_prob = modelo_final.predict(X_test, num_iteration=modelo_final.best_iteration)

      auc_final = roc_auc_score(y_test, y_prob)
      print(f"Modelo treinado - AUC: {auc_final:.3f}")

      # Feature importance
      if modelo_config['algoritmo'] == 'RandomForest':
          feature_importance = pd.DataFrame({
              'feature_original': X.columns,
              'feature_clean': X_clean.columns,
              'importance': modelo_final.feature_importances_
          }).sort_values('importance', ascending=False)
      else:
          # Para LightGBM, usar importances do modelo
          importances = modelo_final.feature_importance(importance_type='gain')
          feature_importance = pd.DataFrame({
              'feature_original': X.columns,
              'feature_clean': X_clean.columns,
              'importance': importances
          }).sort_values('importance', ascending=False)

      # EXIBIR FEATURE IMPORTANCE COMPLETA
      print(f"\n" + "="*70)
      print(f"FEATURE IMPORTANCE COMPLETA - {modelo_config['nome'].upper()}")
      print("="*70)
      for i, (_, row) in enumerate(feature_importance.iterrows(), 1):
          print(f"{i:2d}. {row['feature_clean']}: {row['importance']:.6f}")

      # AN√ÅLISE ESPEC√çFICA: Features telefone_comprimento
      telefone_features = feature_importance[
          feature_importance['feature_clean'].str.contains('telefone_comprimento', case=False, na=False)
      ]
      print(f"\n" + "="*70)
      print(f"AN√ÅLISE TELEFONE_COMPRIMENTO - {modelo_config['nome'].upper()}")
      print("="*70)
      if len(telefone_features) > 0:
          print(f"Total encontradas: {len(telefone_features)}")
          print(f"Import√¢ncia total: {telefone_features['importance'].sum():.6f}")
          for i, (_, row) in enumerate(telefone_features.iterrows(), 1):
              posicao = feature_importance.reset_index(drop=True).index[feature_importance.reset_index(drop=True)['feature_clean'] == row['feature_clean']].tolist()[0] + 1
              print(f"  {i}. {row['feature_clean']}: {row['importance']:.6f} (POSI√á√ÉO: {posicao})")
      else:
          print("‚ùå NENHUMA FEATURE TELEFONE_COMPRIMENTO ENCONTRADA")
      print("="*70)

      # 2. CRIAR REGISTRY DE FEATURES
      print("\n2. CRIANDO FEATURE REGISTRY")
      print("-" * 50)

      # Categorizar features
      features_utm = []
      features_pesquisa = []
      features_derivadas = []
      features_outras = []

      for col in X.columns:
          if any(utm in col for utm in ['Source_', 'Medium_', 'Term_']):
              features_utm.append(col)
          elif any(pesq in col for pesq in ['g√™nero', 'idade', 'faz', 'faixa', 'cart√£o', 'estudou', 'faculdade', 'evento']):
              features_pesquisa.append(col)
          elif any(deriv in col for deriv in ['nome_', 'email_', 'telefone_', 'dia_semana']):
              features_derivadas.append(col)
          else:
              features_outras.append(col)

      # Mapeamento nome original -> nome limpo
      mapeamento_nomes = {}
      for orig, limpo in zip(X.columns, X_clean.columns):
          mapeamento_nomes[orig] = limpo

      # Criar registry completo
      feature_registry = {
          "metadata": {
              "created_at": datetime.now().isoformat(),
              "model_name": modelo_config['nome'],
              "dataset_name": f"dataset_{modelo_config['nome']}",
              "total_features": len(X.columns),
              "total_records": len(dataset_final),
              "target_column": "target",
              "model_type": modelo_config['algoritmo'],
              "sklearn_version": sklearn.__version__
          },
          "data_split": {
              "method": "temporal",
              "train_start": data_min.strftime('%Y-%m-%d'),
              "train_end": data_corte.strftime('%Y-%m-%d'),
              "test_start": data_corte.strftime('%Y-%m-%d'),
              "test_end": data_max.strftime('%Y-%m-%d'),
              "train_records": len(X_train),
              "test_records": len(X_test),
              "train_positive_rate": float(y_train.mean()),
              "test_positive_rate": float(y_test.mean())
          },
          "feature_categories": {
              "utm_features": {
                  "count": len(features_utm),
                  "description": "Features derived from UTM parameters (Source, Medium, Term)",
                  "features": features_utm
              },
              "survey_features": {
                  "count": len(features_pesquisa),
                  "description": "Features from lead survey responses",
                  "features": features_pesquisa
              },
              "derived_features": {
                  "count": len(features_derivadas),
                  "description": "Features engineered from raw data (name, email, phone, temporal)",
                  "features": features_derivadas
              },
              "other_features": {
                  "count": len(features_outras),
                  "description": "Additional features not in main categories",
                  "features": features_outras
              }
          },
          "feature_transformations": {
              "description": "Mapping from original feature names to model-ready names",
              "name_mapping": mapeamento_nomes,
              "encoding_applied": {
                  "categorical_encoding": "one-hot",
                  "ordinal_features": ["Qual a sua idade?", "Atualmente, qual a sua faixa salarial?"],
                  "binary_features": ["dia_semana"]
              },
              "column_cleaning": {
                  "regex_pattern": "[^A-Za-z0-9_] -> _",
                  "multiple_underscores": "__ -> _",
                  "strip_underscores": "leading/trailing removed"
              }
          },
          "feature_importance": {
              "description": f"Feature importance from trained {modelo_config['algoritmo']}",
              "top_10_features": [
                  {
                      "rank": i+1,
                      "feature_original": row['feature_original'],
                      "feature_clean": row['feature_clean'],
                      "importance": float(row['importance'])
                  }
                  for i, (_, row) in enumerate(feature_importance.head(10).iterrows())
              ],
              "utm_total_importance": float(
                  feature_importance[
                      feature_importance['feature_original'].str.contains('Source_|Medium_|Term_', case=False, na=False)
                  ]['importance'].sum()
              )
          },
          "expected_dtypes": {
              feature: str(dataset_final[feature].dtype) if feature in dataset_final.columns else "float64"
              for feature in X.columns
          },
          "validation_rules": {
              "required_features": list(X.columns),
              "optional_features": [],
              "total_expected_features": len(X.columns),
              "target_required": True,
              "missing_value_strategy": "model_will_fail_if_missing_features"
          }
      }

      print(f"Feature registry criado:")
      print(f"  - Features UTM: {len(features_utm)}")
      print(f"  - Features Pesquisa: {len(features_pesquisa)}")
      print(f"  - Features Derivadas: {len(features_derivadas)}")
      print(f"  - Features Outras: {len(features_outras)}")
      print(f"  - Mapeamentos de nomes: {len(mapeamento_nomes)}")
      print(f"  - Feature importance calculada para {len(feature_importance)} features")

      # 3. CRIAR METADADOS DO MODELO
      print("\n3. CRIANDO METADADOS DO MODELO")
      print("-" * 50)

      # Calcular m√©tricas detalhadas
      df_analise = pd.DataFrame({
          'probabilidade': y_prob,
          'target_real': y_test.reset_index(drop=True)
      })

      df_analise['decil'] = pd.qcut(
          df_analise['probabilidade'],
          q=10,
          labels=[f'D{i}' for i in range(1, 11)],
          duplicates='drop'
      )

      analise_decis = df_analise.groupby('decil', observed=True).agg({
          'target_real': ['count', 'sum', 'mean']
      }).round(4)

      analise_decis.columns = ['total_leads', 'conversoes', 'taxa_conversao']
      analise_decis['pct_total_conversoes'] = (
          analise_decis['conversoes'] / analise_decis['conversoes'].sum() * 100
      ).round(2)

      taxa_base = y_test.mean()
      analise_decis['lift'] = (analise_decis['taxa_conversao'] / taxa_base).round(2)

      top3_conversoes = analise_decis.tail(3)['pct_total_conversoes'].sum()
      top5_conversoes = analise_decis.tail(5)['pct_total_conversoes'].sum()
      lift_maximo = analise_decis['lift'].max()

      # Monotonia
      taxas = analise_decis['taxa_conversao'].values
      crescimentos = sum(1 for i in range(1, len(taxas)) if taxas[i] >= taxas[i-1])
      monotonia = (crescimentos / (len(taxas) - 1)) * 100 if len(taxas) > 1 else 100.0

      # Metadados do modelo
      model_metadata = {
          "model_info": {
              "model_name": modelo_config['nome'],
              "model_type": modelo_config['algoritmo'],
              "library": "scikit-learn" if modelo_config['algoritmo'] == 'RandomForest' else "lightgbm",
              "library_version": sklearn.__version__,
              "trained_at": datetime.now().isoformat(),
              "training_duration_info": "Trained on full pipeline with temporal split"
          },
          "hyperparameters": {
              "n_estimators": 100,
              "max_depth": 10,
              "min_samples_split": 2,
              "min_samples_leaf": 1,
              "max_features": "sqrt",
              "class_weight": "balanced",
              "random_state": 42,
              "n_jobs": -1
          } if modelo_config['algoritmo'] == 'RandomForest' else {
              "objective": "binary",
              "boosting_type": "gbdt",
              "num_leaves": 31,
              "learning_rate": 0.05,
              "feature_fraction": 0.8,
              "bagging_fraction": 0.8,
              "random_state": 42
          },
          "training_data": {
              "dataset_name": modelo_config['nome'],
              "total_records": len(dataset_final),
              "training_records": len(X_train),
              "test_records": len(X_test),
              "features_count": len(X_clean.columns),
              "target_distribution": {
                  "training_positive_rate": float(y_train.mean()),
                  "test_positive_rate": float(y_test.mean()),
                  "training_positive_count": int(y_train.sum()),
                  "test_positive_count": int(y_test.sum())
              }
          },
          "performance_metrics": {
              "auc": float(auc_final),
              "top3_decil_concentration": float(top3_conversoes),
              "top5_decil_concentration": float(top5_conversoes),
              "lift_maximum": float(lift_maximo),
              "monotonia_percentage": float(monotonia),
              "baseline_conversion_rate": float(taxa_base)
          },
          "decil_analysis": {
              f"decil_{i+1}": {
                  "total_leads": int(row['total_leads']),
                  "conversions": int(row['conversoes']),
                  "conversion_rate": float(row['taxa_conversao']),
                  "pct_total_conversions": float(row['pct_total_conversoes']),
                  "lift": float(row['lift'])
              }
              for i, (_, row) in enumerate(analise_decis.iterrows())
          },
          "production_notes": {
              "use_case": "Lead scoring for budget allocation",
              "prediction_interpretation": "Higher probability = higher priority for budget allocation",
              "calibration_status": "Not calibrated - use for ranking only",
              "recommended_deployment": "Batch scoring with temporal validation",
              "monitoring_requirements": "Track AUC degradation and decil stability over time"
          }
      }

      print(f"Metadados do modelo criados:")
      print(f"  - AUC: {auc_final:.3f}")
      print(f"  - Top 3 decis: {top3_conversoes:.1f}%")
      print(f"  - Top 5 decis: {top5_conversoes:.1f}%")
      print(f"  - Lift m√°ximo: {lift_maximo:.1f}x")
      print(f"  - Monotonia: {monotonia:.1f}%")
      print(f"  - An√°lise de 10 decis inclu√≠da")

      # 4. SALVAR ARQUIVOS
      print("\n4. SALVANDO ARQUIVOS")
      print("-" * 50)

      # Salvar feature registry
      registry_filename = f'feature_registry_{modelo_config["nome"]}.json'
      with open(registry_filename, 'w', encoding='utf-8') as f:
          json.dump(feature_registry, f, indent=2, ensure_ascii=False)
      print(f"‚úì {registry_filename} salvo")

      # Salvar metadados do modelo
      metadata_filename = f'model_metadata_{modelo_config["nome"]}.json'
      with open(metadata_filename, 'w', encoding='utf-8') as f:
          json.dump(model_metadata, f, indent=2, ensure_ascii=False)
      print(f"‚úì {metadata_filename} salvo")

      # Salvar modelo
      model_filename = f'modelo_lead_scoring_{modelo_config["nome"]}.pkl'
      joblib.dump(modelo_final, model_filename)
      print(f"‚úì {model_filename} salvo")

      # Salvar features ordenadas (para garantir ordem correta em produ√ß√£o)
      features_filename = f'features_ordenadas_{modelo_config["nome"]}.json'
      features_ordenadas = {
          "feature_names": list(X_clean.columns),
          "feature_count": len(X_clean.columns),
          "created_at": datetime.now().isoformat()
      }
      with open(features_filename, 'w', encoding='utf-8') as f:
          json.dump(features_ordenadas, f, indent=2, ensure_ascii=False)
      print(f"‚úì {features_filename} salvo")

      resultados_salvos.append({
          "modelo": modelo_config['nome'],
          "auc": auc_final,
          "arquivos": [registry_filename, metadata_filename, model_filename, features_filename]
      })

  # 5. RESUMO FINAL
  print(f"\n{'='*60}")
  print("RESUMO DOS MODELOS SALVOS")
  print("=" * 60)

  for resultado in resultados_salvos:
      print(f"\nüìÅ {resultado['modelo']}:")
      print(f"   AUC: {resultado['auc']:.3f}")
      print(f"   Arquivos: {len(resultado['arquivos'])} salvos")

  total_arquivos = sum(len(r['arquivos']) for r in resultados_salvos)
  print(f"\nTotal: {len(resultados_salvos)} modelos = {total_arquivos} arquivos salvos")

  print("\nPR√ìXIMOS PASSOS:")
  print("1. Copiar arquivos para ambiente de produ√ß√£o")
  print("2. Implementar pipeline de scoring usando os 4 modelos")
  print("3. Validar pipeline com features_ordenadas_*.json")
  print("4. Implementar monitoramento baseado em model_metadata_*.json")

  return resultados_salvos

# Executar registro completo dos 4 modelos
resultado_registro_multiplo = registrar_features_e_modelos()

