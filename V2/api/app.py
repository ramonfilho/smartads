"""
API V2 para Lead Scoring - Batch Predictions
Otimizada para Google Sheets + Apps Script + Google Cloud
"""

import os
import sys
import pandas as pd
import numpy as np
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import io
import time
import tempfile
import uuid
from datetime import datetime
import logging

# Adicionar diret√≥rio pai ao path para imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importar pipeline V2
from src.production_pipeline import LeadScoringPipeline

# Importar integra√ß√µes
from api.meta_integration import MetaAdsIntegration, enrich_utm_analysis_with_costs, enrich_utm_with_hierarchy
from api.meta_config import META_CONFIG, BUSINESS_CONFIG
from api.economic_metrics import enrich_utm_with_economic_metrics

# Importar m√≥dulos CAPI
from api.database import get_db, init_database, create_lead_capi, count_leads, count_leads_with_fbp, count_leads_with_fbc, get_leads_by_emails
from api.capi_integration import send_batch_events
from fastapi import Depends, Request
from sqlalchemy.orm import Session

# Padr√µes de UTMs inv√°lidos (bare names e gen√©ricos)
BARE_CAMPAIGN_NAMES = ['DEVLF', 'devlf']                      # Prefixos incompletos
BARE_MEDIUM_NAMES = ['dgen', 'paid']                          # Termos gen√©ricos sem estrutura
GENERIC_TERMS = ['fb', 'ig', 'instagram', 'facebook']         # Apenas redes sociais

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === MODELS ===
class LeadData(BaseModel):
    """Modelo para um lead individual"""
    data: Dict[str, Any]
    email: Optional[str] = None  # Para identifica√ß√£o
    row_id: Optional[str] = None  # ID da linha no Google Sheets

class BatchPredictionRequest(BaseModel):
    """Request para predi√ß√µes em batch"""
    leads: List[LeadData] = Field(..., min_items=1, max_items=600)
    request_id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()))

class PredictionResult(BaseModel):
    """Resultado de uma predi√ß√£o"""
    lead_score: float
    email: Optional[str] = None
    row_id: Optional[str] = None

class BatchPredictionResponse(BaseModel):
    """Response para predi√ß√µes em batch"""
    request_id: str
    total_leads: int
    predictions: List[PredictionResult]
    processing_time_seconds: float
    timestamp: str

# Inicializar a aplica√ß√£o FastAPI
app = FastAPI(
    title="Smart Ads Lead Scoring API V2",
    description="API otimizada para predi√ß√µes em batch via Google Sheets",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Adicionar CORS para Google Apps Script e Landing Pages
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://script.google.com",
        "https://script.googleusercontent.com",
        "http://localhost:8001",
        "http://localhost:8000",
        "https://lp.devclub.com.br",
        "*"  # Permitir todos (TEMPOR√ÅRIO - em produ√ß√£o especificar dom√≠nios)
    ],
    allow_methods=["POST", "GET", "OPTIONS"],
    allow_headers=["*"],
    allow_credentials=True,
)

# Vari√°vel global para o pipeline
pipeline = None

def initialize_pipeline():
    """Inicializa o pipeline de lead scoring com modelo ativo do configs/active_model.yaml"""
    global pipeline
    try:
        logger.info("Inicializando pipeline de Lead Scoring...")
        # Usar modelo ativo do configs/active_model.yaml (n√£o passar model_name)
        pipeline = LeadScoringPipeline()
        logger.info("Pipeline inicializado com sucesso!")
        return True
    except Exception as e:
        logger.error(f"Erro ao inicializar pipeline: {e}")
        return False

# DEPRECATED: Decis agora s√£o calculados por janela de an√°lise
# def convert_decile_to_numeric(decile_str: str) -> int:
#     """Converte D1-D10 para 1-10"""
#     try:
#         return int(decile_str.replace('D', ''))
#     except:
#         return 5

@app.on_event("startup")
async def startup_event():
    """Inicializa√ß√£o da aplica√ß√£o"""
    logger.info("üöÄ Iniciando Smart Ads API V2...")
    if not initialize_pipeline():
        logger.error("‚ùå Falha ao inicializar pipeline!")
    else:
        logger.info("‚úÖ API V2 pronta para receber requisi√ß√µes!")

    # Inicializar database
    if init_database():
        logger.info("‚úÖ Database inicializado com sucesso")
    else:
        logger.warning("‚ö†Ô∏è Database n√£o inicializado (desenvolvimento sem PostgreSQL?)")

@app.get("/")
async def root():
    """Endpoint raiz"""
    return {
        "message": "Smart Ads Lead Scoring API V2",
        "status": "online",
        "version": "2.0.0",
        "endpoints": {
            "health": "/health",
            "predict": "/predict/batch (POST)",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    """Health check detalhado"""
    pipeline_status = "healthy" if pipeline is not None else "unhealthy"
    model_loaded = pipeline is not None

    return {
        "status": "healthy",
        "pipeline_status": pipeline_status,
        "model_loaded": model_loaded,
        "timestamp": datetime.now().isoformat(),
        "version": "2.0.0"
    }

@app.get("/model/info")
async def get_model_info():
    """
    Retorna informa√ß√µes sobre o modelo: metadados, performance e feature importances
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="Pipeline n√£o inicializado")

    try:
        # Garantir que o modelo est√° carregado
        if pipeline.predictor.model is None:
            pipeline.predictor.load_model()

        # Obter metadados
        metadata = pipeline.predictor.metadata

        # Obter feature importances (todas)
        feature_importances = pipeline.predictor.get_feature_importances(top_n=None)

        # Carregar mapeamento de nomes de features (transformado ‚Üí leg√≠vel)
        try:
            import json
            from pathlib import Path
            mapping_file = Path(__file__).parent.parent / "arquivos_modelo" / "feature_name_mapping_v1_devclub_rf_temporal_single.json"
            if mapping_file.exists():
                with open(mapping_file) as f:
                    mapping_data = json.load(f)
                    feature_name_mapping = mapping_data.get("feature_name_mapping", {})

                # Traduzir nomes das features para vers√£o leg√≠vel
                for feature_importance in feature_importances:
                    transformed_name = feature_importance['feature']
                    readable_name = feature_name_mapping.get(transformed_name, transformed_name.replace('_', ' '))
                    feature_importance['feature_readable'] = readable_name
                    feature_importance['feature_transformed'] = transformed_name
                    feature_importance['feature'] = readable_name  # Usar nome leg√≠vel por padr√£o
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel carregar mapeamento de features: {e}")

        # Estruturar resposta
        response = {
            "model_info": metadata.get("model_info", {}),
            "training_data": metadata.get("training_data", {}),
            "performance_metrics": metadata.get("performance_metrics", {}),
            "decil_analysis": metadata.get("decil_analysis", {}),
            "feature_importances": feature_importances,
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"‚úÖ Informa√ß√µes do modelo retornadas com sucesso")
        return response

    except Exception as e:
        logger.error(f"‚ùå Erro ao obter informa√ß√µes do modelo: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao obter informa√ß√µes do modelo: {str(e)}")

@app.post("/predict/batch", response_model=BatchPredictionResponse)
async def predict_batch_json(request: BatchPredictionRequest):
    """
    Predi√ß√£o em batch via JSON
    Otimizado para Google Apps Script
    """
    global pipeline

    # Verificar pipeline
    if pipeline is None:
        if not initialize_pipeline():
            raise HTTPException(status_code=500, detail="Pipeline n√£o inicializado")

    start_time = time.time()
    logger.info(f"üìä Processando {len(request.leads)} leads (Request ID: {request.request_id})")

    temp_file = None

    try:
        # Converter leads para DataFrame
        lead_rows = []
        for i, lead in enumerate(request.leads):
            row = lead.data.copy()
            # Adicionar metadados
            row['_email'] = lead.email
            row['_row_id'] = lead.row_id or str(i)
            lead_rows.append(row)

        df = pd.DataFrame(lead_rows)
        logger.info(f"üìã DataFrame criado: {df.shape}")

        # Criar arquivo tempor√°rio para o pipeline
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
            # Salvar sem as colunas de metadados para o modelo
            model_df = df.drop(columns=['_email', '_row_id'], errors='ignore')
            model_df.to_csv(tmp, index=False)
            temp_file = tmp.name

        # Executar pipeline
        logger.info("üîÑ Executando pipeline...")
        result_df = pipeline.run(temp_file, with_predictions=True)

        if result_df is None or len(result_df) == 0:
            raise HTTPException(status_code=500, detail="Pipeline retornou resultado vazio")

        # Processar resultados
        predictions = []
        for i, (_, row) in enumerate(result_df.iterrows()):
            lead_score = float(row['lead_score'])

            # Recuperar metadados do lead original
            original_lead = request.leads[i] if i < len(request.leads) else None
            email = original_lead.email if original_lead else None
            row_id = original_lead.row_id if original_lead else str(i)

            predictions.append(PredictionResult(
                lead_score=lead_score,
                email=email,
                row_id=row_id
            ))

        processing_time = time.time() - start_time

        logger.info(f"‚úÖ Processamento conclu√≠do em {processing_time:.2f}s")
        logger.info(f"üìà Scores: min={min(p.lead_score for p in predictions):.3f}, max={max(p.lead_score for p in predictions):.3f}")

        return BatchPredictionResponse(
            request_id=request.request_id,
            total_leads=len(predictions),
            predictions=predictions,
            processing_time_seconds=round(processing_time, 2),
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.error(f"‚ùå Erro no processamento: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro no processamento: {str(e)}")
    finally:
        # Limpar arquivo tempor√°rio
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

@app.post("/predict/csv")
async def predict_batch_csv(file: UploadFile = File(...)):
    """
    Predi√ß√£o em batch via upload CSV
    Para testes ou uploads manuais
    """
    global pipeline

    if pipeline is None:
        if not initialize_pipeline():
            raise HTTPException(status_code=500, detail="Pipeline n√£o inicializado")

    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="Apenas arquivos CSV s√£o aceitos")

    start_time = time.time()
    logger.info(f"üìÑ Processando arquivo CSV: {file.filename}")

    temp_file = None

    try:
        # Salvar arquivo tempor√°rio
        with tempfile.NamedTemporaryFile(mode='wb', suffix='.csv', delete=False) as tmp:
            content = await file.read()
            tmp.write(content)
            temp_file = tmp.name

        # Executar pipeline
        result_df = pipeline.run(temp_file, with_predictions=True)

        if result_df is None:
            raise HTTPException(status_code=500, detail="Pipeline retornou resultado vazio")

        # Processar resultados
        predictions = []
        for _, row in result_df.iterrows():
            predictions.append({
                "lead_score": float(row['lead_score']),  # Probabilidade
                "email": row.get('E-mail', None),
                "name": row.get('Nome Completo', None)
            })

        processing_time = time.time() - start_time

        logger.info(f"‚úÖ CSV processado: {len(predictions)} leads em {processing_time:.2f}s")

        return {
            "total_leads": len(predictions),
            "predictions": predictions,
            "processing_time_seconds": round(processing_time, 2),
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"‚ùå Erro no processamento CSV: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro no processamento: {str(e)}")
    finally:
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

# === WEBHOOK PARA CAPTURA DE LEADS (CAPI) ===

class LeadCaptureRequest(BaseModel):
    """Dados capturados do lead no frontend"""
    # Dados pessoais
    name: str  # Nome completo (mantido para compatibilidade)
    first_name: Optional[str] = None  # Primeiro nome (para CAPI)
    last_name: Optional[str] = None   # Sobrenome (para CAPI)
    email: str
    phone: Optional[str] = None

    # Dados CAPI
    fbp: Optional[str] = None
    fbc: Optional[str] = None
    event_id: str
    user_agent: Optional[str] = None
    event_source_url: Optional[str] = None

    # UTMs
    utm_source: Optional[str] = None
    utm_medium: Optional[str] = None
    utm_campaign: Optional[str] = None
    utm_term: Optional[str] = None
    utm_content: Optional[str] = None

    # Outros
    tem_comp: Optional[str] = None

@app.post("/webhook/lead_capture")
async def webhook_lead_capture(
    request: Request,
    lead_data: LeadCaptureRequest,
    db: Session = Depends(get_db)
):
    """
    Webhook para capturar dados de leads com FBP/FBC
    Chamado pelo formul√°rio frontend ap√≥s envio do lead
    """
    try:
        # Verificar se √© p√°gina de parab√©ns - IGNORAR para evitar duplicatas
        # P√°gina de Parab√©ns captura dados incompletos (sem first_name/last_name)
        # Lead j√° foi capturado corretamente na LP (inscricao)
        event_url = lead_data.event_source_url or ''
        if 'parabens' in event_url.lower():
            logger.info(f"‚è≠Ô∏è Ignorando captura da p√°gina de Parab√©ns: {lead_data.email}")
            return {
                "status": "success",
                "message": "Captura j√° realizada na LP",
                "skipped": True
            }

        # Capturar IP do cliente (real, n√£o do proxy Cloud Run)
        client_ip = request.headers.get('X-Forwarded-For', request.client.host).split(',')[0].strip()

        # Preparar dados para banco
        lead_dict = {
            'email': lead_data.email,
            'name': lead_data.name,
            'first_name': lead_data.first_name,
            'last_name': lead_data.last_name,
            'phone': lead_data.phone,
            'fbp': lead_data.fbp,
            'fbc': lead_data.fbc,
            'event_id': lead_data.event_id,
            'user_agent': lead_data.user_agent,
            'client_ip': client_ip,
            'event_source_url': lead_data.event_source_url,
            'utm_source': lead_data.utm_source,
            'utm_medium': lead_data.utm_medium,
            'utm_campaign': lead_data.utm_campaign,
            'utm_term': lead_data.utm_term,
            'utm_content': lead_data.utm_content,
            'tem_comp': lead_data.tem_comp
        }

        # Salvar no banco
        lead_record = create_lead_capi(db, lead_dict)

        logger.info(f"‚úÖ Lead capturado: {lead_data.email} (ID: {lead_record.id}, Event ID: {lead_data.event_id})")

        return {
            "status": "success",
            "message": "Lead capturado com sucesso",
            "lead_id": lead_record.id,
            "event_id": lead_data.event_id
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao capturar lead: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao capturar lead: {str(e)}")

@app.get("/webhook/lead_capture/stats")
async def lead_capture_stats(db: Session = Depends(get_db)):
    """
    Estat√≠sticas de captura de leads CAPI
    √ötil para monitoramento e debug
    """
    try:
        total = count_leads(db)
        with_fbp = count_leads_with_fbp(db)
        with_fbc = count_leads_with_fbc(db)

        return {
            "total_leads": total,
            "leads_with_fbp": with_fbp,
            "leads_with_fbc": with_fbc,
            "fbp_fill_rate": round(with_fbp / total * 100, 2) if total > 0 else 0,
            "fbc_fill_rate": round(with_fbc / total * 100, 2) if total > 0 else 0
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao obter stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao obter stats: {str(e)}")

@app.get("/webhook/lead_capture/recent")
async def get_recent_leads_endpoint(limit: int = 10, db: Session = Depends(get_db)):
    """
    DEBUG: Retorna leads mais recentes
    """
    try:
        from api.database import get_recent_leads
        leads = get_recent_leads(db, limit=limit)

        return {
            "total": len(leads),
            "leads": [lead.to_dict() for lead in leads]
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar leads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro: {str(e)}")

# === CAPI BATCH PROCESSING ===

def _safe_parse_timestamp(data_value) -> int:
    """
    Parse timestamp de forma segura, tratando casos de erro

    Args:
        data_value: Valor da data (pode ser int, float, string, ou "#ERROR!")

    Returns:
        int: Timestamp UNIX (segundos desde epoch)
    """
    try:
        # Caso 1: J√° √© timestamp num√©rico
        if isinstance(data_value, (int, float)):
            return int(data_value)

        # Caso 2: String vazia ou None
        if not data_value or str(data_value).strip() == '':
            logger.warning("‚ö†Ô∏è Data vazia, usando timestamp atual")
            return int(time.time())

        # Caso 3: String "#ERROR!" do Google Sheets
        if str(data_value).strip() == '#ERROR!':
            logger.warning("‚ö†Ô∏è Data com erro (#ERROR!), usando timestamp atual")
            return int(time.time())

        # Caso 4: String de data v√°lida
        parsed_date = pd.to_datetime(data_value, errors='coerce')

        # Se parsing falhou (NaT - Not a Time)
        if pd.isna(parsed_date):
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel parsear data '{data_value}', usando timestamp atual")
            return int(time.time())

        return int(parsed_date.timestamp())

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao parsear timestamp '{data_value}': {str(e)}, usando timestamp atual")
        return int(time.time())

class CapiBatchRequest(BaseModel):
    """Request para processamento batch CAPI"""
    leads: List[Dict[str, Any]] = Field(..., description="TODOS os leads do dia anterior (D1-D10)")

class CapiCheckSentRequest(BaseModel):
    """Request para verificar quais leads j√° foram enviados"""
    emails: List[str] = Field(..., description="Lista de emails para verificar")

@app.post("/capi/process_daily_batch")
async def process_daily_batch_capi(
    request: CapiBatchRequest,
    db: Session = Depends(get_db)
):
    """
    Processa batch de CAPI com thresholds fixos
    Envia 2 eventos para cada lead:
    - LeadQualified (com valor): TODOS os leads (D1-D10)
    - LeadQualifiedHighQuality (sem valor): Apenas D9-D10

    Chamado pelo Apps Script a cada 3 horas ap√≥s classifica√ß√£o ML
    """
    try:
        logger.info(f"üìä Processando batch CAPI: {len(request.leads)} leads (D1-D10)")

        # ====================================================================
        # ETAPA 1: CARREGAR THRESHOLDS FIXOS DO MODELO ATIVO
        # ====================================================================
        from src.model.decil_thresholds import atribuir_decis_batch

        # Carregar thresholds do modelo ativo (via pipeline global)
        thresholds = pipeline.predictor.metadata.get('decil_thresholds', {}).get('thresholds')

        if not thresholds:
            logger.error("‚ùå Thresholds n√£o encontrados no metadata do modelo!")
            raise HTTPException(
                status_code=500,
                detail="Thresholds n√£o configurados no modelo. Retreine o modelo com --save-files."
            )

        logger.info(f"   ‚úÖ Thresholds carregados: {pipeline.predictor.metadata['model_info']['model_name']}")

        # ====================================================================
        # ETAPA 2: CALCULAR DECIS USANDO THRESHOLDS FIXOS
        # ====================================================================
        # Criar DataFrame com lead_scores - APENAS leads com score v√°lido
        # CORRE√á√ÉO: Filtrar leads SEM score para evitar erro "list indices must be integers"
        valid_leads = []
        invalid_count = 0

        for lead in request.leads:
            if 'email' not in lead or 'lead_score' not in lead:
                continue

            score_val = lead['lead_score']

            # Validar que score n√£o √© vazio/inv√°lido
            if score_val in [None, '', 'null', 'NaN'] or str(score_val).strip() == '':
                invalid_count += 1
                continue

            try:
                score_float = float(score_val)
                # Validar range (0 < score <= 1)
                if 0 < score_float <= 1:
                    valid_leads.append({
                        'email': lead['email'],
                        'lead_score': score_float
                    })
                else:
                    invalid_count += 1
            except (ValueError, TypeError):
                invalid_count += 1
                continue

        if invalid_count > 0:
            logger.warning(f"‚ö†Ô∏è {invalid_count} leads com lead_score inv√°lido/vazio ignorados")

        leads_df = pd.DataFrame(valid_leads)

        if len(leads_df) == 0:
            logger.warning("‚ö†Ô∏è Nenhum lead com lead_score v√°lido encontrado")
            logger.warning("üí° Sugest√£o: Gere os scores primeiro usando /predict/batch")
            return {
                "status": "error",
                "message": "Nenhum lead com lead_score v√°lido encontrado. Gere os scores primeiro usando /predict/batch antes de enviar para CAPI.",
                "total": len(request.leads),
                "valid_leads": 0,
                "invalid_leads": invalid_count,
                "success": 0,
                "errors": 0
            }

        # Garantir que lead_score √© num√©rico (convers√£o final)
        leads_df['lead_score'] = pd.to_numeric(leads_df['lead_score'], errors='coerce')

        # Remover qualquer NaN que possa ter sido gerado
        nan_count = leads_df['lead_score'].isna().sum()
        if nan_count > 0:
            logger.warning(f"‚ö†Ô∏è {nan_count} scores NaN detectados e removidos")
            leads_df = leads_df[leads_df['lead_score'].notna()].copy()

        if len(leads_df) == 0:
            logger.error("‚ùå Todos os scores s√£o inv√°lidos ap√≥s convers√£o num√©rica")
            return {
                "status": "error",
                "message": "Todos os scores s√£o inv√°lidos",
                "total": len(request.leads),
                "success": 0,
                "errors": len(request.leads)
            }

        # An√°lise de scores
        logger.info(f"   üìä An√°lise de scores:")
        logger.info(f"      - Total de leads: {len(leads_df)}")
        logger.info(f"      - Valores √∫nicos: {leads_df['lead_score'].nunique()}")
        logger.info(f"      - Score min: {leads_df['lead_score'].min():.4f}")
        logger.info(f"      - Score max: {leads_df['lead_score'].max():.4f}")
        logger.info(f"      - Score mean: {leads_df['lead_score'].mean():.4f}")
        logger.info(f"      - Score std: {leads_df['lead_score'].std():.4f}")

        # Atribuir decis usando thresholds fixos
        logger.info(f"   üîÑ Atribuindo decis usando thresholds fixos...")
        leads_df['decil'] = atribuir_decis_batch(
            leads_df['lead_score'].values,
            thresholds
        )

        # Criar mapeamento email ‚Üí decil
        decil_map = dict(zip(leads_df['email'], leads_df['decil']))

        logger.info(f"   ‚úÖ Decis calculados para {len(decil_map)} leads (thresholds fixos)")
        logger.info(f"   üìä Distribui√ß√£o por decil: {leads_df['decil'].value_counts().sort_index().to_dict()}")

        # ====================================================================
        # ETAPA 3: BUSCAR DADOS CAPI DO BANCO
        # ====================================================================
        emails = list(decil_map.keys())
        leads_capi = get_leads_by_emails(db, emails)

        # Criar mapeamento email ‚Üí dados CAPI
        # Prioriza registros com first_name preenchido (evita sobrescrever com registro incompleto)
        capi_map = {}
        for lead in leads_capi:
            existing = capi_map.get(lead.email)
            if existing is None:
                # Primeiro registro para este email
                capi_map[lead.email] = lead
            elif lead.first_name and not existing.first_name:
                # Novo registro tem first_name, existente n√£o tem - usar o novo
                capi_map[lead.email] = lead
            # Caso contr√°rio, manter o existente

        logger.info(f"   {len(capi_map)} leads encontrados no banco CAPI")

        # ====================================================================
        # ETAPA 4: ENRIQUECER LEADS COM DECIS E DADOS CAPI
        # ====================================================================
        enriched_leads = []
        for lead in request.leads:
            email = lead.get('email')
            if not email:
                continue

            # Obter decil calculado
            decil = str(decil_map.get(email, 'D1'))  # Default D1 se n√£o encontrado

            capi_data = capi_map.get(email)

            # Extrair first_name e last_name - prioridade: pesquisa (100% preenchido)
            first_name = None
            last_name = None

            # 1. Tentar da pesquisa (campo 'Nome Completo')
            nome_completo = lead.get('Nome Completo', '')
            if nome_completo and str(nome_completo).strip():
                name_parts = str(nome_completo).strip().split(' ', 1)
                first_name = name_parts[0]
                last_name = name_parts[1] if len(name_parts) > 1 else None
            # 2. Fallback: banco CAPI
            elif capi_data:
                if capi_data.first_name:
                    first_name = capi_data.first_name
                    last_name = capi_data.last_name
                elif capi_data.name:
                    name_parts = capi_data.name.strip().split(' ', 1)
                    first_name = name_parts[0]
                    last_name = name_parts[1] if len(name_parts) > 1 else None

            # Montar dados para CAPI
            # Phone: tentar 'phone' (do Apps Script) ou 'Telefone' (da pesquisa)
            phone = lead.get('phone') or lead.get('Telefone')

            # Dados da pesquisa (enriquecem targeting da Meta)
            # IMPORTANTE: Converter TODOS os valores para string (Apps Script envia valores num√©ricos)
            # Fix para "'int' object is not iterable" - Meta SDK n√£o aceita valores num√©ricos em custom_data
            survey_data_raw = {
                'genero': lead.get('O seu g√™nero:'),
                'estado': lead.get('Qual estado voc√™ mora?'),
                'idade': lead.get('Qual a sua idade?'),
                'ocupacao': lead.get('O que voc√™ faz atualmente?'),
                'faixa_salarial': lead.get('Atualmente, qual a sua faixa salarial?'),
                'tem_cartao': lead.get('Voc√™ possui cart√£o de cr√©dito?'),
                'ja_estudou_prog': lead.get('J√° estudou programa√ß√£o?'),
                'faculdade': lead.get('Voc√™ j√° fez/faz/pretende fazer faculdade?'),
                'investiu_curso': lead.get('J√° investiu em algum curso online para aprender uma nova forma de ganhar dinheiro?'),
                'interesse_prog': lead.get('O que mais te chama aten√ß√£o na profiss√£o de Programador?'),
                'quer_ver_evento': lead.get('O que mais voc√™ quer ver no evento?'),
                'tem_computador': lead.get('Tem computador/notebook?'),
                'cidade': lead.get('cidade'),
                'cep': lead.get('cep')
            }

            # Converter todos os valores para string
            survey_data = {k: str(v) if v is not None else None for k, v in survey_data_raw.items()}

            # Garantir que lead_score √© float (Apps Script pode enviar como n√∫mero ou string)
            lead_score_value = float(lead['lead_score'])

            lead_capi = {
                'email': email,
                'phone': phone,
                'first_name': first_name,
                'last_name': last_name,
                'lead_score': lead_score_value,  # Garantido como float
                'decil': decil,
                'event_id': capi_data.event_id if capi_data else f"lead_{int(time.time())}_{str(email)[:8]}",
                'fbp': capi_data.fbp if capi_data else None,
                'fbc': capi_data.fbc if capi_data else None,
                'user_agent': capi_data.user_agent if capi_data else None,
                'client_ip': capi_data.client_ip if capi_data else None,
                'event_source_url': capi_data.event_source_url if capi_data else None,
                'event_timestamp': _safe_parse_timestamp(lead.get('data')),
                'survey_data': survey_data
            }

            enriched_leads.append(lead_capi)

        logger.info(f"   {len(enriched_leads)} leads enriquecidos para envio CAPI")

        # Logging de qualidade dos dados
        leads_with_fbp = len([l for l in enriched_leads if l.get('fbp')])
        leads_with_fbc = len([l for l in enriched_leads if l.get('fbc')])
        leads_with_both = len([l for l in enriched_leads if l.get('fbp') and l.get('fbc')])

        logger.info(f"   üìä Qualidade dos dados CAPI:")
        logger.info(f"      - Com FBP: {leads_with_fbp}/{len(enriched_leads)} ({leads_with_fbp/len(enriched_leads)*100:.1f}%)")
        logger.info(f"      - Com FBC: {leads_with_fbc}/{len(enriched_leads)} ({leads_with_fbc/len(enriched_leads)*100:.1f}%)")
        logger.info(f"      - Com AMBOS: {leads_with_both}/{len(enriched_leads)} ({leads_with_both/len(enriched_leads)*100:.1f}%)")

        # Enviar batch (com db session para registrar envios)
        results = send_batch_events(enriched_leads, db=db)

        logger.info(f"‚úÖ Batch CAPI processado: {results['success']}/{results['total']} enviados")

        return {
            "status": "success",
            "total": results['total'],
            "success": results['success'],
            "errors": results['errors'],
            "leads_with_capi_data": len([l for l in enriched_leads if l['fbp'] or l['fbc']]),
            "leads_with_fbp": leads_with_fbp,
            "leads_with_fbc": leads_with_fbc,
            "leads_with_both": leads_with_both,
            "capi_data_quality_pct": round(leads_with_both/len(enriched_leads)*100, 1) if enriched_leads else 0,
            "details": results.get('details', [])
        }

    except Exception as e:
        import traceback
        logger.error(f"‚ùå Erro no batch CAPI: {str(e)}")
        logger.error(f"Stack trace: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Erro no batch CAPI: {str(e)}")

@app.post("/capi/check_sent")
async def check_capi_sent(
    request: CapiCheckSentRequest,
    db: Session = Depends(get_db)
):
    """
    Verifica quais leads da lista j√° foram enviados para CAPI

    Args:
        request: Lista de emails para verificar

    Returns:
        {
            "total_checked": int,
            "sent_count": int,
            "not_sent_count": int,
            "sent_emails": List[str]
        }
    """
    try:
        from api.database import get_leads_already_sent_to_capi

        logger.info(f"üîç Verificando {len(request.emails)} emails nos logs CAPI")

        # Buscar leads j√° enviados
        sent_emails = get_leads_already_sent_to_capi(db, request.emails)

        logger.info(f"‚úÖ {len(sent_emails)}/{len(request.emails)} j√° foram enviados para CAPI")

        return {
            "total_checked": len(request.emails),
            "sent_count": len(sent_emails),
            "not_sent_count": len(request.emails) - len(sent_emails),
            "sent_emails": sent_emails
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar logs CAPI: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao verificar logs: {str(e)}")

# === AN√ÅLISE UTM COM CUSTOS ===

class UTMAnalysisRequest(BaseModel):
    """Request para an√°lise UTM com custos"""
    leads: List[LeadData] = Field(..., min_items=1)  # Sem limite m√°ximo - batching interno
    account_id: str = Field(..., description="ID da conta Meta Ads (ex: act_123456)")
    product_value: Optional[float] = Field(default=None, description="Valor do produto (padr√£o: config)")
    min_roas: Optional[float] = Field(default=None, description="ROAS m√≠nimo (padr√£o: 2.0)")

class UTMDimensionMetrics(BaseModel):
    """M√©tricas de uma dimens√£o UTM"""
    campaign: Optional[str] = None  # Para adsets e ads: nome da campanha de origem
    adset: Optional[str] = None     # Para ads: nome do adset de origem
    value: str
    leads: int
    spend: float
    cpl: float
    taxa_proj: float
    receita_proj: float     # Receita projetada (NOVO - Margem de Contribui√ß√£o)
    margem_contrib: float   # Margem de Contribui√ß√£o (NOVO - substitui margem%)
    roas_proj: float
    acao: str
    budget_current: float  # Or√ßamento atual (gasto do per√≠odo)
    budget_target: float   # Or√ßamento alvo (baseado na a√ß√£o)

class UTMPeriodAnalysis(BaseModel):
    """An√°lise UTM para um per√≠odo"""
    campaign: List[UTMDimensionMetrics]
    medium: List[UTMDimensionMetrics]
    ad: List[UTMDimensionMetrics]
    google_ads: List[UTMDimensionMetrics]
    # Metadados do per√≠odo
    period_start: str  # Data/hora do lead mais antigo (ISO format)
    period_end: str    # Data/hora do lead mais recente (ISO format)
    total_leads: int   # Total de leads analisados
    meta_leads: int    # Leads do Meta/Facebook
    google_leads: int  # Leads do Google Ads

class UTMAnalysisResponse(BaseModel):
    """Response completa da an√°lise UTM"""
    request_id: str
    periods: Dict[str, UTMPeriodAnalysis]  # '1D', '3D', '7D', 'Total'
    config: Dict[str, Any]  # product_value, min_roas usado
    processing_time_seconds: float
    timestamp: str

@app.post("/analyze_utms_with_costs", response_model=UTMAnalysisResponse)
async def analyze_utms_with_costs(request: UTMAnalysisRequest):
    """
    An√°lise UTM enriquecida com custos do Meta Ads e m√©tricas econ√¥micas

    Fluxo:
    1. Executar predi√ß√µes (lead_score, decile)
    2. Buscar custos da API Meta (1D, 3D, 7D, Total)
    3. Calcular an√°lise UTM por dimens√£o
    4. Enriquecer com m√©tricas econ√¥micas (CPL, ROAS, Margem, A√ß√£o)
    5. Retornar estrutura por per√≠odo
    """
    global pipeline

    # Verificar pipeline
    if pipeline is None:
        if not initialize_pipeline():
            raise HTTPException(status_code=500, detail="Pipeline n√£o inicializado")

    start_time = time.time()
    request_id = str(uuid.uuid4())

    logger.info(f"üìä Iniciando an√°lise UTM com custos (Request ID: {request_id})")
    logger.info(f"   Leads: {len(request.leads)} | Account: {request.account_id}")

    temp_file = None  # Para limpeza no finally (apenas para caso de lote √∫nico)

    try:
        # Configura√ß√£o
        product_value = request.product_value or BUSINESS_CONFIG['product_value']
        min_roas = request.min_roas or BUSINESS_CONFIG['min_roas']
        conversion_rates = BUSINESS_CONFIG['conversion_rates']

        logger.info(f"   Product Value: R$ {product_value:.2f} | Min ROAS: {min_roas}x")

        # 1. VERIFICAR SE J√Å EXISTEM PREDI√á√ïES
        total_leads = len(request.leads)
        logger.info(f"   Total de leads: {total_leads}")

        # Debug: mostrar estrutura do primeiro lead
        if total_leads > 0:
            first_lead = request.leads[0].data
            logger.info(f"   üîç DEBUG: Chaves do primeiro lead: {list(first_lead.keys())[:10]}...")
            logger.info(f"   üîç DEBUG: Tem 'lead_score'? {'lead_score' in first_lead}")
            logger.info(f"   üîç DEBUG: Tem 'decile'? {'decile' in first_lead}")

            # Verificar se leads j√° t√™m predi√ß√µes (apenas lead_score √© necess√°rio)
            has_predictions = 'lead_score' in first_lead
            logger.info(f"   üîç DEBUG: has_predictions = {has_predictions}")
        else:
            logger.error("   ‚ùå ERRO: Nenhum lead recebido!")
            raise HTTPException(status_code=400, detail="Nenhum lead recebido")

        if has_predictions:
            logger.info("‚úÖ Leads j√° possuem predi√ß√µes existentes, pulando etapa de predi√ß√£o...")

            # Construir DataFrame com predi√ß√µes existentes
            lead_rows = []
            for i, lead in enumerate(request.leads):
                row = lead.data.copy()
                row['_email'] = lead.email
                row['_row_id'] = lead.row_id or str(i)
                lead_rows.append(row)

            result_df = pd.DataFrame(lead_rows)
            result_df['email'] = result_df['_email']
            result_df['row_id'] = result_df['_row_id']
            result_df = result_df.drop(columns=['_email', '_row_id'], errors='ignore')

            # Renomear colunas se necess√°rio para padronizar
            if 'decile' in result_df.columns:
                result_df = result_df.rename(columns={'decile': 'decil'})

            logger.info(f"‚úÖ {len(result_df)} leads carregados com predi√ß√µes existentes")

        else:
            # 1. PREDI√á√ïES COM BATCHING INTERNO
            logger.info("üîÑ Executando predi√ß√µes...")

            # Processar em lotes se necess√°rio
            BATCH_SIZE = 500
            all_results = []

            if total_leads <= BATCH_SIZE:
                # Processar todos de uma vez
                logger.info("   Processando em lote √∫nico")
                lead_rows = []
                for i, lead in enumerate(request.leads):
                    row = lead.data.copy()
                    row['_email'] = lead.email
                    row['_row_id'] = lead.row_id or str(i)
                    lead_rows.append(row)

                df = pd.DataFrame(lead_rows)

                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
                    model_df = df.drop(columns=['_email', '_row_id'], errors='ignore')
                    model_df.to_csv(tmp, index=False)
                    temp_file = tmp.name

                result_df = pipeline.run(temp_file, with_predictions=True)

                if result_df is None or len(result_df) == 0:
                    raise HTTPException(status_code=500, detail="Pipeline retornou resultado vazio")

                result_df['email'] = df['_email'].values
                result_df['row_id'] = df['_row_id'].values

                all_results.append(result_df)

            else:
                # Processar em lotes
                num_batches = (total_leads + BATCH_SIZE - 1) // BATCH_SIZE
                logger.info(f"   Processando em {num_batches} lotes de ~{BATCH_SIZE} leads")

                for batch_idx in range(num_batches):
                    start_idx = batch_idx * BATCH_SIZE
                    end_idx = min(start_idx + BATCH_SIZE, total_leads)
                    batch_leads = request.leads[start_idx:end_idx]

                    logger.info(f"   Lote {batch_idx + 1}/{num_batches}: {len(batch_leads)} leads")

                    lead_rows = []
                    for i, lead in enumerate(batch_leads):
                        row = lead.data.copy()
                        row['_email'] = lead.email
                        row['_row_id'] = lead.row_id or str(start_idx + i)
                        lead_rows.append(row)

                    batch_df = pd.DataFrame(lead_rows)

                    batch_temp_file = None
                    try:
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
                            model_df = batch_df.drop(columns=['_email', '_row_id'], errors='ignore')
                            model_df.to_csv(tmp, index=False)
                            batch_temp_file = tmp.name

                        batch_result = pipeline.run(batch_temp_file, with_predictions=True)

                        if batch_result is None or len(batch_result) == 0:
                            logger.warning(f"   Lote {batch_idx + 1} retornou vazio")
                            continue

                        batch_result['email'] = batch_df['_email'].values
                        batch_result['row_id'] = batch_df['_row_id'].values

                        all_results.append(batch_result)

                    finally:
                        if batch_temp_file and os.path.exists(batch_temp_file):
                            os.remove(batch_temp_file)

            # Consolidar resultados
            if not all_results:
                raise HTTPException(status_code=500, detail="Nenhum resultado de predi√ß√£o obtido")

            result_df = pd.concat(all_results, ignore_index=True)
            logger.info(f"‚úÖ Predi√ß√µes conclu√≠das: {len(result_df)} leads consolidados")

        # 2. CALCULAR JANELAS TEMPORAIS (dias completos: 00:00-23:59)
        now = pd.Timestamp.now(tz=None)
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)

        # Usar at√© 00:00 de hoje (= fim de ontem 23:59:59)
        cutoff_end = today_start

        # Calcular in√≠cio para cada per√≠odo
        period_windows = {
            '1D': cutoff_end - pd.Timedelta(days=1),
            '3D': cutoff_end - pd.Timedelta(days=3),
            '7D': cutoff_end - pd.Timedelta(days=7)
        }

        logger.info(f"üìÖ Janelas temporais (dias completos):")
        for period, start_date in period_windows.items():
            logger.info(f"   {period}: {start_date.strftime('%Y-%m-%d %H:%M')} at√© {cutoff_end.strftime('%Y-%m-%d %H:%M')}")

        # 3. BUSCAR CUSTOS DA API META (HIERARQUIA COMPLETA POR PER√çODO)
        logger.info("üí∞ Buscando hierarquia de custos da API Meta...")
        meta_client = MetaAdsIntegration(
            access_token=META_CONFIG['access_token'],
            api_version=META_CONFIG['api_version']
        )

        # Buscar hierarquia completa para cada per√≠odo separadamente
        # IMPORTANTE: Meta API retorna dados AGREGADOS do per√≠odo solicitado
        # N√£o √© poss√≠vel buscar 7D e filtrar para 1D - os custos s√£o diferentes!
        hierarchy_by_period = {}
        for period_key, start_date in period_windows.items():
            # Converter timestamps para strings no formato YYYY-MM-DD
            since_str = start_date.strftime('%Y-%m-%d')
            until_str = cutoff_end.strftime('%Y-%m-%d')

            logger.info(f"üîç Buscando hierarquia Meta para {period_key} (de {since_str} at√© {until_str} exclusivo)...")

            hierarchy_by_period[period_key] = meta_client.get_costs_hierarchy(
                account_id=request.account_id,
                since_date=since_str,
                until_date=until_str
            )

        logger.info(f"‚úÖ Hierarquia obtida para {len(hierarchy_by_period)} per√≠odos")

        # Log detalhado de hierarquia por per√≠odo
        for period_key, hierarchy in hierarchy_by_period.items():
            total_campaigns = len(hierarchy['campaigns'])
            total_spend = sum(c['spend'] for c in hierarchy['campaigns'].values())
            total_adsets = sum(len(c['adsets']) for c in hierarchy['campaigns'].values())
            total_ads = sum(sum(len(a['ads']) for a in c['adsets'].values()) for c in hierarchy['campaigns'].values())
            logger.info(f"   {period_key}: {total_campaigns} campaigns, {total_adsets} adsets, {total_ads} ads | R$ {total_spend:.2f}")

        # 4. CALCULAR DECIS USANDO THRESHOLDS FIXOS DO MODELO
        logger.info("üìä Calculando decis usando thresholds fixos do modelo...")

        from src.model.decil_thresholds import atribuir_decis_batch

        # Carregar thresholds do modelo ativo
        thresholds = pipeline.predictor.metadata.get('decil_thresholds', {}).get('thresholds')
        threshold_name = pipeline.predictor.metadata.get('decil_thresholds', {}).get('name', 'unknown')

        if not thresholds:
            logger.error("‚ùå Thresholds n√£o encontrados no modelo!")
            raise HTTPException(status_code=500, detail="Thresholds do modelo n√£o configurados")

        logger.info(f"   ‚úÖ Thresholds carregados: {threshold_name}")

        # Garantir que lead_score √© num√©rico
        result_df['lead_score'] = pd.to_numeric(result_df['lead_score'], errors='coerce')

        # Remover linhas com lead_score inv√°lido
        result_df = result_df[result_df['lead_score'].notna()].copy()

        # Atribuir decis usando thresholds fixos
        result_df['decil'] = atribuir_decis_batch(
            result_df['lead_score'].values,
            thresholds
        )

        logger.info(f"‚úÖ Decis calculados para {len(result_df)} leads (thresholds fixos)")
        logger.info(f"   üìä Distribui√ß√£o: {result_df['decil'].value_counts().sort_index().to_dict()}")

        # 5. GERAR AN√ÅLISE UTM POR PER√çODO E DIMENS√ÉO
        logger.info("üìà Gerando an√°lise UTM...")

        periods_analysis = {}

        for period_key, hierarchy in hierarchy_by_period.items():
            logger.info(f"   Processando per√≠odo: {period_key}")

            # Extrair n√∫mero de dias do per√≠odo ('1D' ‚Üí 1, '3D' ‚Üí 3, etc.)
            if period_key == 'Total':
                # Para Total, calcular diferen√ßa real entre datas
                period_days = 30  # Default conservador
            else:
                try:
                    period_days = int(period_key.replace('D', ''))
                except:
                    period_days = 1  # Fallback

            # Usar janela pr√©-calculada
            cutoff_start = period_windows[period_key]

            # Filtrar do dataset (que j√° tem decis calculados via thresholds)
            if 'Data' in result_df.columns:
                dates_df = pd.to_datetime(result_df['Data'], errors='coerce')
                if dates_df.dt.tz is not None:
                    dates_df = dates_df.dt.tz_localize(None)

                # Filtrar: Data >= cutoff_start AND Data < cutoff_end
                valid_dates_mask = dates_df.notna()
                period_df = result_df[valid_dates_mask & (dates_df >= cutoff_start) & (dates_df < cutoff_end)].copy()
                logger.info(f"   Leads no per√≠odo {period_key}: {len(period_df)}")
            else:
                period_df = result_df.copy()
                logger.warning(f"   Coluna 'Data' n√£o encontrada, usando todos os leads")

            # Capturar metadados: timestamps reais dos leads dentro da janela
            if 'Data' in period_df.columns and len(period_df) > 0:
                period_dates = pd.to_datetime(period_df['Data'], errors='coerce')
                if period_dates.dt.tz is not None:
                    period_dates = period_dates.dt.tz_localize(None)

                # Filtrar apenas datas v√°lidas para min/max
                valid_period_dates = period_dates[period_dates.notna()]
                if len(valid_period_dates) > 0:
                    period_start = valid_period_dates.min().isoformat()
                    period_end = valid_period_dates.max().isoformat()
                else:
                    period_start = cutoff_start.isoformat()
                    period_end = cutoff_end.isoformat()
            else:
                period_start = cutoff_start.isoformat()
                period_end = cutoff_end.isoformat()

            # Contar leads por fonte
            total_leads = len(period_df)
            if 'Source' in period_df.columns:
                meta_leads = (period_df['Source'] == 'facebook-ads').sum()
                google_leads = (period_df['Source'] == 'google-ads').sum()
            else:
                meta_leads = total_leads
                google_leads = 0

            logger.info(f"   üìä Metadados: {period_start} at√© {period_end} | Total: {total_leads} | Meta: {meta_leads} | Google: {google_leads}")

            # Validar que temos leads e decis
            if len(period_df) == 0:
                logger.warning(f"   ‚ö†Ô∏è Nenhum lead no per√≠odo {period_key}")
                continue

            if 'decil' not in period_df.columns:
                logger.error(f"   ‚ùå ERRO: Coluna 'decil' n√£o encontrada ap√≥s filtro")
                # Fallback emergencial
                period_df['decil'] = 'D5'

            period_analysis = {}

            # Dimens√µes a analisar (incluindo google_ads como dimens√£o separada)
            # Removido 'term' - n√£o tem custo no Meta API, an√°lise in√∫til
            dimensions = ['campaign', 'medium', 'ad', 'google_ads']

            for dimension in dimensions:
                # Tratamento especial para Google Ads
                if dimension == 'google_ads':
                    # Filtrar apenas leads do Google Ads
                    if 'Source' in period_df.columns:
                        google_df = period_df[period_df['Source'] == 'google-ads']

                        if len(google_df) == 0:
                            logger.info(f"   ‚ÑπÔ∏è Nenhum lead Google Ads no per√≠odo {period_key}")
                            period_analysis[dimension] = []
                            continue

                        logger.info(f"   üìä {len(google_df)} leads Google Ads no per√≠odo {period_key}")

                        # Agrupar por Term (formato: "keyword--campaign_id--ad_id")
                        # Extrair apenas keyword (primeira parte)
                        def extract_keyword_from_term(term_value):
                            """Extrai keyword da primeira parte do Term"""
                            if pd.isna(term_value) or str(term_value).strip() == '':
                                return None
                            # Formato: "keyword--campaign_id--ad_id"
                            parts = str(term_value).split('--')
                            if len(parts) >= 1 and parts[0].strip() != '':
                                return parts[0].strip()  # Keyword
                            return None

                        google_df['keyword'] = google_df['Term'].apply(extract_keyword_from_term)

                        # Filtrar valores gen√©ricos e vazios
                        google_df_filtered = google_df[
                            google_df['keyword'].notna() &
                            ~google_df['keyword'].isin(['fb', 'ig', 'instagram', 'facebook'])
                        ].copy()

                        if len(google_df_filtered) == 0:
                            logger.info(f"   ‚ö†Ô∏è Nenhum keyword v√°lido encontrado para Google Ads")
                            period_analysis[dimension] = []
                            continue

                        grouped = google_df_filtered.groupby('keyword').agg({
                            'lead_score': 'count',
                            'decil': lambda x: (x == 'D10').sum() / len(x) * 100 if len(x) > 0 else 0,
                        }).rename(columns={'lead_score': 'leads', 'decil': 'pct_d10'})

                        # Calcular distribui√ß√£o de decis
                        for value in grouped.index:
                            value_df = google_df_filtered[google_df_filtered['keyword'] == value]
                            for i in range(1, 11):
                                decile_key = f'D{i}'
                                pct = (value_df['decil'] == decile_key).sum() / len(value_df) * 100 if len(value_df) > 0 else 0
                                grouped.at[value, f'%{decile_key}'] = pct

                        grouped = grouped.reset_index().rename(columns={'keyword': 'value'})

                        # Google Ads n√£o tem custos no Meta API
                        grouped['spend'] = 0.0

                        # Enriquecer com m√©tricas econ√¥micas (todas zeradas/n√£o aplic√°veis)
                        enriched = enrich_utm_with_economic_metrics(
                            utm_df=grouped,
                            product_value=product_value,
                            min_roas=min_roas,
                            conversion_rates=conversion_rates,
                            dimension=dimension,
                            period_days=period_days
                        )

                        enriched = enriched.fillna({
                            'leads': 0,
                            'spend': 0.0,
                            'cpl': 0.0,
                            'taxa_proj': 0.0,
                            'receita_proj': 0.0,
                            'margem_contrib': 0.0,
                            'roas_proj': 0.0,
                            'acao': 'N/A - Google Ads',
                            'budget_current': 0.0,
                            'budget_target': 0.0
                        })

                        period_analysis[dimension] = [
                            UTMDimensionMetrics(
                                campaign=None,
                                value=str(row['value']) if pd.notna(row['value']) else '(vazio)',
                                leads=int(row['leads']),
                                spend=float(row['spend']),
                                cpl=float(row['cpl']),
                                taxa_proj=float(row['taxa_proj']),
                                receita_proj=float(row['receita_proj']),
                                margem_contrib=float(row['margem_contrib']),
                                roas_proj=float(row['roas_proj']),
                                acao=str(row['acao']),
                                budget_current=float(row.get('budget_current', 0.0)),
                                budget_target=float(row.get('budget_target', 0.0))
                            )
                            for _, row in enriched.iterrows()
                        ]

                        logger.info(f"‚úÖ Google Ads analisado: {len(period_analysis[dimension])} grupos")
                        continue
                    else:
                        period_analysis[dimension] = []
                        continue

                # Processar dimens√µes Meta normalmente
                # Mapear para coluna do DataFrame
                utm_col_map = {
                    'campaign': 'Campaign',
                    'medium': 'Medium',
                    'term': 'Term',
                    'ad': 'Content'  # Ad = Criativo = Coluna Content
                }

                utm_col = utm_col_map.get(dimension, 'Campaign')

                # Agrupar por dimens√£o
                if utm_col not in period_df.columns:
                    logger.warning(f"‚ö†Ô∏è  Coluna '{utm_col}' n√£o encontrada, pulando dimens√£o '{dimension}'")
                    period_analysis[dimension] = []
                    continue

                # Filtrar UTMs vazios e inv√°lidos
                utm_mask = (
                    (period_df[utm_col].notna()) &
                    (period_df[utm_col] != '') &
                    (~period_df[utm_col].astype(str).str.contains('{{', regex=False))  # Placeholders
                )

                # Filtros espec√≠ficos por dimens√£o
                if dimension == 'campaign':
                    # Remover bare names (case insensitive)
                    for bare in BARE_CAMPAIGN_NAMES:
                        utm_mask &= ~(period_df[utm_col].astype(str).str.upper() == bare.upper())

                elif dimension == 'medium':
                    # Remover bare names
                    for bare in BARE_MEDIUM_NAMES:
                        utm_mask &= ~(period_df[utm_col].astype(str).str.upper() == bare.upper())

                elif dimension == 'term':
                    # Remover termos gen√©ricos (fb, ig, etc)
                    for generic in GENERIC_TERMS:
                        utm_mask &= ~(period_df[utm_col].astype(str).str.upper() == generic.upper())

                # Aplicar filtro
                period_df_filtered = period_df[utm_mask]

                # Log detalhado de valores filtrados
                filtered_count = len(period_df) - len(period_df_filtered)
                if filtered_count > 0:
                    filtered_df = period_df[~utm_mask]
                    filtered_values = filtered_df[utm_col].value_counts()

                    logger.info(f"   ‚ö†Ô∏è {filtered_count} leads filtrados de '{dimension}' ({period_key}):")
                    for val, count in filtered_values.head(5).items():
                        logger.info(f"      - '{val}': {count} leads (bare name/gen√©rico)")

                if len(period_df_filtered) == 0:
                    logger.warning(f"‚ö†Ô∏è Nenhum lead com '{utm_col}' v√°lido no per√≠odo, pulando dimens√£o '{dimension}'")
                    period_analysis[dimension] = []
                    continue

                # CORRE√á√ÉO: Para Campaign, agrupar por Campaign ID (n√£o por UTM completo)
                if dimension == 'campaign':
                    from api.meta_integration import extract_id_from_utm

                    # Extrair Campaign ID de cada UTM
                    period_df_filtered = period_df_filtered.copy()
                    period_df_filtered['campaign_id'] = period_df_filtered[utm_col].apply(extract_id_from_utm)

                    # Remover linhas sem Campaign ID
                    period_df_filtered = period_df_filtered[period_df_filtered['campaign_id'].notna()]

                    # Agrupar por Campaign ID
                    grouped = period_df_filtered.groupby('campaign_id').agg({
                        'lead_score': 'count',
                        'decil': lambda x: (x == 'D10').sum() / len(x) * 100 if len(x) > 0 else 0,
                    }).rename(columns={'lead_score': 'leads', 'decil': 'pct_d10'})

                    # Calcular distribui√ß√£o de decis
                    for campaign_id in grouped.index:
                        value_df = period_df_filtered[period_df_filtered['campaign_id'] == campaign_id]
                        for i in range(1, 11):
                            decile_key = f'D{i}'
                            pct = (value_df['decil'] == decile_key).sum() / len(value_df) * 100 if len(value_df) > 0 else 0
                            grouped.at[campaign_id, f'%{decile_key}'] = pct

                    # Resetar index e renomear para 'value' (ser√° o Campaign ID)
                    grouped = grouped.reset_index().rename(columns={'campaign_id': 'value'})

                elif dimension == 'medium':
                    # Para adsets, agrupar por (campaign, adset) para separar mesmo nome em campanhas diferentes
                    # Extrair campaign_id da coluna Campaign
                    period_df_filtered = period_df_filtered.copy()

                    # Verificar se temos coluna Campaign (utm_campaign)
                    if 'Campaign' not in period_df_filtered.columns:
                        logger.warning(f"‚ö†Ô∏è  Coluna 'Campaign' n√£o encontrada para matchear adsets. Usando apenas nome do adset.")
                        # Fallback: agrupar s√≥ por nome do adset (comportamento antigo)
                        grouped = period_df_filtered.groupby(utm_col).agg({
                            'lead_score': 'count',
                            'decil': lambda x: (x == 'D10').sum() / len(x) * 100 if len(x) > 0 else 0,
                        }).rename(columns={'lead_score': 'leads', 'decil': 'pct_d10'})

                        for value in grouped.index:
                            value_df = period_df_filtered[period_df_filtered[utm_col] == value]
                            for i in range(1, 11):
                                decile_key = f'D{i}'
                                pct = (value_df['decil'] == decile_key).sum() / len(value_df) * 100 if len(value_df) > 0 else 0
                                grouped.at[value, f'%{decile_key}'] = pct

                        grouped = grouped.reset_index().rename(columns={utm_col: 'value'})
                        grouped['campaign_name'] = None  # Sem campaign info
                    else:
                        period_df_filtered['campaign_id'] = period_df_filtered['Campaign'].apply(extract_id_from_utm)

                        # Criar mapeamento (campaign_id, adset_name) ‚Üí adset_id da hierarquia
                        campaign_adset_to_info = {}
                        for campaign_id, campaign_data in hierarchy['campaigns'].items():
                            for adset_id, adset_data in campaign_data['adsets'].items():
                                key = (campaign_id, adset_data['name'])
                                campaign_adset_to_info[key] = {
                                    'adset_id': adset_id,
                                    'campaign_name': campaign_data['name']
                                }

                        # Matchear cada lead para adset_id espec√≠fico
                        def match_adset(row):
                            campaign_id = row['campaign_id']
                            adset_name = row[utm_col]
                            if pd.isna(campaign_id) or pd.isna(adset_name):
                                return None, None
                            key = (str(campaign_id), str(adset_name))
                            info = campaign_adset_to_info.get(key)
                            if info:
                                return info['adset_id'], info['campaign_name']
                            return None, None

                        period_df_filtered[['adset_id', 'campaign_name']] = period_df_filtered.apply(
                            match_adset, axis=1, result_type='expand'
                        )

                        # Remover linhas sem match
                        before_match = len(period_df_filtered)
                        period_df_filtered = period_df_filtered[period_df_filtered['adset_id'].notna()]
                        after_match = len(period_df_filtered)
                        if before_match > after_match:
                            logger.info(f"   ‚ö†Ô∏è  {before_match - after_match} leads sem match campaign+adset (removidos)")

                        # Agrupar por adset_id (j√° √© √∫nico por campanha)
                        grouped = period_df_filtered.groupby('adset_id').agg({
                            'lead_score': 'count',
                            'campaign_name': 'first',  # Pegar o nome da campanha
                            'decil': lambda x: (x == 'D10').sum() / len(x) * 100 if len(x) > 0 else 0,
                        }).rename(columns={'lead_score': 'leads', 'decil': 'pct_d10'})

                        # Calcular distribui√ß√£o de decis
                        for adset_id in grouped.index:
                            value_df = period_df_filtered[period_df_filtered['adset_id'] == adset_id]
                            for i in range(1, 11):
                                decile_key = f'D{i}'
                                pct = (value_df['decil'] == decile_key).sum() / len(value_df) * 100 if len(value_df) > 0 else 0
                                grouped.at[adset_id, f'%{decile_key}'] = pct

                        # Resetar index e renomear
                        grouped = grouped.reset_index().rename(columns={'adset_id': 'value'})

                elif dimension == 'ad':
                    # Para ads, agrupar por nome e extrair campaign/adset das colunas Campaign/Medium
                    period_df_filtered = period_df_filtered.copy()

                    # Extrair campaign e adset names das colunas UTM
                    if 'Campaign' in period_df_filtered.columns and 'Medium' in period_df_filtered.columns:
                        # Campaign: extrair ID (formato "nome|ID")
                        period_df_filtered['campaign_id'] = period_df_filtered['Campaign'].apply(extract_id_from_utm)

                        # Medium: j√° cont√©m NOME do adset (n√£o ID), usar diretamente
                        period_df_filtered['adset_name_from_utm'] = period_df_filtered['Medium']

                        # Mapear campaign_id para nome
                        campaign_id_to_name = {
                            campaign_id: campaign_data['name']
                            for campaign_id, campaign_data in hierarchy['campaigns'].items()
                        }

                        def get_campaign_name(row):
                            """Obt√©m nome da campanha por ID, com fallback para nome do UTM"""
                            campaign_id = row['campaign_id']
                            if pd.notna(campaign_id):
                                # Tentar buscar na hierarquia primeiro
                                name = campaign_id_to_name.get(str(campaign_id))
                                if name:
                                    return name
                            # Fallback: extrair nome do UTM Campaign (remover data e ID)
                            campaign_utm = row.get('Campaign')
                            if pd.notna(campaign_utm):
                                import re
                                # Remover campaign ID do final (|n√∫meros)
                                clean = re.sub(r'\|\d{18}$', '', str(campaign_utm))
                                # Remover data do final (| YYYY-MM-DD)
                                clean = re.sub(r'\|\s*\d{4}-\d{2}-\d{2}$', '', clean)
                                return clean.strip()
                            return None

                        period_df_filtered['campaign_name'] = period_df_filtered.apply(get_campaign_name, axis=1)

                        # adset_name inicial vem da coluna Medium
                        period_df_filtered['adset_name'] = period_df_filtered['adset_name_from_utm']

                        # CORRE√á√ÉO: Detectar e corrigir UTMs gen√©ricas usando hierarquia Meta
                        GENERIC_UTMS = {'paid', 'dgen', 'facebook', 'instagram', 'meta', 'fb', 'ig', 'cpc'}

                        def correct_generic_adset(row):
                            """Corrige adset_name gen√©rico buscando na hierarquia Meta por nome do ad"""
                            try:
                                adset_name = row['adset_name'] if 'adset_name' in row else None
                                ad_name = row[utm_col] if utm_col in row else None

                                # Se adset_name n√£o √© gen√©rico ou est√° vazio, retornar como est√°
                                if pd.isna(adset_name) or str(adset_name).lower() not in GENERIC_UTMS:
                                    return adset_name

                                # Se ad_name est√° vazio, n√£o podemos corrigir
                                if pd.isna(ad_name) or str(ad_name).strip() == '':
                                    logger.debug(f"      ‚ö†Ô∏è Ad sem nome: adset_name='{adset_name}' (ser√° filtrado)")
                                    return None  # Ser√° filtrado

                                # Buscar ad_name na hierarquia para encontrar adset correto
                                ad_name_lower = str(ad_name).lower()
                                for campaign_id, campaign_data in hierarchy['campaigns'].items():
                                    if not isinstance(campaign_data, dict) or 'adsets' not in campaign_data:
                                        continue
                                    for adset_id, adset_data in campaign_data['adsets'].items():
                                        if not isinstance(adset_data, dict) or 'ads' not in adset_data:
                                            continue
                                        for ad_id, ad_data in adset_data['ads'].items():
                                            if not isinstance(ad_data, dict) or 'name' not in ad_data:
                                                continue
                                            # Match por nome (case insensitive)
                                            if ad_data['name'].lower() == ad_name_lower:
                                                logger.info(f"      ‚úì Corrigido '{adset_name}' ‚Üí '{adset_data.get('name', 'Unknown')}' (ad: {str(ad_name)[:40]}...)")
                                                return adset_data.get('name')

                                # N√£o encontrou na hierarquia, filtrar
                                logger.info(f"      üóëÔ∏è UTM gen√©rica sem match: adset='{adset_name}', ad='{str(ad_name)[:40]}...'")
                                return None
                            except Exception as e:
                                logger.error(f"      ‚ùå Erro em correct_generic_adset: {str(e)}")
                                return row.get('adset_name') if hasattr(row, 'get') else None

                        # Aplicar corre√ß√£o
                        logger.info(f"   üîç Verificando UTMs gen√©ricas em {len(period_df_filtered)} ads...")
                        period_df_filtered['adset_name'] = period_df_filtered.apply(correct_generic_adset, axis=1)

                        # Filtrar ads com adset_name None (gen√©ricos sem match)
                        before_filter = len(period_df_filtered)
                        period_df_filtered = period_df_filtered[period_df_filtered['adset_name'].notna()].copy()
                        after_filter = len(period_df_filtered)
                        if before_filter > after_filter:
                            removed = before_filter - after_filter
                            logger.info(f"   üóëÔ∏è Removidos {removed} ads com UTMs gen√©ricas (sem match na hierarquia)")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Colunas Campaign ou Medium n√£o encontradas para ads")
                        period_df_filtered['campaign_name'] = None
                        period_df_filtered['adset_name'] = None

                    # Agrupar por nome do ad (coluna Content)
                    grouped = period_df_filtered.groupby(utm_col).agg({
                        'lead_score': 'count',
                        'campaign_name': 'first',  # Pegar primeira ocorr√™ncia
                        'adset_name': 'first',
                        'decil': lambda x: (x == 'D10').sum() / len(x) * 100 if len(x) > 0 else 0,
                    }).rename(columns={'lead_score': 'leads', 'decil': 'pct_d10'})

                    # Calcular distribui√ß√£o de decis
                    for value in grouped.index:
                        value_df = period_df_filtered[period_df_filtered[utm_col] == value]
                        for i in range(1, 11):
                            decile_key = f'D{i}'
                            pct = (value_df['decil'] == decile_key).sum() / len(value_df) * 100 if len(value_df) > 0 else 0
                            grouped.at[value, f'%{decile_key}'] = pct

                    # Resetar index e renomear
                    grouped = grouped.reset_index().rename(columns={utm_col: 'value'})

                else:
                    # Para outras dimens√µes, agrupar normalmente pelo UTM
                    grouped = period_df_filtered.groupby(utm_col).agg({
                        'lead_score': 'count',  # N√∫mero de leads
                        'decil': lambda x: (x == 'D10').sum() / len(x) * 100 if len(x) > 0 else 0,  # %D10
                    }).rename(columns={'lead_score': 'leads', 'decil': 'pct_d10'})

                    # Calcular distribui√ß√£o de decis para cada valor
                    for value in grouped.index:
                        value_df = period_df_filtered[period_df_filtered[utm_col] == value]
                        for i in range(1, 11):
                            decile_key = f'D{i}'
                            pct = (value_df['decil'] == decile_key).sum() / len(value_df) * 100 if len(value_df) > 0 else 0
                            grouped.at[value, f'%{decile_key}'] = pct

                    grouped = grouped.reset_index().rename(columns={utm_col: 'value'})

                # Adicionar custos usando hierarquia (evita duplica√ß√£o)
                grouped = enrich_utm_with_hierarchy(
                    utm_analysis_df=grouped,
                    hierarchy=hierarchy,
                    dimension=dimension
                )

                # FILTRO: Para Campaign, remover linhas com spend=0 (indicam Campaign IDs inv√°lidos)
                if dimension == 'campaign':
                    before_filter = len(grouped)
                    grouped = grouped[grouped['spend'] > 0].copy()
                    after_filter = len(grouped)
                    if before_filter > after_filter:
                        removed = before_filter - after_filter
                        logger.info(f"   üóëÔ∏è Removidas {removed} campanhas com spend=0 (IDs inv√°lidos)")

                    # Substituir Campaign ID pelo nome leg√≠vel
                    id_to_name = {
                        campaign_id: campaign_data['name']
                        for campaign_id, campaign_data in hierarchy['campaigns'].items()
                    }

                    grouped['value'] = grouped['value'].apply(
                        lambda x: id_to_name.get(str(x), str(x))
                    )
                    logger.info(f"   ‚úèÔ∏è IDs substitu√≠dos por nomes de campanha para exibi√ß√£o")

                elif dimension == 'medium':
                    # Filtrar adsets com spend=0
                    before_filter = len(grouped)
                    grouped = grouped[grouped['spend'] > 0].copy()
                    after_filter = len(grouped)
                    if before_filter > after_filter:
                        removed = before_filter - after_filter
                        logger.info(f"   üóëÔ∏è Removidos {removed} adsets com spend=0 (IDs inv√°lidos)")

                    # Substituir Adset ID pelo nome leg√≠vel
                    id_to_info = {}
                    for campaign_id, campaign_data in hierarchy['campaigns'].items():
                        for adset_id, adset_data in campaign_data['adsets'].items():
                            id_to_info[adset_id] = adset_data['name']

                    grouped['value'] = grouped['value'].apply(
                        lambda x: id_to_info.get(str(x), str(x))
                    )
                    logger.info(f"   ‚úèÔ∏è IDs substitu√≠dos por nomes de adset para exibi√ß√£o")

                elif dimension == 'ad':
                    # Filtrar ads com spend=0
                    before_filter = len(grouped)
                    grouped = grouped[grouped['spend'] > 0].copy()
                    after_filter = len(grouped)
                    if before_filter > after_filter:
                        removed = before_filter - after_filter
                        logger.info(f"   üóëÔ∏è Removidos {removed} ads com spend=0")

                    # Para ads, 'value' j√° √© o nome (n√£o ID), ent√£o n√£o precisa substituir
                    logger.info(f"   ‚úÖ {len(grouped)} ads com custo > 0")

                # Enriquecer com m√©tricas econ√¥micas
                # Para campaigns e adsets (medium), usar budget info para condicionar a√ß√£o
                if dimension == 'campaign':
                    budget_control_col = 'has_campaign_budget'
                elif dimension == 'medium':
                    budget_control_col = 'has_adset_budget'
                else:
                    budget_control_col = None

                enriched = enrich_utm_with_economic_metrics(
                    utm_df=grouped,
                    product_value=product_value,
                    min_roas=min_roas,
                    conversion_rates=conversion_rates,
                    dimension=dimension,
                    budget_control_col=budget_control_col,
                    period_days=period_days
                )

                # Tratar NaN em m√©tricas calculadas (podem surgir de divis√µes por zero)
                enriched = enriched.fillna({
                    'leads': 0,
                    'spend': 0.0,
                    'cpl': 0.0,
                    'taxa_proj': 0.0,
                    'receita_proj': 0.0,
                    'margem_contrib': 0.0,
                    'roas_proj': 0.0,
                    'acao': ''
                })

                # Converter para lista de dicts
                metrics_list = []
                for _, row in enriched.iterrows():
                    # Para adsets, incluir campaign_name
                    # Para ads, incluir campaign_name e adset_name
                    campaign_value = None
                    adset_value = None

                    if dimension == 'medium':
                        campaign_value = row.get('campaign_name')
                    elif dimension == 'ad':
                        campaign_value = row.get('campaign_name')
                        adset_value = row.get('adset_name')

                    metrics_list.append(UTMDimensionMetrics(
                        campaign=campaign_value,
                        adset=adset_value,
                        value=str(row['value']) if pd.notna(row['value']) else '(vazio)',
                        leads=int(row['leads']),
                        spend=float(row['spend']),
                        cpl=float(row['cpl']),
                        taxa_proj=float(row['taxa_proj']),
                        receita_proj=float(row['receita_proj']),
                        margem_contrib=float(row['margem_contrib']),
                        roas_proj=float(row['roas_proj']),
                        acao=row['acao'],
                        budget_current=float(row.get('budget_current', 0.0)),
                        budget_target=float(row.get('budget_target', 0.0))
                    ))

                period_analysis[dimension] = metrics_list

            # Adicionar metadados do per√≠odo
            period_analysis['period_start'] = period_start
            period_analysis['period_end'] = period_end
            period_analysis['total_leads'] = total_leads
            period_analysis['meta_leads'] = meta_leads
            period_analysis['google_leads'] = google_leads

            periods_analysis[period_key] = UTMPeriodAnalysis(**period_analysis)

        processing_time = time.time() - start_time

        logger.info(f"‚úÖ An√°lise conclu√≠da em {processing_time:.2f}s")

        return UTMAnalysisResponse(
            request_id=request_id,
            periods=periods_analysis,
            config={
                'product_value': product_value,
                'min_roas': min_roas
            },
            processing_time_seconds=round(processing_time, 2),
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        import traceback
        logger.error(f"‚ùå Erro na an√°lise UTM: {str(e)}")
        logger.error(f"Traceback completo:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Erro na an√°lise: {str(e)}")
    finally:
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

# =============================================================================
# BIGQUERY SYNC ENDPOINTS
# =============================================================================

from api.bigquery_sync import sync_postgres_to_bigquery, get_bigquery_stats

@app.post("/bigquery/sync")
async def bigquery_sync(limit: int = 1000):
    """
    Sincroniza dados do PostgreSQL para BigQuery

    Args:
        limit: N√∫mero m√°ximo de registros a sincronizar (default: 1000 √∫ltimos)

    Returns:
        Status e estat√≠sticas do sync
    """
    try:
        result = sync_postgres_to_bigquery(limit=limit)
        return result
    except Exception as e:
        logger.error(f"‚ùå Erro no sync com BigQuery: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bigquery/stats")
async def bigquery_stats():
    """
    Estat√≠sticas da tabela leads_capi no BigQuery

    Returns:
        Estat√≠sticas da tabela (total de registros, fbp/fbc, √∫ltima atualiza√ß√£o)
    """
    try:
        result = get_bigquery_stats()
        return result
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar stats do BigQuery: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/migrate_capi_sent_at")
async def migrate_capi_sent_at(db: Session = Depends(get_db)):
    """Endpoint tempor√°rio para executar migra√ß√£o da coluna capi_sent_at"""
    try:
        from sqlalchemy import text

        logger.info("üìù Adicionando coluna capi_sent_at...")
        db.execute(text("""
            ALTER TABLE leads_capi
            ADD COLUMN IF NOT EXISTS capi_sent_at TIMESTAMP NULL
        """))
        db.commit()
        logger.info("‚úÖ Coluna adicionada!")

        logger.info("üìù Criando √≠ndice idx_capi_sent_at...")
        db.execute(text("""
            CREATE INDEX IF NOT EXISTS idx_capi_sent_at
            ON leads_capi(capi_sent_at)
        """))
        db.commit()
        logger.info("‚úÖ √çndice criado!")

        logger.info("üîç Verificando estrutura...")
        result = db.execute(text("""
            SELECT column_name, data_type, is_nullable
            FROM information_schema.columns
            WHERE table_name = 'leads_capi' AND column_name = 'capi_sent_at'
        """))

        row = result.fetchone()
        if row:
            return {
                "status": "success",
                "message": "Migra√ß√£o executada com sucesso",
                "column": {
                    "name": row[0],
                    "type": row[1],
                    "nullable": row[2]
                }
            }
        else:
            return {
                "status": "error",
                "message": "Coluna n√£o encontrada ap√≥s migra√ß√£o"
            }

    except Exception as e:
        logger.error(f"‚ùå Erro na migra√ß√£o: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Erro na migra√ß√£o: {str(e)}")


@app.post("/admin/cleanup_duplicates")
async def cleanup_duplicates(ids_to_delete: List[int], db: Session = Depends(get_db)):
    """
    Deleta registros duplicados da p√°gina Parab√©ns

    SEGURAN√áA:
    - Requer lista expl√≠cita de IDs
    - Executa em transa√ß√£o (rollback autom√°tico em caso de erro)
    - Retorna estat√≠sticas da dele√ß√£o
    """
    try:
        from sqlalchemy import text

        if not ids_to_delete:
            raise HTTPException(status_code=400, detail="Lista de IDs vazia")

        if len(ids_to_delete) > 5000:
            raise HTTPException(status_code=400, detail="Limite de 5000 IDs por execu√ß√£o")

        logger.info(f"üóëÔ∏è Iniciando dele√ß√£o de {len(ids_to_delete)} registros duplicados...")

        # Executar dele√ß√£o em batches de 100
        batch_size = 100
        total_deleted = 0

        for i in range(0, len(ids_to_delete), batch_size):
            batch = ids_to_delete[i:i+batch_size]
            ids_str = ','.join(map(str, batch))

            result = db.execute(text(f"DELETE FROM leads_capi WHERE id IN ({ids_str})"))
            deleted = result.rowcount
            total_deleted += deleted
            logger.info(f"‚úÖ Batch {i//batch_size + 1}: {deleted} registros deletados")

        db.commit()
        logger.info(f"‚úÖ SUCESSO! Total deletado: {total_deleted} registros")

        return {
            "status": "success",
            "total_deleted": total_deleted,
            "expected": len(ids_to_delete),
            "message": f"Deletados {total_deleted} registros duplicados"
        }

    except Exception as e:
        logger.error(f"‚ùå Erro na dele√ß√£o: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Erro na dele√ß√£o: {str(e)}")

if __name__ == "__main__":
    import uvicorn

    # Inicializar pipeline antes de iniciar o servidor
    print("Inicializando pipeline...")
    if initialize_pipeline():
        print("Pipeline inicializado com sucesso!")
    else:
        print("AVISO: Pipeline n√£o foi inicializado. Ser√° inicializado na primeira requisi√ß√£o.")

    # Iniciar o servidor
    print("Iniciando servidor na porta 8080...")
    uvicorn.run(app, host="0.0.0.0", port=8080, reload=False)