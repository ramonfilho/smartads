"""
Pipeline de treino - Reproduz notebook DevClub c√©lula por c√©lula.

Integra fun√ß√µes modularizadas conforme s√£o aprovadas.
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import yaml
import glob
import logging
import argparse
import pandas as pd
from src.data_processing.ingestion import (
    read_excel_files,
    filter_sheets,
    remove_duplicates_per_sheet,
    remove_unnecessary_columns,
    consolidate_datasets
)
from src.data_processing.column_unification import unificar_colunas_datasets
from src.data_processing.category_unification import unificar_categorias_completo, gerar_relatorio_final_categorias
from src.data_processing.feature_removal import remover_features_desnecessarias, listar_colunas_restantes
from src.data_processing.utm_training import unificar_utm_source_term, verificar_consistencia_utm
from src.data_processing.medium_training import extrair_publico_medium, relatorio_final_medium
from src.data_processing.medium_production_training import unificar_medium_para_producao, relatorio_unificacao_producao
from src.data_processing.dataset_versioning_training import criar_dataset_pos_cutoff, disponibilizar_dataset
from src.matching.matching_training import fazer_matching_robusto as fazer_matching_variantes
from src.matching.matching_robusto import fazer_matching_robusto
from src.matching.matching_email_only import fazer_matching_email_only
from src.matching.matching_email_with_validation import fazer_matching_email_with_validation
from src.matching.matching_email_telefone import fazer_matching_email_telefone
from src.data_processing.devclub_filtering_training import criar_dataset_devclub
from src.data_processing.conversion_window import aplicar_janela_conversao
from src.features.feature_engineering_training import criar_features_derivadas
from src.features.encoding_training import aplicar_encoding_estrategico
from src.model.training_model import registrar_features_e_modelo_devclub
from src.model.hyperparameter_tuning import hyperparameter_tuning

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


def main(initial_matching='email_telefone', save_files=False, tune_hyperparams=False, grid_size='small', split_method='temporal', use_guru_only=None):
    """Executa pipeline de treino completo.

    Args:
        initial_matching: M√©todo de matching inicial na c√©lula 15
                         ('email_only', 'email_telefone', 'variantes', 'robusto' ou 'validation')
        split_method: M√©todo de split ('temporal' para 70% dos dias, 'stratified' para 70% dos registros)
        save_files: Se True, salva arquivos locais em files/{timestamp}
        tune_hyperparams: Se True, executa hyperparameter tuning antes do treino
        grid_size: Tamanho do grid search ('small', 'medium', 'large')
        use_guru_only: Se True, usa apenas GURU. Se False, usa GURU+TMB. Se None, usa valor do config.
    """

    print("\n" + "=" * 80)
    print("PIPELINE DE TREINO")
    print("=" * 80)
    print(f"\nüîß CONFIGURA√á√ÉO:")
    print(f"   M√©todo de matching inicial (c√©lula 15): {initial_matching}")
    print(f"   Salvar arquivos locais: {save_files}")
    print(f"   Hyperparameter tuning: {tune_hyperparams}")
    if tune_hyperparams:
        print(f"   Grid size: {grid_size}")
    print("=" * 80)

    # Carregar configura√ß√£o
    config_path = os.path.join(os.path.dirname(__file__), '../configs/devclub.yaml')
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # === C√âLULA 1: Upload/Leitura de arquivos ===
    print("\nüì§ C√âLULA 1: LEITURA DE ARQUIVOS")
    data_dir = config['ingestion']['training_data_dir']

    # Custom sorting para replicar ordem do notebook
    # No notebook, arquivos foram carregados via upload do Colab que preserva
    # a ordem do file picker (macOS/Linux), onde "[" vem antes de letras
    def notebook_sort_key(filepath):
        """Ordena arquivos para replicar a ordem do notebook."""
        basename = os.path.basename(filepath).lower()
        # Converter '[' para um caractere que vem antes de letras na ordena√ß√£o
        # Usar '!' que tem ASCII 33, bem antes de letras
        return basename.replace('[', '!')

    filepaths = sorted(glob.glob(os.path.join(data_dir, "*.xlsx")), key=notebook_sort_key)

    # Aplicar filtro GURU only
    # Se passado via argumento, usa argumento. Caso contr√°rio, usa config.
    if use_guru_only is None:
        use_guru_only = config['ingestion'].get('use_guru_only', False)
    if use_guru_only:
        filepaths_original = filepaths.copy()
        filepaths = [f for f in filepaths if 'guru' in os.path.basename(f).lower() or 'pesquisa' in os.path.basename(f).lower() or 'lead' in os.path.basename(f).lower()]

        # Arquivos removidos (TMB)
        removed = [f for f in filepaths_original if f not in filepaths]
        if removed:
            print(f"\nüö´ GURU ONLY MODE - Arquivos TMB exclu√≠dos:")
            for f in removed:
                print(f"  - {os.path.basename(f)}")

    print(f"\nTotal de arquivos: {len(filepaths)}")
    for f in filepaths:
        print(f"  - {os.path.basename(f)}")

    # Ler arquivos
    all_data = read_excel_files(filepaths)

    # === C√âLULA 2: Filtragem + Remo√ß√£o de Duplicatas ===
    print("\nüîÑ C√âLULA 2: FILTRAGEM DE ABAS + REMO√á√ÉO DE DUPLICATAS")
    print("=" * 60)

    # Filtrar abas
    filtered_data, filter_report = filter_sheets(
        all_data,
        termos_manter=config['ingestion']['termos_manter'],
        termos_remover=config['ingestion']['termos_remover'],
        min_linhas=config['ingestion']['min_linhas']
    )

    # Remover duplicatas
    clean_data, dup_stats = remove_duplicates_per_sheet(filtered_data)

    # === RELAT√ìRIO (linhas 96-127 do notebook) ===
    print(f"\nüìä ABAS MANTIDAS E PROCESSADAS")
    print("=" * 80)
    print(f"{'ARQUIVO':<35} {'ABA':<20} {'ORIGINAL':>10} {'FINAL':>10} {'REMOVIDAS':>10}")
    print("-" * 80)

    total_original = 0
    total_final = 0
    total_duplicatas = 0

    for item in filter_report:
        if item['status'] == 'MANTIDA':
            filename = item['arquivo']
            sheet_name = item['aba']
            linhas_original = item['linhas_original']

            # Pegar estat√≠sticas de duplicatas
            duplicatas = dup_stats.get(filename, {}).get(sheet_name, 0)
            linhas_final = linhas_original - duplicatas

            print(f"{filename[:34]:<35} {sheet_name[:19]:<20} "
                  f"{linhas_original:>10,} {linhas_final:>10,} {duplicatas:>10,}")

            total_original += linhas_original
            total_final += linhas_final
            total_duplicatas += duplicatas

    print("-" * 80)
    print(f"{'TOTAL':<35} {'':<20} {total_original:>10,} {total_final:>10,} {total_duplicatas:>10,}")

    # Resumo final
    abas_mantidas = sum(1 for item in filter_report if item['status'] == 'MANTIDA')
    abas_removidas = len(filter_report) - abas_mantidas

    print(f"\nüìà RESUMO FINAL:")
    print(f"Arquivos processados: {len(clean_data)}")
    print(f"Abas mantidas: {abas_mantidas}")
    print(f"Abas removidas: {abas_removidas}")
    print(f"Linhas totais ap√≥s processamento: {total_final:,}")
    print(f"Duplicatas removidas: {total_duplicatas:,}")
    if total_original > 0:
        print(f"Redu√ß√£o por duplicatas: {(total_duplicatas/total_original*100):.2f}%")

    print(f"\n‚úÖ Dados processados dispon√≠veis na vari√°vel 'arquivos_filtrados'")
    print("=" * 80)

    # === C√âLULA 3: Remo√ß√£o de colunas desnecess√°rias ===
    print("\nüßπ C√âLULA 3: REMO√á√ÉO DE COLUNAS DESNECESS√ÅRIAS")
    print("=" * 60)

    clean_data_cols, cols_report = remove_unnecessary_columns(
        clean_data,
        colunas_remover=config['cleaning']['colunas_remover']
    )

    print(f"\nüìä COLUNAS REMOVIDAS POR ABA")
    print("=" * 80)
    print(f"{'ARQUIVO':<35} {'ABA':<20} {'ANTES':>10} {'DEPOIS':>10} {'REMOVIDAS':>10}")
    print("-" * 80)

    total_antes = 0
    total_depois = 0
    total_removidas_cols = 0

    for item in cols_report:
        print(f"{item['arquivo'][:34]:<35} {item['aba'][:19]:<20} "
              f"{item['colunas_antes']:>10} {item['colunas_depois']:>10} {item['removidas']:>10}")
        total_antes += item['colunas_antes']
        total_depois += item['colunas_depois']
        total_removidas_cols += item['removidas']

    print("-" * 80)
    print(f"{'TOTAL':<35} {'':<20} {total_antes:>10} {total_depois:>10} {total_removidas_cols:>10}")

    print(f"\nüìà RESUMO:")
    print(f"Total de colunas removidas: {total_removidas_cols}")
    print(f"\n‚úÖ Dados sem colunas desnecess√°rias dispon√≠veis")
    print("=" * 80)

    # === C√âLULA 4: Consolida√ß√£o de datasets ===
    print("\nCONSOLIDA√á√ÉO DE DATASETS - PESQUISA E VENDAS")
    print("=" * 45)

    df_pesquisa, df_vendas = consolidate_datasets(
        clean_data_cols,
        pesquisa_keywords=config['consolidation']['pesquisa_keywords'],
        vendas_keywords=config['consolidation']['vendas_keywords']
    )

    # Fun√ß√£o para gerar relat√≥rio de colunas (igual ao notebook)
    def gerar_relatorio_colunas(df, nome_dataset):
        """Gera relat√≥rio detalhado das colunas de um dataset"""

        print(f"\n{nome_dataset.upper()} - {len(df)} registros")
        print("=" * 70)
        print(f"{'COLUNA':<35} {'√öNICOS':>10} {'% AUSENTES':>12} {'TOTAL':>10}")
        print("-" * 70)

        for col in df.columns:
            valores_unicos = df[col].nunique()
            valores_ausentes = df[col].isnull().sum()
            pct_ausentes = (valores_ausentes / len(df)) * 100 if len(df) > 0 else 0
            total_registros = len(df)

            print(f"{col[:34]:<35} {valores_unicos:>10,} {pct_ausentes:>11.1f}% {total_registros:>10,}")

    # Gerar relat√≥rios
    gerar_relatorio_colunas(df_pesquisa, "DATASET PESQUISA")
    gerar_relatorio_colunas(df_vendas, "DATASET VENDAS")

    print(f"\nRESUMO:")
    print(f"Dataset Pesquisa: {len(df_pesquisa):,} registros, {len(df_pesquisa.columns)} colunas")
    print(f"Dataset Vendas: {len(df_vendas):,} registros, {len(df_vendas.columns)} colunas")

    print(f"\nDatasets consolidados dispon√≠veis nas vari√°veis:")
    print(f"- dataset_pesquisa_final")
    print(f"- dataset_vendas_final")

    # === C√âLULA 5: Unifica√ß√£o de colunas duplicadas ===
    print("\nUNIFICA√á√ÉO DE COLUNAS DUPLICADAS")
    print("=" * 32)

    df_pesquisa_final, df_vendas_final = unificar_colunas_datasets(df_pesquisa, df_vendas)

    print(f"\nRESULTADO:")
    print(f"Pesquisa: {len(df_pesquisa_final)} registros, {len(df_pesquisa_final.columns)} colunas")
    print(f"Vendas: {len(df_vendas_final)} registros, {len(df_vendas_final.columns)} colunas")

    # Gerar relat√≥rios finais
    gerar_relatorio_colunas(df_pesquisa_final, "DATASET PESQUISA")
    gerar_relatorio_colunas(df_vendas_final, "DATASET VENDAS")

    # === C√âLULA 7: Unifica√ß√£o completa de categorias ===
    print("\nUNIFICA√á√ÉO COMPLETA DE CATEGORIAS - NOVO C√ìDIGO")
    print("=" * 52)

    df_pesquisa_final_unificado = unificar_categorias_completo(df_pesquisa_final)

    # Gerar relat√≥rio final
    gerar_relatorio_final_categorias(df_pesquisa_final_unificado)

    # === C√âLULA 8: Remo√ß√£o de features desnecess√°rias ===
    print("\nREMO√á√ÉO DE FEATURES DESNECESS√ÅRIAS")
    print("=" * 38)

    df_features_removidas = remover_features_desnecessarias(df_pesquisa_final_unificado)

    # Listar colunas restantes
    listar_colunas_restantes(df_features_removidas)

    # === C√âLULA 10: Unifica√ß√£o de UTM Source e Term ===
    print("\nUNIFICA√á√ÉO DE UTM SOURCE E TERM")
    print("=" * 35)

    df_utm_unificado = unificar_utm_source_term(df_features_removidas)

    # Verificar consist√™ncia
    verificar_consistencia_utm(df_utm_unificado)

    # === C√âLULA 11: Unifica√ß√£o de UTM Medium - Extra√ß√£o de P√∫blicos ===
    print("\nUNIFICA√á√ÉO DE UTM MEDIUM - EXTRA√á√ÉO DE P√öBLICOS")
    print("=" * 52)

    df_medium_unificado = extrair_publico_medium(df_utm_unificado)

    # Gerar relat√≥rio final
    relatorio_final_medium(df_medium_unificado)

    # === C√âLULA 11.1: Unifica√ß√£o de Medium para Produ√ß√£o ===
    print("\nUNIFICA√á√ÉO DE UTM MEDIUM BASEADA EM ACTIONS + TRATAMENTO DE PRODU√á√ÉO")
    print("=" * 72)

    print("Iniciando processo de unifica√ß√£o para produ√ß√£o...")
    df_original = df_medium_unificado.copy()
    df_medium_producao = unificar_medium_para_producao(df_medium_unificado)

    # Gerar relat√≥rio
    relatorio_unificacao_producao(df_original, df_medium_producao)

    print(f"\nProcesso conclu√≠do!")
    print(f"Dataset final dispon√≠vel em: pesquisa_medium_producao_unificado")
    print(f"Este dataset est√° pronto para o pipeline de produ√ß√£o e n√£o gerar√° incompatibilidades!")

    # === C√âLULA 13: Cria√ß√£o de vers√£o do dataset por missing rate ===
    print("\nCRIA√á√ÉO DE VERS√ïES DO DATASET POR MISSING RATE")
    print("=" * 50)

    print("Iniciando cria√ß√£o das vers√µes...")
    df_pos_cutoff = criar_dataset_pos_cutoff(df_medium_producao)

    # Disponibilizar dataset
    disponibilizar_dataset(df_pos_cutoff)

    print(f"\nProcesso conclu√≠do!")
    print(f"Duas vers√µes do dataset criadas com sucesso.")

    # === C√âLULA 15: Matching robusto por email e telefone ===
    if initial_matching == 'email_only':
        dataset_v1_final = fazer_matching_email_only(df_pos_cutoff, df_vendas_final)
    elif initial_matching == 'email_telefone':
        dataset_v1_final = fazer_matching_email_telefone(df_pos_cutoff, df_vendas_final)
    elif initial_matching == 'variantes':
        dataset_v1_final = fazer_matching_variantes(df_pos_cutoff, df_vendas_final)
    elif initial_matching == 'robusto':
        dataset_v1_final = fazer_matching_robusto(df_pos_cutoff, df_vendas_final)
    elif initial_matching == 'validation':
        dataset_v1_final = fazer_matching_email_with_validation(df_pos_cutoff, df_vendas_final)
    else:
        raise ValueError(f"M√©todo de matching inicial inv√°lido: {initial_matching}. Use 'email_only', 'email_telefone', 'variantes', 'robusto' ou 'validation'")

    # === C√âLULA 17: Filtragem DevClub ===
    dataset_v1_devclub = criar_dataset_devclub(dataset_v1_final, df_vendas_final)

    # Aplicar janela de convers√£o de 20 dias (capta√ß√£o + CPL + carrinho)
    # Capta√ß√£o: 7 dias (ter√ßa-segunda) + CPL: 6 dias (ter√ßa-domingo) + Carrinho: 7 dias (segunda-domingo) = 20 dias
    dataset_v1_devclub = aplicar_janela_conversao(
        df_leads=dataset_v1_devclub,
        df_vendas=df_vendas_final,
        janela_dias=20
    )

    # === LOG: VERIFICA√á√ÉO DE PRODUTOS DEVCLUB ===
    print("\n" + "=" * 80)
    print("VERIFICA√á√ÉO DE PRODUTOS DEVCLUB - An√°lise Completa")
    print("=" * 80)

    # 1. Listar TODOS os produtos que cont√™m "devclub"
    print("\nüìã TODOS OS PRODUTOS COM 'DEVCLUB' NO NOME:")
    print("-" * 80)

    produtos_com_devclub = df_vendas_final[
        df_vendas_final['produto'].fillna('').str.lower().str.contains('devclub', na=False)
    ]['produto'].value_counts()

    print(f"\nTotal de varia√ß√µes encontradas: {len(produtos_com_devclub)}")
    print("\nProdutos e quantidade de vendas:")
    for produto, count in produtos_com_devclub.items():
        print(f"  {count:>5} vendas | {produto}")

    # 2. Lista atual de produtos que estamos usando
    produtos_devclub_lista_atual = [
        'DevClub - Full Stack 2025',
        'DevClub FullStack Pro - OFICIAL',
        'Forma√ß√£o DevClub FullStack Pro - OFICI',
        'Forma√ß√£o DevClub FullStack Pro - OFICIAL',
        'DevClub - Full Stack 2025 - EV',
        'DevClub - FS - Vital√≠cio',
        '[Vital√≠cio] Forma√ß√£o DevClub FullStack',
        '[Vital√≠cio] Forma√ß√£o DevClub FullStack Pro - OFICIAL',
        'Forma√ß√£o DevClub FullStack Pro - COMER',
        'Forma√ß√£o DevClub FullStack Pro - COMERCIAL',
        'Forma√ß√£o DevClub FullStack Pro',
        'DevClub Vital√≠cio',
        'DevClub 3.0 - 2024',
    ]

    # 3. Verificar produtos que EXISTEM mas N√ÉO est√£o na lista
    print("\n" + "=" * 80)
    print("‚ö†Ô∏è  PRODUTOS N√ÉO CONTABILIZADOS (existem mas n√£o est√£o na lista):")
    print("-" * 80)

    produtos_nao_contabilizados = []
    vendas_perdidas = 0

    for produto in produtos_com_devclub.index:
        if produto not in produtos_devclub_lista_atual:
            produtos_nao_contabilizados.append(produto)
            vendas_perdidas += produtos_com_devclub[produto]
            print(f"  {produtos_com_devclub[produto]:>5} vendas | {produto}")

    if not produtos_nao_contabilizados:
        print("  ‚úÖ Nenhum produto perdido! Todos est√£o sendo contabilizados.")
    else:
        print(f"\n  ‚ö†Ô∏è  TOTAL DE VENDAS PERDIDAS: {vendas_perdidas}")

    # 4. Verificar produtos na lista que N√ÉO existem
    print("\n" + "=" * 80)
    print("üîç PRODUTOS NA LISTA MAS SEM VENDAS:")
    print("-" * 80)

    produtos_sem_vendas = []
    for produto in produtos_devclub_lista_atual:
        if produto not in produtos_com_devclub.index:
            produtos_sem_vendas.append(produto)
            print(f"  ‚ö†Ô∏è  {produto}")

    if not produtos_sem_vendas:
        print("  ‚úÖ Todos os produtos da lista t√™m vendas!")

    # 5. Atualizar lista completa
    produtos_devclub = list(produtos_com_devclub.index)

    print("\n" + "=" * 80)
    print("‚úÖ LISTA ATUALIZADA - Usando TODOS os produtos DevClub encontrados")
    print("=" * 80)
    print(f"Total de produtos na lista atualizada: {len(produtos_devclub)}")

    # === LOG: C√ÅLCULO DE RECALL E FATOR DE CORRE√á√ÉO ===
    print("\n" + "=" * 80)
    print("C√ÅLCULO DE RECALL - Convers√µes Observadas vs Vendas Reais")
    print("=" * 80)

    # Contar convers√µes observadas (matches)
    conversoes_observadas = dataset_v1_devclub['target'].sum()
    total_leads = len(dataset_v1_devclub)

    # Filtrar vendas DevClub
    vendas_devclub = df_vendas_final[
        df_vendas_final['produto'].isin(produtos_devclub)
    ].copy()

    # Filtrar por per√≠odo (mesmo per√≠odo dos leads)
    if 'data' in vendas_devclub.columns:
        vendas_devclub['data_dt'] = pd.to_datetime(vendas_devclub['data'], errors='coerce')
        # Per√≠odo dos leads (aproximado - 2025-03-01 a 2025-11-04)
        periodo_inicio = pd.to_datetime('2025-03-01')
        periodo_fim = pd.to_datetime('2025-11-04')
        vendas_periodo = vendas_devclub[
            (vendas_devclub['data_dt'] >= periodo_inicio - pd.Timedelta(days=20)) &
            (vendas_devclub['data_dt'] <= periodo_fim + pd.Timedelta(days=20))
        ].copy()
    else:
        vendas_periodo = vendas_devclub.copy()

    # Remover duplicatas (mesmo email/telefone + produto + data + valor)
    vendas_periodo['email_lower'] = vendas_periodo['email'].fillna('').astype(str).str.lower().str.strip()
    vendas_periodo['telefone_clean'] = vendas_periodo['telefone'].fillna('').astype(str).str.strip()
    vendas_periodo['produto_clean'] = vendas_periodo['produto'].fillna('').astype(str).str.strip()
    vendas_periodo['data_str'] = vendas_periodo['data_dt'].astype(str) if 'data_dt' in vendas_periodo.columns else vendas_periodo['data'].astype(str)
    vendas_periodo['valor_str'] = vendas_periodo['valor'].fillna(0).astype(str)

    vendas_periodo['chave_dedup'] = (
        vendas_periodo['email_lower'] + '|' +
        vendas_periodo['telefone_clean'] + '|' +
        vendas_periodo['produto_clean'] + '|' +
        vendas_periodo['data_str'] + '|' +
        vendas_periodo['valor_str']
    )
    vendas_unicas = vendas_periodo.drop_duplicates(subset='chave_dedup', keep='first')

    # Calcular m√©tricas
    vendas_reais = len(vendas_unicas)
    recall = conversoes_observadas / vendas_reais if vendas_reais > 0 else 0
    fator_correcao = 1 / recall if recall > 0 else 0

    taxa_observada = conversoes_observadas / total_leads if total_leads > 0 else 0
    taxa_real = vendas_reais / total_leads if total_leads > 0 else 0

    print(f"\nüìä DADOS:")
    print(f"  Total de leads: {total_leads:,}")
    print(f"  Convers√µes OBSERVADAS (matches): {conversoes_observadas}")
    print(f"  Vendas REAIS (sem duplicatas): {vendas_reais:,}")

    print(f"\nüìà TAXAS:")
    print(f"  Taxa OBSERVADA: {taxa_observada*100:.4f}%")
    print(f"  Taxa REAL: {taxa_real*100:.4f}%")

    print(f"\nüîß M√âTRICAS:")
    print(f"  Recall: {recall*100:.1f}%")
    print(f"  Fator de corre√ß√£o: {fator_correcao:.3f}x")

    if fator_correcao > 1:
        print(f"\nüí° IMPACTO:")
        print(f"  Estamos SUBESTIMANDO em {fator_correcao:.3f}x")
        print(f"  Valores CAPI deveriam ser {(fator_correcao-1)*100:.0f}% maiores")

    print("=" * 80)

    # === C√âLULA 18: Feature Engineering ===
    dataset_v1_devclub_fe = criar_features_derivadas(dataset_v1_devclub)

    # === C√âLULA 20: Encoding Estrat√©gico ===
    dataset_v1_devclub_encoded = aplicar_encoding_estrategico(dataset_v1_devclub_fe)

    # === HYPERPARAMETER TUNING (opcional) ===
    melhores_params = None
    if tune_hyperparams:
        print("\n" + "=" * 80)
        print("EXECUTANDO HYPERPARAMETER TUNING")
        print("=" * 80)

        resultado_tuning = hyperparameter_tuning(
            dataset_v1_devclub_encoded,
            dataset_v1_devclub,
            grid_size=grid_size
        )

        if resultado_tuning and resultado_tuning['usar_tunado']:
            melhores_params = resultado_tuning['melhores_params']
            print(f"\n‚úÖ Usando hiperpar√¢metros tunados no treino final")
        else:
            print(f"\n‚ö†Ô∏è  Mantendo hiperpar√¢metros baseline (tuning n√£o trouxe ganho significativo)")

    # === C√âLULA MODELAGEM: Treino e Registro do Modelo ===
    resultado_registro_devclub = registrar_features_e_modelo_devclub(
        dataset_v1_devclub_encoded,
        dataset_v1_devclub,
        save_files=save_files,
        matching_method=initial_matching,
        custom_hyperparams=melhores_params,
        split_method=split_method
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Pipeline de treino DevClub')
    parser.add_argument(
        '--initial-matching',
        type=str,
        choices=['email_only', 'email_telefone', 'variantes', 'robusto', 'validation'],
        default='email_telefone',
        help='M√©todo de matching inicial (c√©lula 15) - padr√£o: email_telefone (+16.5%% dados, melhor separa√ß√£o D10/D1)'
    )
    parser.add_argument(
        '--save-files',
        action='store_true',
        help='Salvar arquivos locais em files/{timestamp} (padr√£o: False - apenas MLflow)'
    )
    parser.add_argument(
        '--tune-hyperparams',
        action='store_true',
        help='Executar hyperparameter tuning antes do treino (padr√£o: False)'
    )
    parser.add_argument(
        '--grid-size',
        type=str,
        choices=['small', 'medium', 'large'],
        default='small',
        help='Tamanho do grid search: small (6 comb), medium (48), large (96) - padr√£o: small'
    )
    parser.add_argument(
        '--split-method',
        type=str,
        choices=['temporal', 'stratified'],
        default='temporal',
        help='M√©todo de split: temporal (70%% dos dias) ou stratified (70%% dos registros) - padr√£o: temporal'
    )
    parser.add_argument(
        '--use-guru-only',
        type=str,
        choices=['true', 'false'],
        default=None,
        help='Filtro de produtos: true (apenas GURU), false (GURU+TMB) - padr√£o: usar config'
    )

    args = parser.parse_args()

    # Converter string para bool se fornecido
    use_guru_only = None
    if args.use_guru_only:
        use_guru_only = args.use_guru_only.lower() == 'true'

    main(
        initial_matching=args.initial_matching,
        save_files=args.save_files,
        tune_hyperparams=args.tune_hyperparams,
        grid_size=args.grid_size,
        split_method=args.split_method,
        use_guru_only=use_guru_only
    )
