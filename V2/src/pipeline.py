"""
Pipeline principal de lead scoring em produ√ß√£o.
APENAS orquestra componentes, sem conter l√≥gica pr√≥pria.
Reproduz EXATAMENTE a l√≥gica do notebook com par√¢metros configur√°veis.
"""

import pandas as pd
import logging
from .data.preprocessing import remove_duplicates, clean_columns, remove_campaign_features, remove_technical_fields, rename_long_column_names
from .data.utm_unification import unify_utm_columns
from .data.medium_unification import unify_medium_columns
from .features.engineering import create_derived_features
from .features.encoding import apply_categorical_encoding
from .features.utm_removal import remove_utm_features

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LeadScoringPipeline:
    """
    Pipeline de produ√ß√£o para lead scoring.

    Reproduz EXATAMENTE a l√≥gica do notebook com par√¢metros configur√°veis.
    """

    def __init__(self, com_utm: bool = True, versao: str = "v1", usar_cutoff: bool = False):
        """
        Inicializa o pipeline com par√¢metros de configura√ß√£o.

        Args:
            com_utm: Se True, mant√©m features UTM. Se False, remove ap√≥s encoding
            versao: "v1" ou "v2" (v2 tem algumas features a menos)
            usar_cutoff: Se True, aplica cutoff temporal (apenas para refer√™ncia)
        """
        self.data = None
        self.com_utm = com_utm
        self.versao = versao
        self.usar_cutoff = usar_cutoff

    def load_data(self, filepath: str) -> pd.DataFrame:
        """
        Carrega arquivo de leads no formato Excel.

        Args:
            filepath: Caminho para o arquivo Excel

        Returns:
            DataFrame com os dados carregados
        """
        logger.info(f"Carregando arquivo: {filepath}")
        self.data = pd.read_excel(filepath)
        logger.info(f"Arquivo carregado: {len(self.data)} linhas, {len(self.data.columns)} colunas")
        return self.data

    def preprocess(self) -> pd.DataFrame:
        """
        Aplica pr√©-processamento aos dados.

        Returns:
            DataFrame pr√©-processado
        """
        if self.data is None:
            raise ValueError("Dados n√£o carregados. Use load_data() primeiro.")

        initial_rows = len(self.data)
        initial_cols = len(self.data.columns)

        logger.info(f"üìä IN√çCIO DO PIPELINE: {initial_rows} linhas, {initial_cols} colunas")

        # 1. Remover duplicatas (usando componente importado)
        logger.info("üîÑ [1/10] Removendo duplicatas...")
        self.data = remove_duplicates(self.data)

        duplicates_removed = initial_rows - len(self.data)
        logger.info(f"   ‚û§ Duplicatas removidas: {duplicates_removed}")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 2. Limpar colunas desnecess√°rias (usando componente importado)
        logger.info("üîÑ [2/10] Removendo colunas score/faixa...")
        cols_before_clean = len(self.data.columns)
        self.data = clean_columns(self.data)

        columns_removed = cols_before_clean - len(self.data.columns)
        logger.info(f"   ‚û§ Colunas de score/faixa removidas: {columns_removed}")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 3. Remover features de campanha (usando componente importado)
        logger.info("üîÑ [3/10] Removendo features de campanha...")
        cols_before_campaign = len(self.data.columns)
        self.data = remove_campaign_features(self.data)

        campaign_cols_removed = cols_before_campaign - len(self.data.columns)
        logger.info(f"   ‚û§ Features de campanha removidas: {campaign_cols_removed}")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 4. Unificar categorias UTM (usando componente importado)
        logger.info("üîÑ [4/10] Unificando categorias UTM...")
        utm_source_before = self.data['Source'].nunique() if 'Source' in self.data.columns else 0
        utm_term_before = self.data['Term'].nunique() if 'Term' in self.data.columns else 0

        self.data = unify_utm_columns(self.data)

        utm_source_after = self.data['Source'].nunique() if 'Source' in self.data.columns else 0
        utm_term_after = self.data['Term'].nunique() if 'Term' in self.data.columns else 0
        logger.info(f"   ‚û§ Source: {utm_source_before}‚Üí{utm_source_after} categorias")
        logger.info(f"   ‚û§ Term: {utm_term_before}‚Üí{utm_term_after} categorias")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 5. Unificar categorias Medium (usando componente importado)
        logger.info("üîÑ [5/10] Unificando categorias Medium...")
        medium_before = self.data['Medium'].nunique() if 'Medium' in self.data.columns else 0

        self.data = unify_medium_columns(self.data)

        medium_after = self.data['Medium'].nunique() if 'Medium' in self.data.columns else 0
        logger.info(f"   ‚û§ Medium: {medium_before}‚Üí{medium_after} categorias")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 6. Remover campos t√©cnicos (usando componente importado)
        logger.info("üîÑ [6/10] Removendo campos t√©cnicos...")
        cols_before_tech = len(self.data.columns)
        self.data = remove_technical_fields(self.data)

        tech_cols_removed = cols_before_tech - len(self.data.columns)
        logger.info(f"   ‚û§ Campos t√©cnicos removidos: {tech_cols_removed}")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 7. Renomear colunas longas (usando componente importado)
        logger.info("üîÑ [7/10] Renomeando colunas longas...")
        cols_before_rename = len(self.data.columns)
        self.data = rename_long_column_names(self.data)

        # N√∫mero de colunas deveria permanecer o mesmo (renomea√ß√£o n√£o adiciona/remove)
        logger.info(f"   ‚û§ Colunas renomeadas (mant√©m total): {len(self.data.columns)}")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 8. Engenharia de features (usando componente importado)
        logger.info("üîÑ [8/10] Aplicando engenharia de features...")
        cols_before_fe = len(self.data.columns)

        # Verificar se colunas necess√°rias existem
        fe_input_cols = ['Data', 'Nome Completo', 'E-mail', 'Telefone']
        available_fe_cols = [col for col in fe_input_cols if col in self.data.columns]
        logger.info(f"   ‚û§ Colunas dispon√≠veis para FE: {available_fe_cols}")

        self.data = create_derived_features(self.data)

        # DIFEREN√áAS V1 vs V2: V2 remove 3 features espec√≠ficas (conforme notebook original)
        if self.versao == "v2":
            features_v2_remover = [
                'J√° estudou programa√ß√£o?',
                'Voc√™ j√° fez/faz/pretende fazer faculdade?',
                'Tem computador/notebook?'
                # NOTA: 'Qual o seu n√≠vel em programa√ß√£o?' j√° foi removido no pr√©-processamento
            ]
            cols_before_v2_removal = len(self.data.columns)
            self.data = self.data.drop(columns=features_v2_remover, errors='ignore')
            cols_removed_v2 = cols_before_v2_removal - len(self.data.columns)
            logger.info(f"   ‚û§ V2: Features espec√≠ficas removidas: {cols_removed_v2}")

        cols_added = len(self.data.columns) - cols_before_fe
        logger.info(f"   ‚û§ Features criadas/processadas: {cols_added} novas colunas")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 9. Encoding categ√≥rico (usando componente importado)
        logger.info("üîÑ [9/10] Aplicando encoding categ√≥rico...")
        cols_before_encoding = len(self.data.columns)

        self.data = apply_categorical_encoding(self.data, versao=self.versao)

        encoding_cols_added = len(self.data.columns) - cols_before_encoding
        logger.info(f"   ‚û§ Colunas adicionadas pelo encoding: {encoding_cols_added}")
        logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")

        # 10. Remo√ß√£o condicional de UTM (baseada no par√¢metro com_utm)
        if not self.com_utm:
            logger.info("üîÑ [10/10] Removendo features UTM...")
            cols_before_utm = len(self.data.columns)

            self.data = remove_utm_features(self.data)

            utm_cols_removed = cols_before_utm - len(self.data.columns)
            logger.info(f"   ‚û§ Estado atual: {len(self.data)} linhas, {len(self.data.columns)} colunas")
        else:
            logger.info("üîÑ [10/10] Mantendo features UTM")

        # Resumo final
        final_rows = len(self.data)
        final_cols = len(self.data.columns)
        total_rows_removed = initial_rows - final_rows
        net_cols_change = final_cols - initial_cols

        config_info = f" (com_utm={self.com_utm}, versao={self.versao})"
        logger.info(f"üìä RESUMO FINAL{config_info}:")
        logger.info(f"   ‚û§ Linhas: {initial_rows}‚Üí{final_rows} (removidas: {total_rows_removed})")
        logger.info(f"   ‚û§ Colunas: {initial_cols}‚Üí{final_cols} (varia√ß√£o: {net_cols_change:+d})")

        return self.data

    def run(self, filepath: str) -> pd.DataFrame:
        """
        Executa o pipeline completo.

        Args:
            filepath: Caminho para o arquivo de entrada

        Returns:
            DataFrame processado
        """
        logger.info("=== Iniciando Pipeline de Lead Scoring ===")

        # Carregar dados
        self.load_data(filepath)

        # Pr√©-processar
        self.preprocess()

        logger.info("=== Pipeline conclu√≠do ===")
        return self.data