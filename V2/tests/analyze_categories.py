"""
Script para analisar categorias duplicadas nas colunas categ√≥ricas do arquivo LF24.
Verifica se ainda h√° necessidade de unifica√ß√£o de categorias em produ√ß√£o.
"""

import pandas as pd
import numpy as np
from collections import Counter


def analyze_categorical_columns():
    """Analisa categorias duplicadas nas colunas categ√≥ricas."""

    # Carregar dados de produ√ß√£o
    filepath = '../data/devclub/LF + ALUNOS/Lead score LF 24.xlsx'
    df = pd.read_excel(filepath, sheet_name='LF Pesquisa')

    print("=== AN√ÅLISE DE CATEGORIAS DUPLICADAS - ARQUIVO LF24 ===\n")
    print(f"Total de registros: {len(df)}")

    # Colunas categ√≥ricas para analisar
    categorical_columns = [
        'O seu g√™nero:',
        'Qual estado voc√™ mora?',
        'Qual a sua idade?',
        'O que voc√™ faz atualmente?',
        'Atualmente, qual a sua faixa salarial?',
        'Voc√™ possui cart√£o de cr√©dito?',
        'J√° estudou programa√ß√£o?',
        'Voc√™ j√° fez/faz/pretende fazer faculdade?',
        'J√° investiu em algum curso online para aprender uma nova forma de ganhar dinheiro?',
        'O que mais te chama aten√ß√£o na profiss√£o de Programador?',
        'O que mais voc√™ quer ver no evento?'
    ]

    # Armazenar resultados da an√°lise
    duplicates_found = {}

    for col in categorical_columns:
        if col in df.columns:
            print(f"\nüìä Analisando: {col}")
            print("-" * 50)

            # Obter valores √∫nicos (excluindo NaN)
            values = df[col].dropna().astype(str).str.strip()
            unique_values = values.unique()
            value_counts = values.value_counts()

            print(f"Total de categorias √∫nicas: {len(unique_values)}")

            # Detectar poss√≠veis duplicatas (case-insensitive, espa√ßos extras, etc.)
            potential_duplicates = []

            # Agrupar por vers√£o normalizada (lowercase, sem espa√ßos extras)
            normalized_groups = {}
            for val in unique_values:
                normalized = val.lower().strip()
                normalized = ' '.join(normalized.split())  # Remove espa√ßos m√∫ltiplos

                if normalized not in normalized_groups:
                    normalized_groups[normalized] = []
                normalized_groups[normalized].append(val)

            # Identificar grupos com mais de uma varia√ß√£o
            for norm_val, variations in normalized_groups.items():
                if len(variations) > 1:
                    potential_duplicates.append(variations)
                    print(f"\n  ‚ö†Ô∏è Poss√≠veis duplicatas encontradas:")
                    for var in variations:
                        count = value_counts[var]
                        print(f"    - '{var}' ({count} ocorr√™ncias)")

            # Detectar varia√ß√µes similares (ex: "Sim" vs "sim", "SIM")
            if not potential_duplicates:
                # Verificar varia√ß√µes com pequenas diferen√ßas
                for i, val1 in enumerate(unique_values):
                    for val2 in unique_values[i+1:]:
                        # Comparar vers√µes normalizadas
                        if val1.lower().strip() == val2.lower().strip() and val1 != val2:
                            print(f"\n  ‚ö†Ô∏è Varia√ß√µes de caso encontradas:")
                            print(f"    - '{val1}' ({value_counts[val1]} ocorr√™ncias)")
                            print(f"    - '{val2}' ({value_counts[val2]} ocorr√™ncias)")
                            potential_duplicates.append([val1, val2])

            if potential_duplicates:
                duplicates_found[col] = potential_duplicates
            else:
                print("  ‚úÖ Nenhuma duplicata √≥bvia encontrada")

            # Mostrar top 5 categorias mais frequentes
            print(f"\n  Top 5 categorias mais frequentes:")
            for val, count in value_counts.head(5).items():
                pct = (count / len(values)) * 100
                print(f"    - '{val}': {count} ({pct:.1f}%)")

        else:
            print(f"\n‚ùå Coluna n√£o encontrada: {col}")

    # Resumo final
    print("\n" + "=" * 60)
    print("RESUMO DA AN√ÅLISE")
    print("=" * 60)

    if duplicates_found:
        print(f"\n‚ö†Ô∏è Colunas com poss√≠veis duplicatas: {len(duplicates_found)}")
        for col, dups in duplicates_found.items():
            print(f"\n  {col}:")
            print(f"    - {len(dups)} grupos de duplicatas encontrados")
    else:
        print("\n‚úÖ Nenhuma duplicata significativa encontrada nas colunas categ√≥ricas!")
        print("   A Se√ß√£o 7 de unifica√ß√£o de categorias pode n√£o ser necess√°ria.")

    return duplicates_found


def check_specific_unifications():
    """Verifica unifica√ß√µes espec√≠ficas mencionadas no notebook original."""

    filepath = '../data/devclub/LF + ALUNOS/Lead score LF 24.xlsx'
    df = pd.read_excel(filepath, sheet_name='LF Pesquisa')

    print("\n\n=== VERIFICA√á√ÉO DE UNIFICA√á√ïES ESPEC√çFICAS DO NOTEBOOK ===\n")

    # Verificar algumas unifica√ß√µes espec√≠ficas mencionadas no notebook
    checks = {
        'O que voc√™ faz atualmente?': {
            'patterns': [
                ('CLT', 'clt', 'Clt'),
                ('Freelancer', 'freelancer', 'FREELANCER'),
                ('Desempregado', 'desempregado', 'DESEMPREGADO'),
                ('Aut√¥nomo', 'aut√¥nomo', 'AUT√îNOMO')
            ]
        },
        'Qual estado voc√™ mora?': {
            'patterns': [
                ('SP', 'sp', 'S√£o Paulo', 'S√ÉO PAULO'),
                ('RJ', 'rj', 'Rio de Janeiro', 'RIO DE JANEIRO'),
                ('MG', 'mg', 'Minas Gerais', 'MINAS GERAIS')
            ]
        }
    }

    for col, check_data in checks.items():
        if col in df.columns:
            print(f"\nüìå {col}")
            values = df[col].dropna().astype(str)

            for pattern_group in check_data['patterns']:
                found = []
                for pattern in pattern_group:
                    count = (values == pattern).sum()
                    if count > 0:
                        found.append(f"'{pattern}' ({count})")

                if len(found) > 1:
                    print(f"  ‚ö†Ô∏è Varia√ß√µes encontradas: {', '.join(found)}")
                elif len(found) == 1:
                    print(f"  ‚úì Apenas uma forma encontrada: {found[0]}")


if __name__ == "__main__":
    # Executar an√°lise
    duplicates = analyze_categorical_columns()
    check_specific_unifications()

    print("\n\n" + "=" * 60)
    print("CONCLUS√ÉO")
    print("=" * 60)

    if duplicates:
        print("\n‚ö†Ô∏è RECOMENDA√á√ÉO: Implementar componente de unifica√ß√£o de categorias")
        print("   Foram encontradas varia√ß√µes que podem afetar o modelo.")
    else:
        print("\n‚úÖ RECOMENDA√á√ÉO: Pular Se√ß√£o 7 de unifica√ß√£o de categorias")
        print("   Os dados de produ√ß√£o (LF24) parecem estar bem padronizados.")