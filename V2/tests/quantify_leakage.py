"""Quantifica data leakage no split temporal_leads usando test_set_predictions."""
import pandas as pd
import sys

print("="*80)
print("QUANTIFICANDO DATA LEAKAGE - SPLIT TEMPORAL_LEADS")
print("="*80)

# Carregar test_set salvo pelo treino
test_path = "files/20251111_212345/test_set_predictions.csv"
print(f"\nüìÅ Carregando test set: {test_path}")

try:
    df_test = pd.read_csv(test_path)
    print(f"‚úì {len(df_test)} registros no test set")
except Exception as e:
    print(f"‚ùå ERRO: {e}")
    sys.exit(1)

# Verificar colunas dispon√≠veis
print(f"\nColunas dispon√≠veis: {df_test.columns.tolist()[:10]}...")

# An√°lise de leakage via matching com dataset completo
# Como n√£o temos o train_set salvo, vamos carregar o dataset completo
# e calcular quais registros est√£o no treino (70% primeiros por data)

print("\nüîÑ Carregando dataset completo para identificar train set...")

# Carregar CSVs
import glob
pesquisa_files = glob.glob("files/dados_originais/[LF]*.xlsx") + glob.glob("files/dados_originais/Lead*.xlsx") + glob.glob("files/dados_originais/LF*.xlsx")

print(f"  Arquivos encontrados: {len(pesquisa_files)}")

# Simplificar: apenas ler o test_set e ver se conseguimos calcular
# A melhor forma √© calcular os √≠ndices de treino vs teste

# Vou usar uma abordagem mais simples: carregar os dados processados
# Se o pipeline salvou, deve ter um arquivo intermedi√°rio

# Na verdade, vou fazer diferente: vou carregar direto usando o mesmo c√≥digo
# do pipeline mas s√≥ as colunas de identifica√ß√£o

print("\n‚öôÔ∏è  Carregando dataset original processado...")
print("   (Executando c√©lulas do pipeline at√© ter Email/Telefone...)")

# Importar fun√ß√µes do pipeline
sys.path.insert(0, '.')
from src.data_processing.leitura_arquivos import ler_arquivos
from src.data_processing.filtragem_abas_training import filtrar_abas
from src.data_processing.column_cleaning import remover_colunas_desnecessarias
from src.data_processing.column_unification import consolidar_datasets, unificar_colunas_duplicadas, unificar_categorias
from src.data_processing.dataset_versioning_training import criar_dataset_pos_cutoff, disponibilizar_dataset
from src.data_processing.devclub_filtering_training import criar_dataset_devclub
from src.matching.matching_email_telefone import executar_matching_email_telefone

# Executar pipeline at√© ter o dataset
import yaml
with open('configs/devclub.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# Ler arquivos
arquivos_excluidos = ['vendas-tmb_1.1.xlsx', 'vendas_tmb_2.1.xlsx', 'vendas_tmb_2.2.xlsx']
arquivos_excel = ler_arquivos('files/dados_originais', config, exclude_files=arquivos_excluidos)

# Filtrar e limpar
arquivos_filtrados = filtrar_abas(arquivos_excel, config)
arquivos_filtrados_limpos = remover_colunas_desnecessarias(arquivos_filtrados, config)

# Consolidar
dataset_pesquisa, dataset_vendas = consolidar_datasets(arquivos_filtrados_limpos)
dataset_pesquisa = unificar_colunas_duplicadas(dataset_pesquisa, dataset_vendas)
dataset_pesquisa_final, dataset_vendas_final = unificar_categorias(dataset_pesquisa, dataset_vendas, config)

# Criar dataset pos cutoff
df_pos_cutoff = criar_dataset_pos_cutoff(dataset_pesquisa_final)
dataset_v1_final, _ = disponibilizar_dataset(df_pos_cutoff)

# Matching
dataset_v1_vendas = dataset_vendas_final.copy()
dataset_v1_vendas['email'] = dataset_v1_vendas['email'].fillna('').astype(str).str.strip()
dataset_v1_vendas['telefone'] = dataset_v1_vendas['telefone'].fillna('').astype(str).str.strip()
dataset_v1_vendas['nome'] = dataset_v1_vendas['nome'].fillna('').astype(str).str.strip()

dataset_v1_final['Email'] = dataset_v1_final['E-mail'].fillna('').astype(str).str.strip()
dataset_v1_final['Telefone'] = dataset_v1_final['Telefone'].fillna('').astype(str).str.strip()
dataset_v1_final['Nome Completo'] = dataset_v1_final['Nome Completo'].fillna('').astype(str).str.strip()

matches_df = executar_matching_email_telefone(
    dataset_v1_final, 
    dataset_v1_vendas,
    email_col_pesquisa='Email',
    email_col_vendas='email',
    telefone_col_pesquisa='Telefone',
    telefone_col_vendas='telefone'
)

# Criar dataset DevClub
dataset_v1_devclub = criar_dataset_devclub(dataset_v1_final, dataset_v1_vendas)

print(f"\n‚úì Dataset completo: {len(dataset_v1_devclub)} registros")
print(f"  Colunas: Email, Telefone, Data, ...")

# Simular split temporal_leads (70% dos leads ordenados por data)
df = dataset_v1_devclub.copy()
df['Data'] = pd.to_datetime(df['Data'])
df_sorted = df.sort_values('Data').reset_index(drop=True)
n_train = int(len(df_sorted) * 0.7)

df_train = df_sorted.iloc[:n_train]
df_test_calc = df_sorted.iloc[n_train:]

print(f"\n‚úì Split calculado:")
print(f"  Treino: {len(df_train)} registros ({len(df_train)/len(df)*100:.1f}%)")
print(f"  Teste: {len(df_test_calc)} registros ({len(df_test_calc)/len(df)*100:.1f}%)")
print(f"\nüìÖ Per√≠odos:")
print(f"  Treino: {df_train['Data'].min().date()} a {df_train['Data'].max().date()}")
print(f"  Teste: {df_test_calc['Data'].min().date()} a {df_test_calc['Data'].max().date()}")

# An√°lise EMAIL
print("\n" + "="*80)
print("LEAKAGE POR EMAIL")
print("="*80)

train_emails = set(df_train['Email'].dropna().str.lower().str.strip())
train_emails.discard('')
test_emails = set(df_test_calc['Email'].dropna().str.lower().str.strip())
test_emails.discard('')
emails_both = train_emails & test_emails

print(f"Emails √∫nicos TREINO: {len(train_emails)}")
print(f"Emails √∫nicos TESTE: {len(test_emails)}")
print(f"Emails em AMBOS: {len(emails_both)}")
leakage_email = len(emails_both) / len(test_emails) * 100 if test_emails else 0
print(f"\nüìä LEAKAGE EMAIL: {leakage_email:.2f}% do test set")

if emails_both and len(emails_both) <= 10:
    print(f"\nüìß Emails vazados:")
    for i, email in enumerate(list(emails_both), 1):
        print(f"  {i}. {email}")
elif emails_both:
    print(f"\nüìß Exemplos (primeiros 5):")
    for i, email in enumerate(list(emails_both)[:5], 1):
        print(f"  {i}. {email}")

# An√°lise TELEFONE
print("\n" + "="*80)
print("LEAKAGE POR TELEFONE")
print("="*80)

def norm_phone(phone):
    if pd.isna(phone) or phone == '': return None
    return str(phone).replace(' ','').replace('(','').replace(')','').replace('-','').strip()

train_phones = set(df_train['Telefone'].apply(norm_phone))
train_phones.discard(None)
train_phones.discard('')
test_phones = set(df_test_calc['Telefone'].apply(norm_phone))
test_phones.discard(None)
test_phones.discard('')
phones_both = train_phones & test_phones

print(f"Telefones √∫nicos TREINO: {len(train_phones)}")
print(f"Telefones √∫nicos TESTE: {len(test_phones)}")
print(f"Telefones em AMBOS: {len(phones_both)}")
leakage_phone = len(phones_both) / len(test_phones) * 100 if test_phones else 0
print(f"\nüìä LEAKAGE TELEFONE: {leakage_phone:.2f}% do test set")

# An√°lise COMBINADA
print("\n" + "="*80)
print("LEAKAGE COMBINADO (EMAIL OU TELEFONE)")
print("="*80)

def person_id(row):
    ids = []
    email = row.get('Email')
    telefone = row.get('Telefone')
    if pd.notna(email) and email != '':
        ids.append(f"e:{str(email).lower().strip()}")
    if pd.notna(telefone) and telefone != '':
        phone = norm_phone(telefone)
        if phone:
            ids.append(f"t:{phone}")
    return tuple(sorted(ids))

train_persons = set(df_train.apply(person_id, axis=1))
train_persons.discard(())
test_persons = set(df_test_calc.apply(person_id, axis=1))
test_persons.discard(())
persons_both = train_persons & test_persons

print(f"Pessoas √∫nicas TREINO: {len(train_persons)}")
print(f"Pessoas √∫nicas TESTE: {len(test_persons)}")
print(f"Pessoas em AMBOS: {len(persons_both)}")
leakage_combined = len(persons_both) / len(test_persons) * 100 if test_persons else 0
print(f"\nüìä LEAKAGE COMBINADO: {leakage_combined:.2f}% do test set")

# RESUMO
print("\n" + "="*80)
print("RESUMO E RECOMENDA√á√ÉO")
print("="*80)

max_leak = max(leakage_email, leakage_phone, leakage_combined)
print(f"\nLeakage EMAIL: {leakage_email:.2f}%")
print(f"Leakage TELEFONE: {leakage_phone:.2f}%")
print(f"Leakage COMBINADO: {leakage_combined:.2f}%")

emoji = '‚úÖ' if max_leak < 5 else '‚ö†Ô∏è' if max_leak < 10 else '‚ùå'
print(f"\n{emoji} Leakage m√°ximo: {max_leak:.2f}%")

if max_leak < 5:
    print("\n‚úÖ ACEIT√ÅVEL")
    print("   Impacto m√≠nimo nas m√©tricas de valida√ß√£o.")
    print("   Performance do modelo √© confi√°vel.")
elif max_leak < 10:
    print("\n‚ö†Ô∏è MODERADO - Provavelmente aceit√°vel")
    print(f"   Ganhos observados: AUC +7%, Lift +14%")
    print("   Leakage moderado pode inflado levemente as m√©tricas,")
    print("   mas provavel que ganhos sejam reais.")
else:
    print("\n‚ùå ALTO - M√©tricas podem n√£o ser confi√°veis")
    print("   Recomendo implementar temporal_leads com groupby por pessoa")

print("\n" + "="*80)
print("CONTEXTO DA DECIS√ÉO")
print("="*80)

print("\nSplit STRATIFIED (anterior):")
print("  ‚úÖ Zero leakage (100% garantido)")
print("  ‚ùå Test set n√£o representa produ√ß√£o")
print("  ‚ùå Thresholds fixos invi√°veis (CV=0.593)")

print("\nSplit TEMPORAL_LEADS (novo):")
print(f"  {emoji} Leakage: {max_leak:.2f}%")
print("  ‚úÖ Test set representa melhor produ√ß√£o")
print("  ‚úÖ Performance: AUC 0.747 (+7%), Lift 3.0x (+14%)")
print("  ‚úÖ Valida√ß√£o temporal preservada")
print("  ‚úÖ Simula produ√ß√£o (scoramos returning leads)")

print("\nüí° RECOMENDA√á√ÉO:")
if max_leak < 10:
    print(f"   Aceitar split temporal_leads.")
    print(f"   Leakage de {max_leak:.2f}% √© aceit√°vel porque:")
    print("   1. Em produ√ß√£o, realmente scoramos leads recorrentes")
    print("   2. Valida√ß√£o temporal est√° preservada (treino passado, teste futuro)")
    print("   3. Performance melhorou significativamente")
    print("   4. Test set representa melhor a distribui√ß√£o de produ√ß√£o")
else:
    print("   Implementar vers√£o h√≠brida:")
    print("   1. Ordenar dataset por data")
    print("   2. Agrupar por pessoa (email/telefone)")
    print("   3. Split 70/30 garantindo mesma pessoa sempre no mesmo set")

print("="*80)
